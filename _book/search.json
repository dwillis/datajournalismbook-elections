[
  {
    "objectID": "aggregates.html",
    "href": "aggregates.html",
    "title": "10  Aggregates",
    "section": "",
    "text": "10.1 Libraries\nR is a statistical programming language that is purpose-built for data analysis.\nBase R does a lot, but there are a mountain of external libraries that do things to make R better/easier/more fully featured. We already installed the tidyverse – or you should have if you followed the instructions for the last assignment – which isn’t exactly a library, but a collection of libraries. Together, they make up the Tidyverse. Individually, they are extraordinarily useful for what they do. We can load them all at once using the tidyverse name, or we can load them individually. Let’s start with individually.\nThe two libraries we are going to need for this assignment are readr and dplyr. The library readr reads different types of data in. For this assignment, we’re going to read in csv data or Comma Separated Values data. That’s data that has a comma between each column of data.\nThen we’re going to use dplyr to analyze it.\nTo use a library, you need to import it. Good practice – one I’m going to insist on – is that you put all your library steps at the top of your notebooks.\nThat code looks like this:\nlibrary(readr)\nTo load them both, you need to do this:\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\nBut, because those two libraries – and several others that we’re going to use over the course of this class – are so commonly used, there’s a shortcut to loading all of the libraries we’ll need:\nlibrary(tidyverse)\nYou can keep doing that for as many libraries as you need.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Aggregates</span>"
    ]
  },
  {
    "objectID": "aggregates.html#importing-data",
    "href": "aggregates.html#importing-data",
    "title": "10  Aggregates",
    "section": "10.2 Importing data",
    "text": "10.2 Importing data\nThe first thing we need to do is get some data to work with. We do that by reading it in. In our case, we’re going to read a datatable from an “rds” file, which is a format for storing data with R. Later in the course, we’ll more frequently work with a format called a CSV. A CSV is a stripped down version of a spreadsheet you might open in a program like Excel, in which each column is separated by a comma. RDS files are less common when getting data from other people. But reading in CSVs is less foolproof than reading in rds files, so for now we’ll work with rds.\nThe rds file we’re going to read in contains individual campaign contributions from Maryland donors via WinRed, an online fundraising platform used by conservatives. We’ll be working with a slice of the data from earlier this year.\nSo step 1 is to import the data. The code to import the data looks like this:\nmaryland_winred_contributions &lt;- read_rds(\"maryland_winred.rds\")\nLet’s unpack that.\nThe first part – maryland_winred_contributions – is the name of a variable.\nA variable is just a name that we’ll use to refer to some more complex thing. In this case, the more complex thing is the data we’re importing into R that will be stored as a dataframe, which is one way R stores data.\nWe can call this variable whatever we want. The variable name doesn’t matter, technically. We could use any word. You could use your first name, if you like. Generally, though, we want to give variables names that are descriptive of the thing they refer to. Which is why we’re calling this one maryland_winred_contributions. Variable names, by convention are one word all lower case (or two or more words connected by an underscore). You can end a variable with a number, but you can’t start one with a number.\nThe &lt;- bit, you’ll recall from the basics, is the variable assignment operator. It’s how we know we’re assigning something to a word. Think of the arrow as saying “Take everything on the right of this arrow and stuff it into the thing on the left.” So we’re creating an empty vessel called maryland_winred_contributions and stuffing all this data into it.\nread_rds() is a function, one that only works when we’ve loaded the tidyverse. A function is a little bit of computer code that takes in information and follows a series of pre-determined steps and spits it back out. A recipe to make pizza is a kind of function. We might call it make_pizza().\nThe function does one thing. It takes a preset collection of ingredients – flour, water, oil, cheese, tomato, salt – and passes them through each step outlined in a recipe, in order. Things like: mix flour and water and oil, knead, let it sit, roll it out, put tomato sauce and cheese on it, bake it in an oven, then take it out.\nThe output of our make pizza() function is a finished pie.\nWe’ll make use of a lot of pre-written functions from the tidyverse and other packages, and even write some of our own. Back to this line of code:\nmaryland_winred_contributions &lt;- read_rds(\"maryland_winred.rds\")\nInside of the read_rds() function, we’ve put the name of the file we want to load. Things we put inside of function, to customize what the function does, are called arguments.\nThe easiest thing to do, if you are confused about how to find your data, is to put your data in the same folder as as your notebook (you’ll have to save that notebook first). If you do that, then you just need to put the name of the file in there (maryland_winred.rds). If you’re loading that file from the Internet, your function would instead look like this:\n\nmaryland_winred_contributions &lt;- read_rds(\"https://thescoop.org/files/maryland_winred.rds\")\n\nIn this data set, each row represents an individual contribution to a federal political committee, typically a candidate’s campaign account.\nAfter loading the data, it’s a good idea to get a sense of its shape. What does it look like? There are several ways we can examine it.\nBy looking in the R Studio environment window, we can see the number of rows (called “obs.”, which is short for observations), and the number of columns(called variables). We can double click on the dataframe name in the environment window, and explore it like a spreadsheet.\nThere are several useful functions for getting a sense of the dataset right in our markdown document.\nIf we run glimpse(maryland_winred_contributions), it will give us a list of the columns, the data type for each column and and the first few values for each column.\n\nglimpse(maryland_winred_contributions)\n\nRows: 131,395\nColumns: 24\n$ linenumber       &lt;chr&gt; \"SA11AI\", \"SA11AI\", \"SA11AI\", \"SA11AI\", \"SA11AI\", \"SA…\n$ fec_committee_id &lt;chr&gt; \"C00694323\", \"C00694323\", \"C00694323\", \"C00694323\", \"…\n$ tran_id          &lt;chr&gt; \"A000BA09B6F8D45FCBA7\", \"A0011063AFC5B47B2AE6\", \"A001…\n$ flag_orgind      &lt;chr&gt; \"IND\", \"IND\", \"IND\", \"IND\", \"IND\", \"IND\", \"IND\", \"IND…\n$ org_name         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ last_name        &lt;chr&gt; \"Curro\", \"Mukai\", \"Smith\", \"SaylorJones\", \"Gillissen\"…\n$ first_name       &lt;chr&gt; \"Peter\", \"Peggy\", \"Alan\", \"Jean\", \"Troy\", \"Bryan\", \"K…\n$ middle_name      &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ prefix           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ suffix           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ address_one      &lt;chr&gt; \"1902 Blakewood Ct\", \"729 Fox Bow Dr\", \"308 Troon Cir…\n$ address_two      &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ city             &lt;chr&gt; \"Fallston\", \"Bel Air\", \"Mount Airy\", \"Gaithersburg\", …\n$ state            &lt;chr&gt; \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\",…\n$ zip              &lt;chr&gt; \"21047\", \"21014\", \"21771\", \"20877\", \"20695\", \"21228\",…\n$ prigen           &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ date             &lt;date&gt; 2024-04-04, 2024-04-04, 2024-04-02, 2024-04-05, 2024…\n$ amount           &lt;dbl&gt; 5.21, 17.76, 26.03, 1.67, 5.21, 75.00, 0.99, 5.21, 2.…\n$ aggregate_amount &lt;dbl&gt; 10.21, 1221.77, 3884.66, 23.38, 11.21, 218.00, 29.33,…\n$ employer         &lt;chr&gt; \"Woodfield outdoors\", \"RETIRED\", \"RETIRED\", \"RETIRED\"…\n$ occupation       &lt;chr&gt; \"BUSINESS DEVELOPMENT\", \"RETIRED\", \"RETIRED\", \"RETIRE…\n$ memo_code        &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ memo_text        &lt;chr&gt; \"Earmarked for TRUMP NATIONAL COMMITTEE JFC (C0087389…\n$ cycle            &lt;dbl&gt; 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024, 2024,…\n\n\nIf we type head(maryland_winred_contributions), it will print out the columns and the first six rows of data.\n\nhead(maryland_winred_contributions)\n\n# A tibble: 6 × 24\n  linenumber fec_committee_id tran_id  flag_orgind org_name last_name first_name\n  &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     \n1 SA11AI     C00694323        A000BA0… IND         &lt;NA&gt;     Curro     Peter     \n2 SA11AI     C00694323        A001106… IND         &lt;NA&gt;     Mukai     Peggy     \n3 SA11AI     C00694323        A0011E5… IND         &lt;NA&gt;     Smith     Alan      \n4 SA11AI     C00694323        A001C3D… IND         &lt;NA&gt;     SaylorJo… Jean      \n5 SA11AI     C00694323        A00219C… IND         &lt;NA&gt;     Gillissen Troy      \n6 SA11AI     C00694323        A002A2E… IND         &lt;NA&gt;     Lally     Bryan     \n# ℹ 17 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;chr&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   prigen &lt;lgl&gt;, date &lt;date&gt;, amount &lt;dbl&gt;, aggregate_amount &lt;dbl&gt;,\n#   employer &lt;chr&gt;, occupation &lt;chr&gt;, memo_code &lt;lgl&gt;, memo_text &lt;chr&gt;,\n#   cycle &lt;dbl&gt;\n\n\nWe can also click on the data name in the R Studio environment window to explore it interactively.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Aggregates</span>"
    ]
  },
  {
    "objectID": "aggregates.html#group-by-and-count",
    "href": "aggregates.html#group-by-and-count",
    "title": "10  Aggregates",
    "section": "10.3 Group by and count",
    "text": "10.3 Group by and count\nSo what if we wanted to know how many contributions went to each recipient?\nTo do that by hand, we’d have to take each of the 133,363 individual rows (or observations or records) and sort them into a pile. We’d put them in groups – one for each recipient – and then count them.\ndplyr has a group by function in it that does just this. A massive amount of data analysis involves grouping like things together and then doing simple things like counting them, or averaging them together. So it’s a good place to start.\nSo to do this, we’ll take our dataset and we’ll introduce a new operator: |&gt;. The best way to read that operator, in my opinion, is to interpret that as “and then do this.”\nWe’re going to establish a pattern that will come up again and again throughout this book: data |&gt; function. In English: take your data set and then do this specific action to it.\nThe first step of every analysis starts with the data being used. Then we apply functions to the data.\nIn our case, the pattern that you’ll use many, many times is: data |&gt; group_by(COLUMN NAME) |&gt; summarize(VARIABLE NAME = AGGREGATE FUNCTION(COLUMN NAME))\nIn our dataset, the column with recipient information is called “memo_text”\nHere’s the code to count the number of contributions to each recipient:\n\nmaryland_winred_contributions |&gt;\n  group_by(memo_text) |&gt;\n  summarise(\n    count_contribs = n()\n  )\n\n# A tibble: 481 × 2\n   memo_text                                            count_contribs\n   &lt;chr&gt;                                                         &lt;int&gt;\n 1 Earmarked for AARON BEAN FOR CONGRESS (C00816983)                 1\n 2 Earmarked for AARON DIMMOCK FOR CONGRESS (C00877225)              6\n 3 Earmarked for ABE FOR ARIZONA (C00853986)                       178\n 4 Earmarked for ADAM MORGAN FOR CONGRESS (C00857060)               58\n 5 Earmarked for ADRIAN SMITH FOR CONGRESS (C00412890)               1\n 6 Earmarked for ALABAMA FIRST PAC (C00821058)                       3\n 7 Earmarked for ALAMO PAC (C00387464)                               2\n 8 Earmarked for ALASKANS FOR DAN SULLIVAN (C00570994)               3\n 9 Earmarked for ALASKANS FOR NICK BEGICH (C00792341)                8\n10 Earmarked for ALEX FOR NORTH DAKOTA (C00873927)                   2\n# ℹ 471 more rows\n\n\nSo let’s walk through that.\nWe start with our dataset – maryland_winred_contributions – and then we tell it to group the data by a given field in the data. In this case, we wanted to group together all the recipients, signified by the field name memo_text, which you could get from using the glimpse() function. After we group the data, we need to count them up.\nIn dplyr, we use the summarize() function, which can do alot more than just count things.\nInside the parentheses in summarize, we set up the summaries we want. In this case, we just want a count of the number of loans for each county grouping. The line of code count_contribs = n(), says create a new field, called count_contribs and set it equal to n(). n() is a function that counts the number of rows or records in each group. Why the letter n? The letter n is a common symbol used to denote a count of something. The number of things (or rows or observations or records) in a dataset? Statisticians call it n. There are n number of contributions in this dataset.\nWhen we run that, we get a list of recipients with a count next to them. But it’s not in any order.\nSo we’ll add another “and then do this” symbol – |&gt; – and use a new function called arrange(). Arrange does what you think it does – it arranges data in order. By default, it’s in ascending order – smallest to largest. But if we want to know the county with the most loans, we need to sort it in descending order. That looks like this:\n\nmaryland_winred_contributions |&gt;\n  group_by(memo_text) |&gt;\n  summarise(\n    count_contribs = n()\n  ) |&gt;\n  arrange(desc(count_contribs))\n\n# A tibble: 481 × 2\n   memo_text                                                      count_contribs\n   &lt;chr&gt;                                                                   &lt;int&gt;\n 1 Earmarked for TRUMP NATIONAL COMMITTEE JFC, INC. (C00873893)            41835\n 2 Earmarked for TRUMP SAVE AMERICA JOINT FUNDRAISING COMMITTEE …          10812\n 3 Earmarked for NRSC (C00027466)                                           9063\n 4 Earmarked for HOGAN FOR MARYLAND INC. (C00869016)                        5754\n 5 Earmarked for NRCC (C00075820)                                           4416\n 6 Earmarked for TEAM SCALISE (C00750521)                                   3184\n 7 Earmarked for REPUBLICAN NATIONAL COMMITTEE (C00003418)                  3063\n 8 Earmarked for TRUMP NATIONAL COMMITTEE JFC (C00873893)                   3042\n 9 Earmarked for MIKE JOHNSON FOR LOUISIANA (C00608695)                     2811\n10 Earmarked for TEAM ELISE (C00830679)                                     2389\n# ℹ 471 more rows\n\n\nThe Trump National Committee JFC Inc. has 41,835 contributions, more than any other recipient.\nWe can, if we want, group by more than one thing.\nThe WinRed data contains a column detailing the date of the contribution: “date”.\nWe can group by “memo_text” and “date” to see how many contributions occurred on every date to every recipient. We’ll sort by the count of contributions in descending order.\n\nmaryland_winred_contributions |&gt;\n  group_by(memo_text, date) |&gt;\n  summarise(\n    count_contribs = n()\n  ) |&gt;\n  arrange(desc(count_contribs))\n\n`summarise()` has grouped output by 'memo_text'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 12,935 × 3\n# Groups:   memo_text [481]\n   memo_text                                           date       count_contribs\n   &lt;chr&gt;                                               &lt;date&gt;              &lt;int&gt;\n 1 Earmarked for TRUMP NATIONAL COMMITTEE JFC, INC. (… 2024-05-31           5629\n 2 Earmarked for TRUMP NATIONAL COMMITTEE JFC, INC. (… 2024-05-30           4845\n 3 Earmarked for TRUMP NATIONAL COMMITTEE JFC, INC. (… 2024-06-01           1681\n 4 Earmarked for TRUMP NATIONAL COMMITTEE JFC, INC. (… 2024-06-27           1228\n 5 Earmarked for TRUMP NATIONAL COMMITTEE JFC, INC. (… 2024-06-02           1178\n 6 Earmarked for TRUMP NATIONAL COMMITTEE JFC, INC. (… 2024-06-03            939\n 7 Earmarked for TRUMP SAVE AMERICA JOINT FUNDRAISING… 2024-05-31            863\n 8 Earmarked for TRUMP NATIONAL COMMITTEE JFC, INC. (… 2024-06-30            832\n 9 Earmarked for TRUMP SAVE AMERICA JOINT FUNDRAISING… 2024-05-30            830\n10 Earmarked for TRUMP NATIONAL COMMITTEE JFC, INC. (… 2024-04-15            710\n# ℹ 12,925 more rows\n\n\nOk, now go and find out why contributions to Trump would be exponentially higher at the end of May.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Aggregates</span>"
    ]
  },
  {
    "objectID": "aggregates.html#other-summarization-methods-summing-mean-median-min-and-max",
    "href": "aggregates.html#other-summarization-methods-summing-mean-median-min-and-max",
    "title": "10  Aggregates",
    "section": "10.4 Other summarization methods: summing, mean, median, min and max",
    "text": "10.4 Other summarization methods: summing, mean, median, min and max\nIn the last example, we grouped like records together and counted them, but there’s so much more we can to summarize each group.\nLet’s say we wanted to know the total dollar amount of contributions to each recipient? For that, we could use the sum() function to add up all of the loan values in the column “amount”. We put the column we want to total – “amount” – inside the sum() function sum(amount). Note that we can simply add a new summarize function here, keeping our count_contribs field in our output table.\n\nmaryland_winred_contributions |&gt;\n  group_by(memo_text) |&gt;\n  summarise(\n    count_contribs = n(),\n    total_amount = sum(amount)\n  ) |&gt;\n  arrange(desc(total_amount))\n\n# A tibble: 481 × 3\n   memo_text                                         count_contribs total_amount\n   &lt;chr&gt;                                                      &lt;int&gt;        &lt;dbl&gt;\n 1 Earmarked for TRUMP NATIONAL COMMITTEE JFC, INC.…          41835     1735741.\n 2 Earmarked for HOGAN FOR MARYLAND INC. (C00869016)           5754      479170.\n 3 Earmarked for NRSC (C00027466)                              9063      245778.\n 4 Earmarked for TRUMP SAVE AMERICA JOINT FUNDRAISI…          10812      197486.\n 5 Earmarked for NRCC (C00075820)                              4416      117003.\n 6 Earmarked for PARROTT FOR CONGRESS (C00691931)               504      115467.\n 7 Earmarked for REPUBLICAN NATIONAL COMMITTEE (C00…           3063       89033.\n 8 Earmarked for TRUMP NATIONAL COMMITTEE JFC (C008…           3042       71084.\n 9 Earmarked for TEAM SCALISE (C00750521)                      3184       61450.\n10 Earmarked for TED CRUZ FOR SENATE (C00492785)               2187       33310.\n# ℹ 471 more rows\n\n\nWe can also calculate the average amount for each recipient – the mean – and the amount that sits at the midpoint of our data – the median.\n\nmaryland_winred_contributions |&gt;\n  group_by(memo_text) |&gt;\n  summarise(\n    count_contribs = n(),\n    total_amount = sum(amount),\n    mean_amount = mean(amount),\n    median_amount = median(amount)\n  ) |&gt;\n  arrange(desc(count_contribs))\n\n# A tibble: 481 × 5\n   memo_text               count_contribs total_amount mean_amount median_amount\n   &lt;chr&gt;                            &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n 1 Earmarked for TRUMP NA…          41835     1735741.       41.5          20.2 \n 2 Earmarked for TRUMP SA…          10812      197486.       18.3           5   \n 3 Earmarked for NRSC (C0…           9063      245778.       27.1          10   \n 4 Earmarked for HOGAN FO…           5754      479170.       83.3          26.0 \n 5 Earmarked for NRCC (C0…           4416      117003.       26.5          15   \n 6 Earmarked for TEAM SCA…           3184       61450.       19.3          10   \n 7 Earmarked for REPUBLIC…           3063       89033.       29.1          10   \n 8 Earmarked for TRUMP NA…           3042       71084.       23.4           7.88\n 9 Earmarked for MIKE JOH…           2811       25322.        9.01          2   \n10 Earmarked for TEAM ELI…           2389       31282.       13.1           4.75\n# ℹ 471 more rows\n\n\nWe see something interesting here. The mean contribution amount is higher than the median amount in most cases, but the difference isn’t huge. In some cases the mean gets skewed by larger amounts. Examining both the median – which is less sensitive to extreme values – and the mean – which is more sensitive to extreme values – gives you a clearer picture of the composition of the data.\nWhat about the highest and lowest amounts for each recipient? For that, we can use the min() and max() functions.\n\nmaryland_winred_contributions |&gt;\n  group_by(memo_text) |&gt;\n  summarise(\n    count_contribs = n(),\n    total_amount = sum(amount),\n    mean_amount = mean(amount),\n    median_amount = median(amount),\n    min_amount = min(amount),\n    max_amount = max(amount)\n  ) |&gt;\n  arrange(desc(max_amount))\n\n# A tibble: 481 × 7\n   memo_text    count_contribs total_amount mean_amount median_amount min_amount\n   &lt;chr&gt;                 &lt;int&gt;        &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n 1 Earmarked f…              8       28162.      3520.         799.       200   \n 2 Earmarked f…          41835     1735741.        41.5         20.2        0.01\n 3 Earmarked f…           5754      479170.        83.3         26.0        1   \n 4 Earmarked f…            145       11723.        80.9          5.21       0.05\n 5 Earmarked f…           1189       23238.        19.5          5          0.01\n 6 Earmarked f…              2        6625       3312.        3312.        25   \n 7 Earmarked f…            446       12958.        29.1          5          0.01\n 8 Earmarked f…              3        5050       1683.          25         25   \n 9 Earmarked f…              3        5552.      1851.         500         52.0 \n10 Earmarked f…              3        6000       2000          500        500   \n# ℹ 471 more rows\n# ℹ 1 more variable: max_amount &lt;dbl&gt;\n\n\nFrom this, we can see that some committees focus on small-dollar donors while others ask for (and get) larger amounts. This pattern isn’t random: campaigns make choices about how they will raise money.\nIt would be interesting to see what the largest donation was. To do that, we could simply take our original data set and sort it from highest to lowest on the amount.\n\nmaryland_winred_contributions |&gt;\n  arrange(desc(amount))\n\n# A tibble: 131,395 × 24\n   linenumber fec_committee_id tran_id flag_orgind org_name last_name first_name\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     \n 1 SA11AI     C00694323        AEB164… IND         &lt;NA&gt;     Boland    John      \n 2 SA11AI     C00694323        AED8E1… IND         &lt;NA&gt;     Scully    Finbar    \n 3 SA11AI     C00694323        A598DB… IND         &lt;NA&gt;     Morgan    Kenneth   \n 4 SA11AI     C00694323        ABDDBD… IND         &lt;NA&gt;     Jacobsen  Mark      \n 5 SA11AI     C00694323        A6AE2F… IND         &lt;NA&gt;     Jacobsen  Mark      \n 6 SA11AI     C00694323        AA7420… IND         &lt;NA&gt;     Jacobsen  Mark      \n 7 SA11AI     C00694323        A05F19… IND         &lt;NA&gt;     DETERMAN  MARK      \n 8 SA11AI     C00694323        AFB4D6… IND         &lt;NA&gt;     Wheeler   Rex       \n 9 SA11AI     C00694323        A0490D… IND         &lt;NA&gt;     Gallagher Daniel    \n10 SA11AI     C00694323        A2EF52… IND         &lt;NA&gt;     Chaney    Bryan     \n# ℹ 131,385 more rows\n# ℹ 17 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;chr&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   prigen &lt;lgl&gt;, date &lt;date&gt;, amount &lt;dbl&gt;, aggregate_amount &lt;dbl&gt;,\n#   employer &lt;chr&gt;, occupation &lt;chr&gt;, memo_code &lt;lgl&gt;, memo_text &lt;chr&gt;,\n#   cycle &lt;dbl&gt;\n\n\nOnly two contributions of $10,000 or more. That’s because the maximum contribution an individual can give for both a primary and a general election in this cycle is $3,300, but campaigns have to report whatever they are given, no matter how large (they likely will refund the difference).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Aggregates</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "",
    "text": "1 Introduction\nIf you were at all paying attention in pre-college science classes, you have probably seen this equation:\nIn English, that says we can know how far something has traveled if we know how fast it’s going and for how long. If we multiply the rate by the time, we’ll get the distance.\nIf you remember just a bit about algebra, you know we can move these things around. If we know two of them, we can figure out the third. So, for instance, if we know the distance and we know the time, we can use algebra to divide the distance by the time to get the rate.\nIn 2012, the South Florida Sun Sentinel found a story in this formula.\nPeople were dying on South Florida tollways in terrible car accidents. What made these different from other car fatal car accidents that happen every day in the US? Police officers driving way too fast were causing them.\nBut do police regularly speed on tollways or were there just a few random and fatal exceptions?\nThanks to Florida’s public records laws, the Sun Sentinel got records from the toll transponders in police cars in south Florida. The transponders recorded when a car went through a given place. And then it would do it again. And again.\nGiven that those places are fixed – they’re toll plazas – and they had the time it took to go from one toll plaza to another, they had the distance and the time.\nIt took high school algebra to find how fast police officers were driving. And the results were shocking.\nTwenty percent of police officers had exceeded 90 miles per hour on toll roads. In a 13-month period, officers drove between 90 and 110 mph more than 5,000 times. And these were just instances found on toll roads. Not all roads have tolls.\nThe story was a stunning find, and the newspaper documented case after case of police officers violating the law and escaping punishment. And, in 2013, they won the Pulitzer Prize for Public Service.\nAll with simple high school algebra.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#modern-data-journalism",
    "href": "index.html#modern-data-journalism",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "1.1 Modern data journalism",
    "text": "1.1 Modern data journalism\nIt’s a single word in a single job description, but a Buzzfeed job posting in 2017 is another indicator in what could be a profound shift in how data journalism is both practiced and taught.\n“We’re looking for someone with a passion for news and a commitment to using data to find amazing, important stories — both quick hits and deeper analyses that drive conversations,” the posting seeking a data journalist says. It goes on to list five things BuzzFeed is looking for: Excellent collaborator, clear writer, deep statistical understanding, knowledge of obtaining and restructuring data.\nAnd then there’s this:\n“You should have a strong command of at least one toolset that (a) allows for filtering, joining, pivoting, and aggregating tabular data, and (b) enables reproducible workflows.”\nThis is not the data journalism of 20 years ago. When it started, it was a small group of people in newsrooms using spreadsheets and databases. Data journalism now encompases programming for all kinds of purposes, product development, user interface design, data visualization and graphics on top of more traditional skills like analyzing data and writing stories.\nIn this book, you’ll get a taste of modern data journalism through programming in R, a statistics language. You’ll be challenged to think programmatically while thinking about a story you can tell to readers in a way that they’ll want to read. They might seem like two different sides of the brain – mutually exclusive skills. They aren’t. I’m confident you’ll see programming is a creative endeavor and storytelling can be analytical.\nCombining them together has the power to change policy, expose injustice and deeply inform.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#installations",
    "href": "index.html#installations",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "1.2 Installations",
    "text": "1.2 Installations\nThis book is all in the R statistical language. To follow along, you’ll go here and download and install both R the language and RStudio.\nGoing forward, you’ll see passages like this:\n\ninstall.packages(\"tidyverse\")\n\nThat is code that you’ll need to run in your RStudio. When you see that, you’ll know what to do.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "1.3 About this book",
    "text": "1.3 About this book\nThis book is the collection of class materials originally written for Matt Waite’s Data Journalism class at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. It has been substantially updated by Derek Willis and Sean Mussenden for data journalism classes at the University of Maryland Philip Merrill College of Journalism, with contributions from Sarah Cohen of Arizona State University.\nThere’s some things you should know about it:\n\nIt is free for students.\nThe topics will remain the same but the text is going to be constantly tinkered with.\nWhat is the work of the authors is copyright Matt Waite 2020, Sarah Cohen 2022 and Derek Willis, Daniel Trielli and Sean Mussenden 2024.\nThe text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either.\n\nAs such, the whole book – authored in Quarto – in its original form is open sourced on Github. Pull requests welcomed!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "index.html#what-well-cover",
    "href": "index.html#what-well-cover",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "1.4 What we’ll cover",
    "text": "1.4 What we’ll cover\n\nPublic records and open data\nR Basics\nReplication\nData basics and structures\nAggregates\nMutating\nWorking with dates\nFilters\nCleaning I: Data smells\nCleaning II: Janitor\nCleaning III: Open Refine\nCleaning IV: Pulling Data from PDFs\nJoins\nBasic data scraping\nGetting data from APIs: Census\nVisualizing for reporting: Basics\nVisualizing for reporting: Publishing\nGeographic data basics\nGeographic queries\nGeographic visualization\nText analysis basics\nWriting with and about data\nData journalism ethics\nAI & Data journalism",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "7  Setting Up Your Computer To Use GitHub",
    "section": "",
    "text": "7.1 Space, the Final Frontier\nYou will need to have a substantial amount of hard drive space to store the files for this class. If you are not in the practice of deleting files because “everything’s in the cloud” anyway, you will need to clear out some space on your laptop. How much space? The more, the better, but let’s go with at least 10GB.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Setting Up Your Computer To Use GitHub</span>"
    ]
  },
  {
    "objectID": "github.html#github",
    "href": "github.html#github",
    "title": "7  Setting Up Your Computer To Use GitHub",
    "section": "7.2 GitHub",
    "text": "7.2 GitHub\nGitHub is a platform for managing and storing files, data and code built atop Git, a popular open source version control software. GitHub accounts are free and it’s easy to get started. The one prerequisite is that you have Git installed on your local computer. There are installers for Mac, Windows and Linux. Install Git first.\n\n7.2.1 How GitHub Works\nVersion control is based on the ideas that you want to keep track of changes you make to a collection of files and that multiple people can work together without getting in each other’s way or having to do things in a set order. For individual users, it’s great for making sure that you always have your work.\nGitHub users work in what are known as repositories on their local computers and also push changes to a remote repository located on GitHub. That remote repository is key: if you lose your computer, you can fetch a version of your files from GitHub. If you want to work with someone else on the same files, you can each have a local copy, push changes to GitHub and then pull each others’ changes back to your local computers.\nSo, like Microsoft Word’s track changes, but with a remote backup and multiple editors.\n\n\n7.2.2 Getting Started with GitHub\nAfter installing Git and signing up for a GitHub account, download and install GitHub Desktop. It will have you sign into your GitHub account and then you’ll have access to any existing repositories. If you don’t have any, that’s fine! You can make one locally.\nGitHub has good documentation for working in the Desktop app, and while the emphasis in this book will be on using GitHub for version control, it also supports recording issues (read: problems or questions) with your files, contributing to projects that aren’t yours and more.\n\n\n7.2.3 A Place for Your Stuff\nWe all will be working in the same way, and every student will have a repository for class assignments. Go here and click the “Use this template” button and choose “Create a new repository”. This will create a copy of the repository in your GitHub account. Give it a name that makes sense, like data_journalism_2024_fall. Your work will occur in that repository and you will submit links to individual documents. Pro tip: do not put assignment files outside your repository.\nYou will clone your repository from GitHub.com to your laptop. You can sync it to multiple machines as long as you have GitHub Desktop (or the command line version) installed. The iMacs in the classroom have both, so you can use them during class if needed.\nYou can store your repository anywhere on your computer, but my STRONG RECOMMENDATION is that you NOT store it in your Downloads or Desktop folders. Instead, make a new folder at the root (or home) directory and put your files there. Why? Because many folks have their Download and Desktop folders as part of their iCloud or OneNote setups, and Git DOES NOT LIKE THAT. Unless you want to spend time troubleshooting seemingly inexplicable problems, avoid those folders.\nAlso, maybe clean up those Download and Desktop folders.\n\n\n7.2.4 Local vs. GitHub\nGitHub works on a system of pulling and pushing changes to files between what you have on your computer and what is on your GitHub.com account. Here’s how you should work:\nWhen you are working locally, before you start you should pull from GitHub.com to make sure you have the latest version of your files. When you finish, you should save and add, commit and push any changes/additions to GitHub.com. You can add/commit/push as often as you like, and there’s reason not to do it often.\nThe nature of Git and GitHub means that you can have a copy of your files on the Web and on your computer. You can work offline and then update your file when you connect to the Internet again. When submitting your assignments, you need to submit the full URL to your assignment’s file from GitHub.com.\n\n\n7.2.5 GitHub Tips\nIn your class repository you will see various files that begin with a period, such as .gitignore. These are essentially helper files that contain metadata or instructions that tell Git and RStudio what to do and not do. For example, .gitignore contains a list of files and file types that should NOT be under version control. Maybe you have a big data file (GitHub doesn’t really deal well with files of more than 50MB) or you have files containing credentials that you don’t want on the Internet. Those files can still be in your local folder, but they won’t be added to your GitHub repository.\n\n\n7.2.6 Advanced GitHub Use\nAlthough our focus is on the GitHub Desktop app, you can use Git and GitHub from your computer’s command line interface, and GitHub has a purpose-built command line client, too. GitHub can also serve as a publishing platform for many types of files, and entire websites are hosted on GitHub Pages.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Setting Up Your Computer To Use GitHub</span>"
    ]
  },
  {
    "objectID": "mutating.html",
    "href": "mutating.html",
    "title": "11  Mutating data",
    "section": "",
    "text": "11.1 Another use of mutate\nMutate is also useful for standardizing data - for example, making different spellings of, say, cities into a single one.\nLet’s load some campaign contribution data - in this case Maryland donors to Republican committees via WinRed’s online platform earlier this year - and take a look at the city column in our data.\nmaryland_cities &lt;- read_csv(\"data/winred_md_cities.csv\")\n\nRows: 469 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): city\ndbl (2): count, sum\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nYou’ll notice that there’s a mix of styles: “Baltimore” and “BALTIMORE” for example. R will think those are two different cities, and that will mean that any aggregates we create based on city won’t be accurate.\nSo how can we fix that? Mutate - it’s not just for math! And a function called str_to_upper that will convert a character column into all uppercase. Now we can say exactly how many donations came from Baltimore (I mean, of course, BALTIMORE).\nstandardized_maryland_cities &lt;- maryland_cities |&gt;\n  mutate(\n    upper_city = str_to_upper(city)\n)\nNote that mutate doesn’t literally combine similar records together - you’ll still need to do another group_by and summarize block - but it does make your data more accurate. There are lots of potential uses for standardization - addresses, zip codes, anything that can be misspelled or abbreviated.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mutating data</span>"
    ]
  },
  {
    "objectID": "mutating.html#a-more-powerful-use",
    "href": "mutating.html#a-more-powerful-use",
    "title": "11  Mutating data",
    "section": "11.2 A more powerful use",
    "text": "11.2 A more powerful use\nMutate is even more useful when combined with some additional functions. Let’s focus on individual contributions from Maryland donors via WinRed; we’d like to group their donations by amount into one of four categories:\n\nUnder $100\n$101-$499\n$500-$1,499\n$1,500-$2,999\nMore than $2,999\n\nMutate can make that happen by creating a new column and putting in a category value based on the amount of each record. First, let’s load the individual contributions that we did in the previous chapter:\n\nmaryland_winred_contributions &lt;- read_rds(\"data/maryland_winred.rds\")\n\nhead(maryland_winred_contributions)\n\n# A tibble: 6 × 24\n  linenumber fec_committee_id tran_id  flag_orgind org_name last_name first_name\n  &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     \n1 SA11AI     C00694323        A000BA0… IND         &lt;NA&gt;     Curro     Peter     \n2 SA11AI     C00694323        A001106… IND         &lt;NA&gt;     Mukai     Peggy     \n3 SA11AI     C00694323        A0011E5… IND         &lt;NA&gt;     Smith     Alan      \n4 SA11AI     C00694323        A001C3D… IND         &lt;NA&gt;     SaylorJo… Jean      \n5 SA11AI     C00694323        A00219C… IND         &lt;NA&gt;     Gillissen Troy      \n6 SA11AI     C00694323        A002A2E… IND         &lt;NA&gt;     Lally     Bryan     \n# ℹ 17 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;chr&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   prigen &lt;lgl&gt;, date &lt;date&gt;, amount &lt;dbl&gt;, aggregate_amount &lt;dbl&gt;,\n#   employer &lt;chr&gt;, occupation &lt;chr&gt;, memo_code &lt;lgl&gt;, memo_text &lt;chr&gt;,\n#   cycle &lt;dbl&gt;\n\n\nNow that we’ve gotten a look, we can use case_when to give our new category column a value using some standard numeric logic:\n\nmaryland_winred_categories &lt;- maryland_winred_contributions |&gt;\n  mutate(\n    amount_category = case_when(\n        amount &lt; 100 ~ \"Less than $100\",\n        amount &gt;= 100 & amount &lt; 500 ~ \"Between $100 and $499\",\n        amount &gt;= 500 & amount &lt; 1500 ~ \"Between $500 and $1499\",\n        amount &gt;= 1500 & amount &lt; 3000 ~ \"Between $500 and $2999\",\n        amount &gt;= 3000 ~ \"$3,000 or more\"\n      )\n  )\nhead(maryland_winred_categories)\n\n# A tibble: 6 × 25\n  linenumber fec_committee_id tran_id  flag_orgind org_name last_name first_name\n  &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;    &lt;chr&gt;       &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     \n1 SA11AI     C00694323        A000BA0… IND         &lt;NA&gt;     Curro     Peter     \n2 SA11AI     C00694323        A001106… IND         &lt;NA&gt;     Mukai     Peggy     \n3 SA11AI     C00694323        A0011E5… IND         &lt;NA&gt;     Smith     Alan      \n4 SA11AI     C00694323        A001C3D… IND         &lt;NA&gt;     SaylorJo… Jean      \n5 SA11AI     C00694323        A00219C… IND         &lt;NA&gt;     Gillissen Troy      \n6 SA11AI     C00694323        A002A2E… IND         &lt;NA&gt;     Lally     Bryan     \n# ℹ 18 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;chr&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   prigen &lt;lgl&gt;, date &lt;date&gt;, amount &lt;dbl&gt;, aggregate_amount &lt;dbl&gt;,\n#   employer &lt;chr&gt;, occupation &lt;chr&gt;, memo_code &lt;lgl&gt;, memo_text &lt;chr&gt;,\n#   cycle &lt;dbl&gt;, amount_category &lt;chr&gt;\n\n\nWe can then use our new amount_category column in group_by statements to make summarizing easier:\n\nmaryland_winred_categories |&gt;\n  group_by(amount_category) |&gt;\n  summarize(total_amount = sum(amount)) |&gt;\n  arrange(desc(total_amount))\n\n# A tibble: 5 × 2\n  amount_category        total_amount\n  &lt;chr&gt;                         &lt;dbl&gt;\n1 Less than $100             1716339.\n2 Between $100 and $499      1256984.\n3 Between $500 and $1499      689883.\n4 $3,000 or more              365885.\n5 Between $500 and $2999      140922.\n\n\nThe largest category - by far - in dollar amount is the sub-$100 category, which makes sense for an online fundraising platform. Big little money.\nMutate is there to make your data more useful and to make it easier for you to ask more and better questions of it.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Mutating data</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-i.html",
    "href": "data-cleaning-part-i.html",
    "title": "14  Data Cleaning Part I: Data smells",
    "section": "",
    "text": "14.1 Wrong Type\nFirst, let’s look at Wrong Type Of Data.\nWe can sniff that out by looking at the output of readr.\nLet’s load the tidyverse.\n# Remove scientific notation\noptions(scipen=999)\n# Load the tidyverse\nlibrary(tidyverse)\nThen let’s load some precinct-level election results data from Texas for the 2020 general election.\nThis time, we’re going to load the data in a CSV format, which stands for comma separated values and is essentially a fancy structured text file. Each column in the csv is separated – “delimited” – by a comma from the next column.\nWe’re also going to introduce a new argument to our function that reads in the data, read_csv(), called “guess_max”. As R reads in the csv file, it will attempt to make some calls on what “data type” to assign to each field: number, character, date, and so on. The “guess_max” argument says: look at the values in the whatever number of rows we specify before deciding which data type to assign. In this case, we’ll pick 10.\n# Load the data\ntexas_precinct_20 &lt;- read_csv(\"data/tx_precinct_2020.csv\", guess_max=10)\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 476915 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): county, precinct, office, candidate, party\ndbl (6): district, votes, absentee, election_day, early_voting, mail\nlgl (2): provisional, limited\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nPay attention to the red warning that signals “one or more parsing issues.” It advises us to run the problems() function to see what went wrong. Let’s do that.\nproblems(texas_precinct_20)\n\n# A tibble: 1,640 × 5\n     row   col expected           actual file \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;  &lt;chr&gt;\n 1 28450    12 1/0/T/F/TRUE/FALSE 20     \"\"   \n 2 28450    13 1/0/T/F/TRUE/FALSE 21     \"\"   \n 3 28451    12 1/0/T/F/TRUE/FALSE 15     \"\"   \n 4 28451    13 1/0/T/F/TRUE/FALSE 3      \"\"   \n 5 28465    12 1/0/T/F/TRUE/FALSE 20     \"\"   \n 6 28465    13 1/0/T/F/TRUE/FALSE 20     \"\"   \n 7 28466    12 1/0/T/F/TRUE/FALSE 15     \"\"   \n 8 28466    13 1/0/T/F/TRUE/FALSE 3      \"\"   \n 9 28472    12 1/0/T/F/TRUE/FALSE 22     \"\"   \n10 28472    13 1/0/T/F/TRUE/FALSE 17     \"\"   \n# ℹ 1,630 more rows\nIt produces a table of all the parsing problems. It has 1,640 rows, which means we have that many problems. In almost every case here, the readr library has guessed that a given column was of a “logical” data type – True or False. It did it based on very limited information – only 1,000 rows. So, when it hit a value that looked like a date, or a character string, it didn’t know what to do. So it just didn’t read in that value correctly.\nThe easy way to fix this is to set the guess_max argument higher. It will take a little longer to load, but we’ll use every single row in the data set to guess the column type – 476,915\ntexas_precinct_20 &lt;- read_csv(\"data/tx_precinct_2020.csv\", guess_max=476915)\n\nRows: 476915 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): county, precinct, office, candidate, party, election_day\ndbl (5): district, absentee, mail, provisional, limited\nnum (2): votes, early_voting\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nThis time, we got no parsing failures. And if we examine the data types readr assigned to each column using glimpse(), they generally make sense.\nglimpse(texas_precinct_20)\n\nRows: 476,915\nColumns: 13\n$ county       &lt;chr&gt; \"Newton\", \"Newton\", \"Newton\", \"Newton\", \"Newton\", \"Newton…\n$ precinct     &lt;chr&gt; \"Box 1\", \"Box 1\", \"Box 1\", \"Box 1\", \"Box 1\", \"Box 1\", \"Bo…\n$ office       &lt;chr&gt; \"Registered Voters\", \"Ballots Cast\", \"President\", \"Presid…\n$ district     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ candidate    &lt;chr&gt; NA, NA, \"Donald J Trump\", \"Joseph R Biden\", \"Jo Jorgensen…\n$ party        &lt;chr&gt; NA, NA, \"REP\", \"DEM\", \"LBT\", \"GRE\", NA, NA, NA, NA, NA, N…\n$ votes        &lt;dbl&gt; 1003, 665, 557, 92, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 553,…\n$ absentee     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ election_day &lt;chr&gt; NA, \"141\", \"124\", \"11\", \"2\", \"2\", \"0\", \"0\", \"0\", \"0\", \"0\"…\n$ early_voting &lt;dbl&gt; NA, 524, 433, 81, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 430, 8…\n$ mail         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ provisional  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ limited      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\nThings that should be characters – like county, precinct, candidate – are characters (chr). Things that should be numbers (dbl) – like votes – are numbers.\nThere are some minor problems. The election_day column is a good example. It read in as a number (chr), even though there clearly are numbers in it judging from our initial inspection. Here’s why: the original file has a single value in that column that is “5+”.\ntexas_precinct_20 |&gt; filter(election_day == \"5+\")\n\n# A tibble: 1 × 13\n  county   precinct office  district candidate party votes absentee election_day\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 Anderson 11       Railro…       NA Chrysta … DEM     141       NA 5+          \n# ℹ 4 more variables: early_voting &lt;dbl&gt;, mail &lt;dbl&gt;, provisional &lt;dbl&gt;,\n#   limited &lt;dbl&gt;\nBecause this is just one result that’s weird, we can fix it by comparing the other votes Castaneda received in Anderson to the county totals for her. The difference should be what that “5+” value should be. I’ve done those calculations and it turns out that 49 is the actual likely value.\nWe can fix that pretty easily, by changing that value to “49” using case_when and then using mutate to make the entire column numeric.\ntexas_precinct_20 &lt;- texas_precinct_20 |&gt;\n  mutate(election_day = case_when(\n    election_day == '5+' ~ '49',\n    TRUE ~ election_day\n  ))\n\ntexas_precinct_20 &lt;- texas_precinct_20 |&gt; mutate(election_day = as.numeric(election_day))\nWhen we glimpse() the dataframe again, it’s been changed\nglimpse(texas_precinct_20)\n\nRows: 476,915\nColumns: 13\n$ county       &lt;chr&gt; \"Newton\", \"Newton\", \"Newton\", \"Newton\", \"Newton\", \"Newton…\n$ precinct     &lt;chr&gt; \"Box 1\", \"Box 1\", \"Box 1\", \"Box 1\", \"Box 1\", \"Box 1\", \"Bo…\n$ office       &lt;chr&gt; \"Registered Voters\", \"Ballots Cast\", \"President\", \"Presid…\n$ district     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ candidate    &lt;chr&gt; NA, NA, \"Donald J Trump\", \"Joseph R Biden\", \"Jo Jorgensen…\n$ party        &lt;chr&gt; NA, NA, \"REP\", \"DEM\", \"LBT\", \"GRE\", NA, NA, NA, NA, NA, N…\n$ votes        &lt;dbl&gt; 1003, 665, 557, 92, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 553,…\n$ absentee     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ election_day &lt;dbl&gt; NA, 141, 124, 11, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 123, 1…\n$ early_voting &lt;dbl&gt; NA, 524, 433, 81, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 430, 8…\n$ mail         &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ provisional  &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ limited      &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\nNow we’ve got numbers in the election_day column that we can add.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Cleaning Part I: Data smells</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-i.html#missing-data",
    "href": "data-cleaning-part-i.html#missing-data",
    "title": "14  Data Cleaning Part I: Data smells",
    "section": "14.2 Missing Data",
    "text": "14.2 Missing Data\nThe second smell we can find in code is missing data.\nWe can do that by grouping and counting columns. In addition to identifying the presence of NA values, this method will also give us a sense of the distribution of values in those columns.\nLet’s start with the “mail” column, which represents the number of votes a candidate received in a precinct from ballots cast by mail. The following code groups by the mail column, counts the number in each group, and then sorts from highest to lowest. There are 402,345 NA values in this column. This is most of our rows, so that should give us some pause. Either counties didn’t report votes by mail as a separate category or called it something else. This will impact how we can describe the data.\n\ntexas_precinct_20 |&gt;\n  group_by(mail) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  arrange(desc(count))\n\n# A tibble: 30 × 2\n    mail  count\n   &lt;dbl&gt;  &lt;int&gt;\n 1    NA 402345\n 2     0  71589\n 3     1   1708\n 4     2    646\n 5     3    277\n 6     4    125\n 7     5     79\n 8     6     35\n 9     7     21\n10     8     21\n# ℹ 20 more rows\n\n\nNow let’s try the “provisional” column, which represents the number of accepted provisional votes cast. In this case, there are 135,073 NA values. The rest have different dollar amounts.\n\ntexas_precinct_20 |&gt;\n  group_by(provisional) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  arrange(desc(count))\n\n# A tibble: 27 × 2\n   provisional  count\n         &lt;dbl&gt;  &lt;int&gt;\n 1          NA 473381\n 2           0   2836\n 3           1    210\n 4           2    105\n 5           3     69\n 6           5     53\n 7           4     44\n 8           6     44\n 9          10     29\n10           7     27\n# ℹ 17 more rows\n\n\nThe number of NA values - 473,381 - is even higher, which should give us confidence that most Texas counties did not report provisional votes at the precinct level. Like with mail votes, this helps define the constraints we have to work under with this data.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Cleaning Part I: Data smells</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-i.html#gaps-in-data",
    "href": "data-cleaning-part-i.html#gaps-in-data",
    "title": "14  Data Cleaning Part I: Data smells",
    "section": "14.3 Gaps in data",
    "text": "14.3 Gaps in data\nLet’s now look at gaps in data. It’s been my experience that gaps in data often have to do with time, but there are other potential gaps, too. To illustrate those, we’re going to introduce some voter registration data from Yadkin County, North Carolina. Let’s load it and take a look:\n\nyadkin_voters &lt;- read_csv(\"data/yadkin_voters.csv\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 28456 Columns: 67\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (37): county_desc, voter_reg_num, ncid, last_name, first_name, middle_n...\ndbl   (9): county_id, zip_code, full_phone_number, birth_year, age_at_year_e...\nlgl  (20): mail_addr3, mail_addr4, ward_abbrv, ward_desc, county_commiss_abb...\ndate  (1): registr_dt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nEach row represents a current or previously registered voter in the county, along with information about that person and the political jurisdictions they reside in. When we talk about gaps, often they indicate the administrative boundaries. Here’s an example: let’s find the most recent registr_dt in this dataset:\n\nyadkin_voters |&gt; arrange(desc(registr_dt))\n\n# A tibble: 28,456 × 67\n   county_id county_desc voter_reg_num ncid     last_name first_name middle_name\n       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;      &lt;chr&gt;      \n 1        99 YADKIN      000000086882  ER48215  FINCANNON JOY        LYNN       \n 2        99 YADKIN      000000086881  ER48214  LUCK      ERIN       EILEEN     \n 3        99 YADKIN      000000086883  ER37698  OSBORNE   EMILY      DANIELLE   \n 4        99 YADKIN      000000086880  AC8089   TEMPLE    BRENDA     PUGH       \n 5        99 YADKIN      000000086879  EN44870  EDWARDS   ROCKY      ALLEN      \n 6        99 YADKIN      000000086875  BN367045 HAWKINS   COREY      ALAN       \n 7        99 YADKIN      000000086877  BN273260 LING      ANDREW     STUART     \n 8        99 YADKIN      000000086878  ER48213  SPICER    CHARLES    STANLEY    \n 9        99 YADKIN      000000086876  CG173637 WHITAKER  SABRINA    JOLEIGH RAY\n10        99 YADKIN      000000086874  ER48212  GLEI      SALLY      COWIN      \n# ℹ 28,446 more rows\n# ℹ 60 more variables: name_suffix_lbl &lt;chr&gt;, status_cd &lt;chr&gt;,\n#   voter_status_desc &lt;chr&gt;, reason_cd &lt;chr&gt;, voter_status_reason_desc &lt;chr&gt;,\n#   res_street_address &lt;chr&gt;, res_city_desc &lt;chr&gt;, state_cd &lt;chr&gt;,\n#   zip_code &lt;dbl&gt;, mail_addr1 &lt;chr&gt;, mail_addr2 &lt;chr&gt;, mail_addr3 &lt;lgl&gt;,\n#   mail_addr4 &lt;lgl&gt;, mail_city &lt;chr&gt;, mail_state &lt;chr&gt;, mail_zipcode &lt;chr&gt;,\n#   full_phone_number &lt;dbl&gt;, confidential_ind &lt;chr&gt;, registr_dt &lt;date&gt;, …\n\n\nIt’s July 14, 2022. That means that this dataset doesn’t have any records newer than that, so if we were describing it we’d need to include that information.\nWhat about the most recent birth_year?\n\nyadkin_voters |&gt; arrange(desc(birth_year))\n\n# A tibble: 28,456 × 67\n   county_id county_desc voter_reg_num ncid    last_name  first_name middle_name\n       &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;   &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      \n 1        99 YADKIN      000000086335  ER47897 ADAMS      BLAIN      NOAH       \n 2        99 YADKIN      000000086376  ER47937 ALEXANDER  PERRY      NATHANIEL  \n 3        99 YADKIN      000000086569  ER48064 ALVARENGA… KAREN      ESMERALDA  \n 4        99 YADKIN      000000086849  ER48202 ALVAREZ-A… PABLO      &lt;NA&gt;       \n 5        99 YADKIN      000000086392  ER47953 ARCADIO-P… JAZMIN     &lt;NA&gt;       \n 6        99 YADKIN      000000086060  ER47741 ARELLANO-… ALEX       &lt;NA&gt;       \n 7        99 YADKIN      000000086414  ER47975 ARZATE-HE… ELIDETH    &lt;NA&gt;       \n 8        99 YADKIN      000000086336  ER47898 ARZATE-VE… JESSICA    &lt;NA&gt;       \n 9        99 YADKIN      000000086371  ER47932 BALL       ALEXIS     NYKOL      \n10        99 YADKIN      000000086147  ER47790 BAUTISTA   ALEXANDRIA ROSE       \n# ℹ 28,446 more rows\n# ℹ 60 more variables: name_suffix_lbl &lt;chr&gt;, status_cd &lt;chr&gt;,\n#   voter_status_desc &lt;chr&gt;, reason_cd &lt;chr&gt;, voter_status_reason_desc &lt;chr&gt;,\n#   res_street_address &lt;chr&gt;, res_city_desc &lt;chr&gt;, state_cd &lt;chr&gt;,\n#   zip_code &lt;dbl&gt;, mail_addr1 &lt;chr&gt;, mail_addr2 &lt;chr&gt;, mail_addr3 &lt;lgl&gt;,\n#   mail_addr4 &lt;lgl&gt;, mail_city &lt;chr&gt;, mail_state &lt;chr&gt;, mail_zipcode &lt;chr&gt;,\n#   full_phone_number &lt;dbl&gt;, confidential_ind &lt;chr&gt;, registr_dt &lt;date&gt;, …\n\n\nLots of 2004 records in there, which makes sense, since those folks are just becoming eligible to vote in North Carolina, where the minimum age is 18. In other words, we shouldn’t see records in here where the “birth_year” is greater than 2004. If we do, we should ask some questions.\nIt’s good to be aware of all gaps in data, but they don’t always represent a problem.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Cleaning Part I: Data smells</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-i.html#suspicious-outliers",
    "href": "data-cleaning-part-i.html#suspicious-outliers",
    "title": "14  Data Cleaning Part I: Data smells",
    "section": "14.4 Suspicious Outliers",
    "text": "14.4 Suspicious Outliers\nAny time you are going to focus on a column for analysis, you should check for suspicious values. Are there any unusually large values or unusually small values? Are there any values that should not exist in the data?\nFinally, let’s first look at “registr_dt” again, so we can see if there’s any missing months, or huge differences in the number of registrations by month. If we’re going to work with dates, we should have lubridate handy for floor_date.\n\nlibrary(lubridate)\n\nThe floor_date function will allow us to group by month, instead of a single day.\n\nyadkin_voters |&gt;\n  mutate(registration_month = floor_date(registr_dt, \"month\")) |&gt;\n  group_by(registration_month) |&gt;\n   summarise(\n    count=n()\n  ) |&gt;\n  arrange(registration_month)\n\n# A tibble: 634 × 2\n   registration_month count\n   &lt;date&gt;             &lt;int&gt;\n 1 1900-01-01            12\n 2 1900-05-01             1\n 3 1933-02-01             1\n 4 1949-04-01             1\n 5 1949-12-01             1\n 6 1951-04-01             1\n 7 1955-06-01             1\n 8 1956-05-01             1\n 9 1956-10-01             1\n10 1960-04-01             1\n# ℹ 624 more rows\n\n\nSo, uh, if this data is accurate, then we have 13 registered voters who are more than 120 years old in Yadkin County. What’s the most likely explanation for this? Some data systems have placeholder values when certain information isn’t known or available. The next oldest registration month is from 1933, which seems plausible.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Data Cleaning Part I: Data smells</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-ii.html",
    "href": "data-cleaning-part-ii.html",
    "title": "15  Data Cleaning Part II: Janitor",
    "section": "",
    "text": "15.1 Cleaning headers\nOne of the first places we can start with cleaning data is cleaning the column names (or headers).\nEvery system has their own way of recording headers, and every developer has their own thoughts of what a good idea is within it. R is most happy when headers are lower case, without special characters.\nIf column headers start with a number, or have a space in between two words, you have to set them off with backticks when using them in a function. Generally speaking, we want one word (or words separated by an underscore), all lowercase, that don’t start with numbers.\nThe janitor library makes fixing headers trivially simple with the function clean_names()\n# cleaning function\ncleaned_conowingo &lt;- conowingo |&gt;\n  clean_names()\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 14 × 21\n   x1_linenumber fec_committee_id tran_id       flag_orgind last_name first_name\n   &lt;chr&gt;         &lt;chr&gt;            &lt;chr&gt;         &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     \n 1 SA11AI        C00694323        AC8C71551726… IND         Pabis     Sherry    \n 2 SA11AI        C00694323        A61211F59EBD… IND         Garvey    Brian     \n 3 SA11AI        C00694323        A9C895B091C8… IND         Huddlest… Deborah   \n 4 SA11AI        C00694323        ABAA82A5E70D… IND         Huddlest… Deborah   \n 5 SA11AI        C00694323        A45519E3246C… IND         Buonaugu… Wm        \n 6 SA11AI        C00694323        A33987D736EC… IND         Huddlest… Deborah   \n 7 SA11AI        C00694323        AE7C6D0C0742… IND         Hamilton  Derrick   \n 8 SA11AI        C00694323        A07D7A1ECE3C… IND         Buonaugu… Wm        \n 9 SA11AI        C00694323        A290335F896E… IND         Buonaugu… Wm        \n10 SA11AI        C00694323        AD6C11068B72… IND         Buonaugu… Wm        \n11 SA11AI        C00694323        A99BBCB87D53… IND         Buonaugu… Wm        \n12 SA11AI        C00694323        A142B275F787… IND         Buonaugu… Wm        \n13 SA11AI        C00694323        AF8E4C981687… IND         Buonaugu… Wm        \n14 SA11AI        C00694323        AE7C6D0C0742… IND         Hamilton  Derrick   \n# ℹ 15 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;lgl&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   date &lt;date&gt;, amount &lt;chr&gt;, aggregate_amount &lt;dbl&gt;, employer &lt;chr&gt;,\n#   occupation &lt;chr&gt;, memo_text &lt;chr&gt;, cycle &lt;dbl&gt;\nThis function changed LAST_NAME to last_name. It put an underscore in address_one to get rid of the space. And it changed 1_linenumber to x1_linenumber. That last one was an improvement – it no longer starts with a number – but it’s still kind of clunky.\nWe can use a tidyverse function rename() to fix that. Let’s just call it linenumber. NOTE: when using rename(), the new name comes first.\n# cleaning function\ncleaned_conowingo &lt;- conowingo |&gt;\n  clean_names() |&gt;\n  rename(linenumber = x1_linenumber)\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 14 × 21\n   linenumber fec_committee_id tran_id          flag_orgind last_name first_name\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     \n 1 SA11AI     C00694323        AC8C7155172624C… IND         Pabis     Sherry    \n 2 SA11AI     C00694323        A61211F59EBD845… IND         Garvey    Brian     \n 3 SA11AI     C00694323        A9C895B091C8A4A… IND         Huddlest… Deborah   \n 4 SA11AI     C00694323        ABAA82A5E70D445… IND         Huddlest… Deborah   \n 5 SA11AI     C00694323        A45519E3246C947… IND         Buonaugu… Wm        \n 6 SA11AI     C00694323        A33987D736EC447… IND         Huddlest… Deborah   \n 7 SA11AI     C00694323        AE7C6D0C074234D… IND         Hamilton  Derrick   \n 8 SA11AI     C00694323        A07D7A1ECE3C44B… IND         Buonaugu… Wm        \n 9 SA11AI     C00694323        A290335F896EC41… IND         Buonaugu… Wm        \n10 SA11AI     C00694323        AD6C11068B72B40… IND         Buonaugu… Wm        \n11 SA11AI     C00694323        A99BBCB87D53C45… IND         Buonaugu… Wm        \n12 SA11AI     C00694323        A142B275F787E48… IND         Buonaugu… Wm        \n13 SA11AI     C00694323        AF8E4C981687E4D… IND         Buonaugu… Wm        \n14 SA11AI     C00694323        AE7C6D0C074234D… IND         Hamilton  Derrick   \n# ℹ 15 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;lgl&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   date &lt;date&gt;, amount &lt;chr&gt;, aggregate_amount &lt;dbl&gt;, employer &lt;chr&gt;,\n#   occupation &lt;chr&gt;, memo_text &lt;chr&gt;, cycle &lt;dbl&gt;",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Cleaning Part II: Janitor</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-ii.html#changing-data-types",
    "href": "data-cleaning-part-ii.html#changing-data-types",
    "title": "15  Data Cleaning Part II: Janitor",
    "section": "15.2 Changing data types",
    "text": "15.2 Changing data types\nRight now, the amount column is stored as a character. Do you see the little &lt;chr&gt; under the amount column in the table above? If we wanted to do math to it, we’d get an error, like so.\n\n# cleaning function\ntotal_conowingo &lt;- cleaned_conowingo |&gt;\n  summarise(total_amount = sum(amount))\n\nError in `summarise()`:\nℹ In argument: `total_amount = sum(amount)`.\nCaused by error in `sum()`:\n! invalid 'type' (character) of argument\n\n# display the cleaned dataset\ntotal_conowingo\n\nError in eval(expr, envir, enclos): object 'total_conowingo' not found\n\n\nWe got an “invalid ‘type’ (character)” error. So let’s fix that using the mutate() function in concert with as.numeric(). We’ll reuse the same column name, so it overwrites it.\n\n# cleaning function\ncleaned_conowingo &lt;- conowingo |&gt;\n  clean_names() |&gt;\n  rename(linenumber = x1_linenumber) |&gt;\n  mutate(amount = as.numeric(amount))\n\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 14 × 21\n   linenumber fec_committee_id tran_id          flag_orgind last_name first_name\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     \n 1 SA11AI     C00694323        AC8C7155172624C… IND         Pabis     Sherry    \n 2 SA11AI     C00694323        A61211F59EBD845… IND         Garvey    Brian     \n 3 SA11AI     C00694323        A9C895B091C8A4A… IND         Huddlest… Deborah   \n 4 SA11AI     C00694323        ABAA82A5E70D445… IND         Huddlest… Deborah   \n 5 SA11AI     C00694323        A45519E3246C947… IND         Buonaugu… Wm        \n 6 SA11AI     C00694323        A33987D736EC447… IND         Huddlest… Deborah   \n 7 SA11AI     C00694323        AE7C6D0C074234D… IND         Hamilton  Derrick   \n 8 SA11AI     C00694323        A07D7A1ECE3C44B… IND         Buonaugu… Wm        \n 9 SA11AI     C00694323        A290335F896EC41… IND         Buonaugu… Wm        \n10 SA11AI     C00694323        AD6C11068B72B40… IND         Buonaugu… Wm        \n11 SA11AI     C00694323        A99BBCB87D53C45… IND         Buonaugu… Wm        \n12 SA11AI     C00694323        A142B275F787E48… IND         Buonaugu… Wm        \n13 SA11AI     C00694323        AF8E4C981687E4D… IND         Buonaugu… Wm        \n14 SA11AI     C00694323        AE7C6D0C074234D… IND         Hamilton  Derrick   \n# ℹ 15 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;lgl&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   date &lt;date&gt;, amount &lt;dbl&gt;, aggregate_amount &lt;dbl&gt;, employer &lt;chr&gt;,\n#   occupation &lt;chr&gt;, memo_text &lt;chr&gt;, cycle &lt;dbl&gt;\n\n\nNotice that the amount has been converted to a &lt;dbl&gt;, which is short for double, a number format. When we attempt to add up all of the amounts to create a total, this time it works fine.\n\n# cleaning function\ntotal_conowingo &lt;- cleaned_conowingo |&gt;\n  summarise(total_amount = sum(amount))\n\n# display the cleaned dataset\ntotal_conowingo\n\n# A tibble: 1 × 1\n  total_amount\n         &lt;dbl&gt;\n1          226",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Cleaning Part II: Janitor</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-ii.html#duplicates",
    "href": "data-cleaning-part-ii.html#duplicates",
    "title": "15  Data Cleaning Part II: Janitor",
    "section": "15.3 Duplicates",
    "text": "15.3 Duplicates\nOne of the most difficult problems to fix in data is duplicate records in the data. They can creep in with bad joins, bad data entry practices, mistakes – all kinds of reasons. A duplicated record isn’t always there because of an error, but you need to know if it’s there before making that determination.\nSo the question is, do we have any records repeated?\nHere we’ll use a function called get_dupes from the janitor library to check for fully repeated records in our cleaned data set.\n\ncleaned_conowingo |&gt;\n  get_dupes()\n\nNo variable names specified - using all columns.\n\n\n# A tibble: 2 × 22\n  linenumber fec_committee_id tran_id           flag_orgind last_name first_name\n  &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;             &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     \n1 SA11AI     C00694323        AE7C6D0C074234DC… IND         Hamilton  Derrick   \n2 SA11AI     C00694323        AE7C6D0C074234DC… IND         Hamilton  Derrick   \n# ℹ 16 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;lgl&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   date &lt;date&gt;, amount &lt;dbl&gt;, aggregate_amount &lt;dbl&gt;, employer &lt;chr&gt;,\n#   occupation &lt;chr&gt;, memo_text &lt;chr&gt;, cycle &lt;dbl&gt;, dupe_count &lt;int&gt;\n\n\nIn this case, a contribution by Derrick Hamilton in our table is fully duplicated. Every field is identical in each.\nWe can fix this by adding the function distinct() to our cleaning script. This will keep only one copy of each unique record in our table\n\n# cleaning function\ncleaned_conowingo &lt;- conowingo |&gt;\n  clean_names() |&gt;\n  rename(linenumber = x1_linenumber) |&gt;\n  mutate(amount = as.numeric(amount)) |&gt;\n  distinct()\n\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 13 × 21\n   linenumber fec_committee_id tran_id          flag_orgind last_name first_name\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     \n 1 SA11AI     C00694323        AC8C7155172624C… IND         Pabis     Sherry    \n 2 SA11AI     C00694323        A61211F59EBD845… IND         Garvey    Brian     \n 3 SA11AI     C00694323        A9C895B091C8A4A… IND         Huddlest… Deborah   \n 4 SA11AI     C00694323        ABAA82A5E70D445… IND         Huddlest… Deborah   \n 5 SA11AI     C00694323        A45519E3246C947… IND         Buonaugu… Wm        \n 6 SA11AI     C00694323        A33987D736EC447… IND         Huddlest… Deborah   \n 7 SA11AI     C00694323        AE7C6D0C074234D… IND         Hamilton  Derrick   \n 8 SA11AI     C00694323        A07D7A1ECE3C44B… IND         Buonaugu… Wm        \n 9 SA11AI     C00694323        A290335F896EC41… IND         Buonaugu… Wm        \n10 SA11AI     C00694323        AD6C11068B72B40… IND         Buonaugu… Wm        \n11 SA11AI     C00694323        A99BBCB87D53C45… IND         Buonaugu… Wm        \n12 SA11AI     C00694323        A142B275F787E48… IND         Buonaugu… Wm        \n13 SA11AI     C00694323        AF8E4C981687E4D… IND         Buonaugu… Wm        \n# ℹ 15 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;lgl&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   date &lt;date&gt;, amount &lt;dbl&gt;, aggregate_amount &lt;dbl&gt;, employer &lt;chr&gt;,\n#   occupation &lt;chr&gt;, memo_text &lt;chr&gt;, cycle &lt;dbl&gt;",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Cleaning Part II: Janitor</span>"
    ]
  },
  {
    "objectID": "data-cleaning-part-ii.html#cleaning-strings",
    "href": "data-cleaning-part-ii.html#cleaning-strings",
    "title": "15  Data Cleaning Part II: Janitor",
    "section": "15.4 Cleaning strings",
    "text": "15.4 Cleaning strings\nThe rest of the problems with this data set all have to do with inconsistent format of values in a few of the columns. To fix these problems, we’re going to make use of mutate() and a new function, case_when() in concert with “string functions” – special functions that allow us to clean up columns stored as character strings. The tidyverse package stringr has lots of useful string functions, more than we’ll learn in this chapter.\nLet’s start by cleaning up the zip field. Remember, three of rows had a five-digit ZIP code, while two had a nine-digit ZIP code, separated by a hyphen.\nWe’re going to write code that tells R to keep the first five digits on the left, and get rid of anything after that by using mutate() in concert with str_sub(), from the stringr package.\n\n# cleaning function\ncleaned_conowingo &lt;- conowingo |&gt;\n  clean_names() |&gt;\n  rename(linenumber = x1_linenumber) |&gt;\n  mutate(amount = as.numeric(amount)) |&gt;\n  distinct() |&gt;\n  mutate(zip = str_sub(zip, start=1L, end=5L))\n\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 13 × 21\n   linenumber fec_committee_id tran_id          flag_orgind last_name first_name\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     \n 1 SA11AI     C00694323        AC8C7155172624C… IND         Pabis     Sherry    \n 2 SA11AI     C00694323        A61211F59EBD845… IND         Garvey    Brian     \n 3 SA11AI     C00694323        A9C895B091C8A4A… IND         Huddlest… Deborah   \n 4 SA11AI     C00694323        ABAA82A5E70D445… IND         Huddlest… Deborah   \n 5 SA11AI     C00694323        A45519E3246C947… IND         Buonaugu… Wm        \n 6 SA11AI     C00694323        A33987D736EC447… IND         Huddlest… Deborah   \n 7 SA11AI     C00694323        AE7C6D0C074234D… IND         Hamilton  Derrick   \n 8 SA11AI     C00694323        A07D7A1ECE3C44B… IND         Buonaugu… Wm        \n 9 SA11AI     C00694323        A290335F896EC41… IND         Buonaugu… Wm        \n10 SA11AI     C00694323        AD6C11068B72B40… IND         Buonaugu… Wm        \n11 SA11AI     C00694323        A99BBCB87D53C45… IND         Buonaugu… Wm        \n12 SA11AI     C00694323        A142B275F787E48… IND         Buonaugu… Wm        \n13 SA11AI     C00694323        AF8E4C981687E4D… IND         Buonaugu… Wm        \n# ℹ 15 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;lgl&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   date &lt;date&gt;, amount &lt;dbl&gt;, aggregate_amount &lt;dbl&gt;, employer &lt;chr&gt;,\n#   occupation &lt;chr&gt;, memo_text &lt;chr&gt;, cycle &lt;dbl&gt;\n\n\nLet’s break down this line of code. It says: take the value in each zip column and extract the first character on the left (1L) through the fifth character on the left (5L), and then use that five-digit zip to overwrite the zip column.\nWe’ll use a different set of functions to standardize how we standardize the different flavors of the word “Conowingo” in the city column. Let’s start by changing every value to title case – first letter uppercase, subsequent letters lowercase – using the str_to_title() function from stringr.\n\n# cleaning function\ncleaned_conowingo &lt;- conowingo |&gt;\n  clean_names() |&gt;\n  rename(linenumber = x1_linenumber) |&gt;\n  mutate(amount = as.numeric(amount)) |&gt;\n  distinct() |&gt;\n  mutate(zip = str_sub(zip, start=1L, end=5L)) |&gt;\n  mutate(city = str_to_title(city))\n\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 13 × 21\n   linenumber fec_committee_id tran_id          flag_orgind last_name first_name\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     \n 1 SA11AI     C00694323        AC8C7155172624C… IND         Pabis     Sherry    \n 2 SA11AI     C00694323        A61211F59EBD845… IND         Garvey    Brian     \n 3 SA11AI     C00694323        A9C895B091C8A4A… IND         Huddlest… Deborah   \n 4 SA11AI     C00694323        ABAA82A5E70D445… IND         Huddlest… Deborah   \n 5 SA11AI     C00694323        A45519E3246C947… IND         Buonaugu… Wm        \n 6 SA11AI     C00694323        A33987D736EC447… IND         Huddlest… Deborah   \n 7 SA11AI     C00694323        AE7C6D0C074234D… IND         Hamilton  Derrick   \n 8 SA11AI     C00694323        A07D7A1ECE3C44B… IND         Buonaugu… Wm        \n 9 SA11AI     C00694323        A290335F896EC41… IND         Buonaugu… Wm        \n10 SA11AI     C00694323        AD6C11068B72B40… IND         Buonaugu… Wm        \n11 SA11AI     C00694323        A99BBCB87D53C45… IND         Buonaugu… Wm        \n12 SA11AI     C00694323        A142B275F787E48… IND         Buonaugu… Wm        \n13 SA11AI     C00694323        AF8E4C981687E4D… IND         Buonaugu… Wm        \n# ℹ 15 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;lgl&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   date &lt;date&gt;, amount &lt;dbl&gt;, aggregate_amount &lt;dbl&gt;, employer &lt;chr&gt;,\n#   occupation &lt;chr&gt;, memo_text &lt;chr&gt;, cycle &lt;dbl&gt;\n\n\nThat was enough to standardize two values (CONOWINGO and conowingo). The only ones that remain are the two clear misspellings (Conowing and Conowingoo). To fix those, we’re going to do some manual editing. And for that, we’re going to use case_when(), a function that let’s us say if a value meets a certain condition, then change it, and if it doesn’t, don’t change it.\n\n# cleaning function\ncleaned_conowingo &lt;- conowingo |&gt;\n  clean_names() |&gt;\n  rename(linenumber = x1_linenumber) |&gt;\n  mutate(amount = as.numeric(amount)) |&gt;\n  distinct() |&gt;\n  mutate(zip = str_sub(zip, start=1L, end=5L)) |&gt;\n  mutate(city = str_to_title(city)) |&gt;\n  mutate(city = case_when(\n    city == \"Conowing\" ~ \"Conowingo\",\n    TRUE ~ city\n  ))\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 13 × 21\n   linenumber fec_committee_id tran_id          flag_orgind last_name first_name\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     \n 1 SA11AI     C00694323        AC8C7155172624C… IND         Pabis     Sherry    \n 2 SA11AI     C00694323        A61211F59EBD845… IND         Garvey    Brian     \n 3 SA11AI     C00694323        A9C895B091C8A4A… IND         Huddlest… Deborah   \n 4 SA11AI     C00694323        ABAA82A5E70D445… IND         Huddlest… Deborah   \n 5 SA11AI     C00694323        A45519E3246C947… IND         Buonaugu… Wm        \n 6 SA11AI     C00694323        A33987D736EC447… IND         Huddlest… Deborah   \n 7 SA11AI     C00694323        AE7C6D0C074234D… IND         Hamilton  Derrick   \n 8 SA11AI     C00694323        A07D7A1ECE3C44B… IND         Buonaugu… Wm        \n 9 SA11AI     C00694323        A290335F896EC41… IND         Buonaugu… Wm        \n10 SA11AI     C00694323        AD6C11068B72B40… IND         Buonaugu… Wm        \n11 SA11AI     C00694323        A99BBCB87D53C45… IND         Buonaugu… Wm        \n12 SA11AI     C00694323        A142B275F787E48… IND         Buonaugu… Wm        \n13 SA11AI     C00694323        AF8E4C981687E4D… IND         Buonaugu… Wm        \n# ℹ 15 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;lgl&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   date &lt;date&gt;, amount &lt;dbl&gt;, aggregate_amount &lt;dbl&gt;, employer &lt;chr&gt;,\n#   occupation &lt;chr&gt;, memo_text &lt;chr&gt;, cycle &lt;dbl&gt;\n\n\nThis is a little complex, so let’s break it down.\nWhat the code above says, in English, is this: Look at all the values in the city column. If the value is “Conowing”, then (that’s what the “~” means, then) replace it with the word “Conowingo”. If it’s anything other than that (that’s what “TRUE” means, otherwise), then keep the existing value in that column.\nWe could fix “Conowingoo” by adding another line inside that function, that looks identical: city == \"Conowingoo\" ~ \"Conowingo\". Like so.\n\n# cleaning function\ncleaned_conowingo &lt;- conowingo |&gt;\n  clean_names() |&gt;\n  rename(linenumber = x1_linenumber) |&gt;\n  mutate(amount = as.numeric(amount)) |&gt;\n  distinct() |&gt;\n  mutate(zip = str_sub(zip, start=1L, end=5L)) |&gt;\n  mutate(city = str_to_title(city)) |&gt;\n  mutate(city = case_when(\n    city == \"Conowing\" ~ \"Conowingo\",\n    city == \"Conowingoo\" ~ \"Conowingo\",\n    TRUE ~ city\n  ))\n\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 13 × 21\n   linenumber fec_committee_id tran_id          flag_orgind last_name first_name\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     \n 1 SA11AI     C00694323        AC8C7155172624C… IND         Pabis     Sherry    \n 2 SA11AI     C00694323        A61211F59EBD845… IND         Garvey    Brian     \n 3 SA11AI     C00694323        A9C895B091C8A4A… IND         Huddlest… Deborah   \n 4 SA11AI     C00694323        ABAA82A5E70D445… IND         Huddlest… Deborah   \n 5 SA11AI     C00694323        A45519E3246C947… IND         Buonaugu… Wm        \n 6 SA11AI     C00694323        A33987D736EC447… IND         Huddlest… Deborah   \n 7 SA11AI     C00694323        AE7C6D0C074234D… IND         Hamilton  Derrick   \n 8 SA11AI     C00694323        A07D7A1ECE3C44B… IND         Buonaugu… Wm        \n 9 SA11AI     C00694323        A290335F896EC41… IND         Buonaugu… Wm        \n10 SA11AI     C00694323        AD6C11068B72B40… IND         Buonaugu… Wm        \n11 SA11AI     C00694323        A99BBCB87D53C45… IND         Buonaugu… Wm        \n12 SA11AI     C00694323        A142B275F787E48… IND         Buonaugu… Wm        \n13 SA11AI     C00694323        AF8E4C981687E4D… IND         Buonaugu… Wm        \n# ℹ 15 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;lgl&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   date &lt;date&gt;, amount &lt;dbl&gt;, aggregate_amount &lt;dbl&gt;, employer &lt;chr&gt;,\n#   occupation &lt;chr&gt;, memo_text &lt;chr&gt;, cycle &lt;dbl&gt;\n\n\nInstead of specifying the exact value, we can also solve the problem by using something more generalizable, using a function called str_detect(), which allows us to search parts of words.\nThe second line of our case_when() function below now says, in English: look in the city column. If you find that one of the values starts with “Conowing” (the “^” symbol means “starts with”), then (the tilde ~ means then) change it to “Conowingo”.\n\n# cleaning function\ncleaned_conowingo &lt;- conowingo |&gt;\n  clean_names() |&gt;\n  rename(linenumber = x1_linenumber) |&gt;\n  mutate(amount = as.numeric(amount)) |&gt;\n  distinct() |&gt;\n  mutate(zip = str_sub(zip, start=1L, end=5L)) |&gt;\n  mutate(city = str_to_title(city)) |&gt;\n  mutate(city = case_when(\n    str_detect(city,\"^Conowing\") ~ \"Conowingo\",\n    TRUE ~ city\n  ))\n\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 13 × 21\n   linenumber fec_committee_id tran_id          flag_orgind last_name first_name\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;            &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     \n 1 SA11AI     C00694323        AC8C7155172624C… IND         Pabis     Sherry    \n 2 SA11AI     C00694323        A61211F59EBD845… IND         Garvey    Brian     \n 3 SA11AI     C00694323        A9C895B091C8A4A… IND         Huddlest… Deborah   \n 4 SA11AI     C00694323        ABAA82A5E70D445… IND         Huddlest… Deborah   \n 5 SA11AI     C00694323        A45519E3246C947… IND         Buonaugu… Wm        \n 6 SA11AI     C00694323        A33987D736EC447… IND         Huddlest… Deborah   \n 7 SA11AI     C00694323        AE7C6D0C074234D… IND         Hamilton  Derrick   \n 8 SA11AI     C00694323        A07D7A1ECE3C44B… IND         Buonaugu… Wm        \n 9 SA11AI     C00694323        A290335F896EC41… IND         Buonaugu… Wm        \n10 SA11AI     C00694323        AD6C11068B72B40… IND         Buonaugu… Wm        \n11 SA11AI     C00694323        A99BBCB87D53C45… IND         Buonaugu… Wm        \n12 SA11AI     C00694323        A142B275F787E48… IND         Buonaugu… Wm        \n13 SA11AI     C00694323        AF8E4C981687E4D… IND         Buonaugu… Wm        \n# ℹ 15 more variables: middle_name &lt;lgl&gt;, prefix &lt;lgl&gt;, suffix &lt;lgl&gt;,\n#   address_one &lt;chr&gt;, address_two &lt;lgl&gt;, city &lt;chr&gt;, state &lt;chr&gt;, zip &lt;chr&gt;,\n#   date &lt;date&gt;, amount &lt;dbl&gt;, aggregate_amount &lt;dbl&gt;, employer &lt;chr&gt;,\n#   occupation &lt;chr&gt;, memo_text &lt;chr&gt;, cycle &lt;dbl&gt;\n\n\nBy using str_detect(city,“^Conowing”), we pick up any values that start with “Conowing”, so it would change all four of these. If we used city == “Conowing”, it would only pick up one.\nLastly, there’s the issue with inconsistent spelling of Merry Knoll Lane in the street address column. Do we need to clean this?\nRemember the motivating question that’s driving us to do this cleaning: what is the total amount of contributions from Conowingo, MD in ZIP code 21918?\nWe don’t need the street_address field to answer that question. So we’re not going to bother cleaning it.\nThat’s a good approach for the future. A good rule of thumb is that you should only spend time cleaning fields that are critical to the specific analysis you want to do.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Data Cleaning Part II: Janitor</span>"
    ]
  },
  {
    "objectID": "merging.html",
    "href": "merging.html",
    "title": "17  Combining and joining",
    "section": "",
    "text": "17.1 Combining data (stacking)\nLet’s say that we have Maryland county voter registration data from five different elections in five different files. They have the same record layout and the same number of counties (plus Baltimore City). We can combine them into a single dataframe.\nLet’s do what we need to import them properly. I’ve merged it all into one step for each of the datasets.\nlibrary(tidyverse)\ncounty_voters_2016 &lt;- read_csv(\"data/county_voters_2016.csv\")\n\nRows: 25 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (7): YEAR, DEM, REP, LIB, UNA, OTH, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncounty_voters_2018 &lt;- read_csv(\"data/county_voters_2018.csv\")\n\nRows: 25 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (7): YEAR, DEM, REP, LIB, UNA, OTH, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncounty_voters_2020 &lt;- read_csv(\"data/county_voters_2020.csv\")\n\nRows: 25 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (7): YEAR, DEM, REP, LIB, OTH, UNA, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncounty_voters_2022 &lt;- read_csv(\"data/county_voters_2022.csv\")\n\nRows: 25 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (7): YEAR, DEM, REP, LIB, OTH, UNA, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncounty_voters_2024 &lt;- read_csv(\"data/county_voters_2024.csv\")\n\nRows: 25 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (7): YEAR, DEM, REP, LIB, UNA, OTH, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nAll of these datasets have the same number of columns, all with the same names, so if we want to merge them together to compare them over time, we need to stack them together. The verb here, in R, is bind_rows. You tell the function what you want to combine and it does it, assuming that you’ve got column names in common containing identically formatted data.\nSince we have five dataframes, we’re going to need to pass them as a list, meaning they’ll be enclosed inside the list function.\ncounty_voters_combined &lt;- bind_rows(list(county_voters_2016, county_voters_2018, county_voters_2020, county_voters_2022, county_voters_2024))\nAnd boom, like that, we have 125 rows of data together instead of five dataframes. Now we can ask more interesting questions like how a county’s registration patterns have changed over time.\nThere are plenty of uses for bind_rows: any regularly updated data that comes in the same format like crime reports or award recipients or player game statistics. Or election results.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Combining and joining</span>"
    ]
  },
  {
    "objectID": "merging.html#joining-data",
    "href": "merging.html#joining-data",
    "title": "17  Combining and joining",
    "section": "17.2 Joining data",
    "text": "17.2 Joining data\nMore complicated is when you have two separate tables that are connected by a common element or elements. But there’s a verb for that, too: join.\nLet’s start by reading in some Maryland 2020 county population data:\n\nmaryland_population &lt;- read_csv('data/maryland_population_2020.csv')\n\nRows: 24 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (1): POP2020\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOne of the columns we have is called county, which is what we have in our county_voters_2020 dataframe.\nTo put the Maryland population data and voter registration data together, we need to use something called a join. There are different kinds of joins. It’s better if you think of two tables sitting next to each other. A left_join takes all the records from the left table and only the records that match in the right one. A right_join does the same thing. An inner_join takes only the records where they are equal. There’s one other join – a full_join which returns all rows of both, regardless of if there’s a match – but I’ve never once had a use for a full join.\nIn the best-case scenario, the two tables we want to join share a common column. In this case, both of our tables have a column called county that has the same characteristics: values in both look identical, including how they distinguish Baltimore City from Baltimore County. This is important, because joins work on exact matches.\nWe can do this join multiple ways and get a similar result. We can put the population file on the left and the registration data on the right and use a left join to get them all together. And we use by= to join by the correct column. I’m going to count the rows at the end. The reason I’m doing this is important: Rule 1 in joining data is having an idea of what you are expecting to get. So with a left join with population on the left, I have 24 rows, so I expect to get 24 rows when I’m done.\n\nmaryland_population |&gt; left_join(county_voters_2020, by=\"COUNTY\") |&gt; nrow()\n\n[1] 24\n\n\nRemove the nrow and run it again for yourself. By default, dplyr will do a “natural” join, where it’ll match all the matching columns in both tables. So if we take out the by, it’ll use all the common columns between the tables. That may not be right in every instance but let’s try it. If it works, we should get 24 rows.\n\nmaryland_population |&gt; left_join(county_voters_2020)\n\nJoining with `by = join_by(COUNTY)`\n\n\n# A tibble: 24 × 9\n   COUNTY           POP2020  YEAR    DEM    REP   LIB   OTH    UNA  TOTAL\n   &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n 1 Allegany           68106  2020  12820  22530   204   508   7674  43736\n 2 Anne Arundel      588261  2020 174494 135457  1922  3581  90162 405616\n 3 Baltimore City    585708  2020 311610  30163   951  4511  52450 399685\n 4 Baltimore County  854535  2020 313870 142534  2227  7201 100576 566408\n 5 Calvert            92783  2020  24587  28181   332   706  14178  67984\n 6 Caroline           33293  2020   6629  10039    86   215   4208  21177\n 7 Carroll           172891  2020  33662  63967   670  1292  25770 125361\n 8 Cecil             103725  2020  21601  30880   341   887  15110  68819\n 9 Charles           166617  2020  72416  24711   349   977  19849 118302\n10 Dorchester         32531  2020   9848   8730    78   183   3348  22187\n# ℹ 14 more rows\n\n\nSince we only have one column in common between the two tables, the join only used that column. And we got the same answer. If we had more columns in common, you could see in your results columns with .X after them - that’s a sign of duplicative columns between two tables, and you may decide you don’t need both moving forward.\nLet’s save our joined data to a new dataframe, but this time let’s remove the select function so we don’t limit the columns to just three.\n\nmaryland_population_with_voters &lt;- maryland_population |&gt; left_join(county_voters_2020)\n\nJoining with `by = join_by(COUNTY)`\n\n\nNow, with our joined data, we can answer questions in a more useful way. But joins can do even more than just bring data together; they can include additional data to enable you to ask more sophisticated questions. Right now we have registered voters and total population. But we can do more.\nLet’s try adding more Maryland demographic data to the mix. Using a file describing the 18-and-over population (from which eligible voters come) from the state’s data catalog, we can read it into R:\n\nmaryland_demographics &lt;- read_csv('data/maryland_demographics.csv')\n\nRows: 24 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): NAME\ndbl (10): GEOCODE, pop_18_over, pop_one_race, pop_white, pop_black, pop_nati...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAgain, we can use a left_join to make our demographic data available. This time we’ll need to specify the two fields to join because they do not have identical names. We’ll use COUNTY from our population data and NAME from the demographic data, and the order matters - the first column is from the dataframe you name first.\n\nmaryland_population_with_voters_and_demographics &lt;- maryland_population_with_voters |&gt; left_join(maryland_demographics, by=c(\"COUNTY\"=\"NAME\"))\n\nNow we’ve got population data and demographic data by county. That means we can draw from both datasets in asking our questions. For example, we could see the counties with the highest 18+ Black population as a percentage of all population 18 and over and also the percentage of Democrats in that county.\nWe can get this by using mutate and arrange:\n\nmaryland_population_with_voters_and_demographics |&gt;\n  mutate(pct_black_18_plus = (pop_black/pop_18_over)*100, pct_dems = (DEM/TOTAL)*100) |&gt;\n  arrange(desc(pct_black_18_plus)) |&gt;\n  select(COUNTY, pct_black_18_plus, pct_dems)\n\n# A tibble: 24 × 3\n   COUNTY           pct_black_18_plus pct_dems\n   &lt;chr&gt;                        &lt;dbl&gt;    &lt;dbl&gt;\n 1 Prince George's               60.9     78.3\n 2 Baltimore City                56.3     78.0\n 3 Charles                       48.2     61.2\n 4 Somerset                      39.0     41.8\n 5 Baltimore County              28.8     55.4\n 6 Dorchester                    26.2     44.4\n 7 Wicomico                      25.6     42.3\n 8 Howard                        18.7     52.4\n 9 Montgomery                    18.1     61.0\n10 Anne Arundel                  17.4     43.0\n# ℹ 14 more rows\n\n\nIf you know Maryland political demographics, this result isn’t too surprising, but Somerset County - the state’s 2nd smallest in terms of population - stands out for its Black population, which is a greater percentage than Baltimore County and Montgomery County.\nLet’s change that to look at Asian population:\n\nmaryland_population_with_voters_and_demographics |&gt;\n  mutate(pct_asian_18_plus = (pop_asian/pop_18_over)*100, pct_dems = (DEM/TOTAL)*100) |&gt;\n  arrange(desc(pct_asian_18_plus)) |&gt;\n  select(COUNTY, pct_asian_18_plus, pct_dems)\n\n# A tibble: 24 × 3\n   COUNTY           pct_asian_18_plus pct_dems\n   &lt;chr&gt;                        &lt;dbl&gt;    &lt;dbl&gt;\n 1 Howard                       19.4      52.4\n 2 Montgomery                   16.0      61.0\n 3 Baltimore County              6.34     55.4\n 4 Frederick                     4.88     38.9\n 5 Prince George's               4.68     78.3\n 6 Anne Arundel                  4.52     43.0\n 7 Baltimore City                4.17     78.0\n 8 Charles                       3.55     61.2\n 9 Harford                       3.15     35.4\n10 St. Mary's                    3.13     35.7\n# ℹ 14 more rows\n\n\nHere, Howard and Montgomery County stand out in terms of the percentage of Asian population 18 and over. The jurisdictions with the highest percentage of Democrats - Prince George’s and Baltimore City - have small Asian populations.\nSometimes joins look like they should work but don’t. Often this is due to the two columns you’re joining on having different data types: joining a  column to a  column, for example. Let’s walk through an example of that using some demographic data by zip code.\n\nmaryland_zcta &lt;- read_csv('data/maryland_zcta.csv')\n\nRows: 468 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): FIRST_CLAS, FIRST_MTFC, FIRST_FUNC, REPORT_2_P, REPORT_9_P\ndbl (35): OBJECTID_1, ZCTA5CE10, FIRST_STAT, FIRST_GEOI, ZCTA5N, STATE, AREA...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(maryland_zcta)\n\nRows: 468\nColumns: 40\n$ OBJECTID_1 &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ ZCTA5CE10  &lt;dbl&gt; 20601, 20602, 20603, 20606, 20607, 20608, 20609, 20611, 206…\n$ FIRST_STAT &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,…\n$ FIRST_GEOI &lt;dbl&gt; 2420601, 2420602, 2420603, 2420606, 2420607, 2420608, 24206…\n$ FIRST_CLAS &lt;chr&gt; \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\",…\n$ FIRST_MTFC &lt;chr&gt; \"G6350\", \"G6350\", \"G6350\", \"G6350\", \"G6350\", \"G6350\", \"G635…\n$ FIRST_FUNC &lt;chr&gt; \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\",…\n$ ZCTA5N     &lt;dbl&gt; 20601, 20602, 20603, 20606, 20607, 20608, 20609, 20611, 206…\n$ STATE      &lt;dbl&gt; 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,…\n$ AREALAND   &lt;dbl&gt; 115635266, 35830723, 44239637, 7501011, 54357590, 45583064,…\n$ AREAWATR   &lt;dbl&gt; 387684, 352762, 219356, 1248760, 448221, 5330329, 6602735, …\n$ POP100     &lt;dbl&gt; 24156, 24955, 28967, 431, 9802, 919, 1120, 1078, 261, 11860…\n$ HU100      &lt;dbl&gt; 8722, 9736, 10317, 230, 3504, 426, 554, 413, 142, 4424, 204…\n$ NHW        &lt;dbl&gt; 9785, 8466, 9625, 377, 2165, 438, 1009, 798, 245, 4044, 352…\n$ NHB        &lt;dbl&gt; 11146, 13054, 15025, 45, 6321, 453, 82, 215, 12, 6786, 32, …\n$ NHAI       &lt;dbl&gt; 155, 116, 98, 1, 33, 5, 2, 5, 0, 106, 2, 32, 3, 4, 38, 8, 1…\n$ NHA        &lt;dbl&gt; 880, 731, 1446, 4, 560, 2, 1, 10, 0, 186, 3, 165, 5, 1, 402…\n$ NHNH       &lt;dbl&gt; 11, 15, 24, 0, 3, 0, 1, 0, 0, 4, 1, 2, 0, 0, 4, 1, 0, 3, 1,…\n$ NHO        &lt;dbl&gt; 48, 58, 65, 0, 6, 0, 0, 0, 0, 8, 0, 1, 0, 3, 5, 8, 0, 5, 10…\n$ NHT        &lt;dbl&gt; 849, 999, 1091, 0, 234, 9, 15, 33, 1, 321, 13, 213, 14, 4, …\n$ HISP       &lt;dbl&gt; 1282, 1516, 1593, 4, 480, 12, 10, 17, 3, 405, 2, 244, 9, 7,…\n$ PNHW       &lt;dbl&gt; 40.5, 33.9, 33.2, 87.5, 22.1, 47.7, 90.1, 74.0, 93.9, 34.1,…\n$ PNHB       &lt;dbl&gt; 46.1, 52.3, 51.9, 10.4, 64.5, 49.3, 7.3, 19.9, 4.6, 57.2, 7…\n$ PNHAI      &lt;dbl&gt; 0.6, 0.5, 0.3, 0.2, 0.3, 0.5, 0.2, 0.5, 0.0, 0.9, 0.5, 0.5,…\n$ PNHA       &lt;dbl&gt; 3.6, 2.9, 5.0, 0.9, 5.7, 0.2, 0.1, 0.9, 0.0, 1.6, 0.7, 2.8,…\n$ PNHNH      &lt;dbl&gt; 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.2, 0.0,…\n$ PNHO       &lt;dbl&gt; 0.2, 0.2, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0,…\n$ PNHT       &lt;dbl&gt; 3.5, 4.0, 3.8, 0.0, 2.4, 1.0, 1.3, 3.1, 0.4, 2.7, 3.2, 3.6,…\n$ PHISP      &lt;dbl&gt; 5.3, 6.1, 5.5, 0.9, 4.9, 1.3, 0.9, 1.6, 1.1, 3.4, 0.5, 4.2,…\n$ POP65_     &lt;dbl&gt; 1922, 1964, 1400, 108, 847, 173, 271, 129, 54, 1372, 73, 55…\n$ PCTPOP65_  &lt;dbl&gt; 8.0, 7.9, 4.8, 25.1, 8.6, 18.8, 24.2, 12.0, 20.7, 11.6, 18.…\n$ MEDAGE     &lt;dbl&gt; 37.3, 32.6, 34.5, 49.1, 40.9, 46.6, 47.6, 44.3, 47.3, 40.8,…\n$ VACNS      &lt;dbl&gt; 376, 769, 531, 15, 172, 39, 32, 22, 14, 249, 18, 158, 8, 18…\n$ PVACNS     &lt;dbl&gt; 4.3, 7.9, 5.1, 6.5, 4.9, 9.2, 5.8, 5.3, 9.9, 5.6, 8.8, 7.2,…\n$ PHOWN      &lt;dbl&gt; 71.1, 59.7, 73.8, 49.7, 83.1, 60.4, 44.8, 63.8, 38.3, 73.9,…\n$ PWOMORT    &lt;dbl&gt; 11.2, 9.0, 4.7, 39.3, 10.3, 28.2, 38.7, 21.8, 43.9, 17.4, 2…\n$ PRENT      &lt;dbl&gt; 19.9, 34.4, 22.6, 18.1, 7.4, 15.9, 27.0, 18.3, 31.7, 10.5, …\n$ PLT18SP    &lt;dbl&gt; 30.4, 43.6, 29.9, 31.2, 22.1, 14.1, 28.9, 24.5, 43.9, 26.7,…\n$ REPORT_2_P &lt;chr&gt; \"http://mdpgis.mdp.state.md.us/Census2010/PDF/00_SF1DP_2Pro…\n$ REPORT_9_P &lt;chr&gt; \"http://mdpgis.mdp.state.md.us/census2010/PDF/00_SF1_9PROFI…\n\n\nYou can see that ZCTA5N, the column representing the Zip Code Tabulation Area, is a numeric column. But should it be? Do we ever want to know the average zip code in Maryland? Zip codes and ZCTAs look like numbers but really are character columns. Let’s change that so that we can be sure to join them correctly with other data where the zip codes are not numbers. We’ll use mutate:\n\nmaryland_zcta &lt;- maryland_zcta |&gt; mutate(across(ZCTA5N, as.character))\n\nWhat’s happening here is that we’re telling R to take all of the values in the ZCTA5N column and make them “as.character”. If we wanted to change a column to numeric, we’d do “as.numeric”. When you join two dataframes, the join columns must be the same datatype.\nJoining datasets allows you to expand the range and sophistication of questions you’re able to ask. It is one of the most powerful tools in a journalist’s toolkit.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Combining and joining</span>"
    ]
  },
  {
    "objectID": "rvest.html",
    "href": "rvest.html",
    "title": "19  Scraping data with Rvest",
    "section": "",
    "text": "Sometimes, governments put data online on a page or in a searchable database. And when you ask them for a copy of the data underneath the website, they say no.\nWhy? Because they have a website. That’s it. That’s their reason. They say they don’t have to give you the data because they’ve already given you the data, never mind that they haven’t given to you in a form you can actually load into R with ease.\nLucky for us, there’s a way for us to write code to get data even when an agency hasn’t made it easy: webscraping.\nOne of the most powerful tools you can learn as a data journalist is how to scrape data from the web. Scraping is the process of programming a computer to act like a human that opens a web browser, goes to a website, ingests the HTML from that website into R and turns it into data.\nThe degree of difficulty here goes from “Easy” to “So Hard You Want To Throw Your Laptop Out A Window.” And the curve between the two can be steep. You can learn how to scrape “Easy” in a day. The hard ones take a little more time, but it’s often well worth the effort because it lets you get stories you couldn’t get without it.\nIn this chapter, we’ll show you an easy one. And in the next chapter, we’ll so you a moderately harder one.\nLet’s start easy.\nWe’re going to use a library called rvest, which you can install it the same way we’ve done all installs: go to the console and install.packages(\"rvest\").\nLike so many R package names, rvest is a bad pun. You’re supposed to read it to sound like “harvest”, as in “harvesting” information from a website the same way you’d harvest crops in a field.\nWe’ll load these packages first:\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(janitor)\n\nFor this example, we’re going to work on loading a simple table of data from the Maryland State Board of Elections. This is a table of unofficial county-level election results for the Attorney General’s race from the November general election.\nLet’s suppose we can’t find a table like that for download, but we do see a version on the SBOE website at this URL: https://elections.maryland.gov/elections/2022/general_results/gen_detail_results_2022_3_1.html.\n\n\n\n\n\n\n\n\n\nWe could get this table into R with the following manual steps: highlighting the text, copying it into Excel, saving it as a csv, and reading it into R. Or, we could write a few lines of webscraping code to have R do that for us!\nIn this simple example, it’s probably faster to do it manually than have R do it for us. But during the time when ballots are being counted, this table is likely to change, and we don’t want to keep doing manual repetitive tasks.\nWhy would we ever write code to grab a single table? There’s several reasons:\n\nOur methods are transparent. If a colleague wants to run our code from scratch to factcheck our work, they don’t need to repeat the manual steps, which are harder to document than writing code.\nLet’s suppose we wanted to grab the same table every day, to monitor for changes. Writing a script once, and pressing a single button every day is going to be much more efficient than doing this manually every day.\nIf we’re doing it manually, we’re more likely to make a mistake, like maybe failing to copy every row from the whole table.\nIt’s good practice to prepare us to do more complex scraping jobs. As we’ll see in the next chapter, if we ever want to grab the same table from hundreds of pages, writing code is much faster and easier than going to a hundred different pages ourselves and downloading data.\n\nSo, to scrape, the first thing we need to do is start with the URL. Let’s store it as an object called ag_url.\n\nag_url &lt;- \"https://elections.maryland.gov/elections/2022/general_results/gen_detail_results_2022_3_1.html\"\n\nWhen we go to the web page, we can see a nicely-designed page that contains our information.\nBut what we really care about, for our purposes, is the html code that creates that page.\nIn our web browser, if we right-click anywhere on the page and select “view source” from the popup menu, we can see the source code. Or you can just copy this into Google Chrome: view-source:https://elections.maryland.gov/elections/2022/general_results/gen_detail_results_2022_3_1.html.\nHere’s a picture of what some of the source code looks like.\n\n\n\n\n\n\n\n\n\nWe’ll use those HTML tags – things like &lt;table&gt; and &lt;tr&gt; – to grab the info we need.\nOkay, step 1.\nLet’s write a bit of code to tell R to go to the URL for the page and ingest all of that HTML code. In the code below, we’re starting with our URL and using the read_html() function from rvest to ingest all of the page html, storing it as an object called results.\n\n# read in the html\nresults &lt;- ag_url |&gt;\n  read_html()\n\n# display the html below\nresults\n\n{html_document}\n&lt;html lang=\"en\" manifest=\"manifest.appcache\"&gt;\n[1] &lt;head&gt;\\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] &lt;body&gt;\\r\\n    &lt;div class=\"container\"&gt;  \\r\\n        &lt;div class=\"skipNav\"&gt;\\ ...\n\n\nIf you’re running this code in R Studio, in our environment window at right, you’ll see results as a “list of 2”.\nThis is not a dataframe, it’s a different type of data structure a “nested list.”\nIf we click on the name “results” in our environment window, we can see that it’s pulled in the html and shown us the general page structure. Nested within the &lt;html&gt; tag is the &lt;head&gt; and &lt;body&gt;, the two fundamental sections of most web pages. We’re going to pull information out of the &lt;body&gt; tag in a bit.\n\n\n\n\n\n\n\n\n\nNow, our task is to just pull out the section of the html that contains the information we need.\nBut which part do we need from that mess of html code? To figure that out, we can go back to the page in a web browser like chrome, and use built in developer tools to “inspect” the html code underlying the page.\nOn the page, find the data we want to grab – “Vote for 1” - and right click on the word “Jurisdiction” in the column header of the table. That will bring up a dropdown menu. Select “Inspect”, which will pop up a window called the “element inspector” that shows us where different elements on the page are located, what html tags created those elements, and other info.\n\n\n\n\n\n\n\n\n\nThe entire table that we want of results is actually contained inside an html &lt;table&gt;. It has a &lt;tbody&gt; that contains one row &lt;tr&gt; per county.\nBecause it’s inside of a table, and not some other kind of element (like a &lt;div&gt;), rvest has a special function for easily extracting and converting html tables, called html_table(). This function extracts all the html tables on the page, but this page only has one so we’re good.\n\n# read in the html and extract all the tables\nresults &lt;- ag_url |&gt;\n  read_html() |&gt;\n  html_table()\n\n# show the dataframe\n\nresults\n\n[[1]]\n# A tibble: 25 × 4\n   Jurisdiction  Michael Anthony Pero…¹ Anthony G. Brown\\r\\n…² `Other Write-Ins`\n   &lt;chr&gt;         &lt;chr&gt;                  &lt;chr&gt;                  &lt;chr&gt;            \n 1 Allegany      14,382                 6,994                  16               \n 2 Anne Arundel  91,718                 121,096                433              \n 3 Baltimore Ci… 14,594                 128,282                288              \n 4 Baltimore Co… 100,480                170,858                467              \n 5 Calvert       20,275                 16,647                 41               \n 6 Caroline      7,128                  3,475                  13               \n 7 Carroll       44,195                 26,928                 112              \n 8 Cecil         20,750                 11,844                 35               \n 9 Charles       16,042                 37,945                 55               \n10 Dorchester    6,615                  4,784                  16               \n# ℹ 15 more rows\n# ℹ abbreviated names: ¹​`Michael Anthony Peroutka\\r\\n\\r\\n\\r\\nRepublican`,\n#   ²​`Anthony G. Brown\\r\\n\\r\\n\\r\\nDemocratic`\n\n\nIn the environment window at right, look at results Note that it’s now a “list of 1”.\nThis gets a little complicated, but what you’re seeing here is a nested list that contains one data frame – also called tibbles – one for each table that exists on the web page we scraped.\nSo, all we need to do now is to store that single dataframe as an object. We can do that with this code, which says “keep only the first dataframe from our nested list.”\n\n# Read in all html from table, store all tables on page as nested list of dataframes.\nresults &lt;- ag_url |&gt;\n  read_html() |&gt;\n  html_table()\n\n# Just keep the first dataframe in our list\n\nresults &lt;- results[[1]]\n\n# show the dataframe\n\nresults\n\n# A tibble: 25 × 4\n   Jurisdiction  Michael Anthony Pero…¹ Anthony G. Brown\\r\\n…² `Other Write-Ins`\n   &lt;chr&gt;         &lt;chr&gt;                  &lt;chr&gt;                  &lt;chr&gt;            \n 1 Allegany      14,382                 6,994                  16               \n 2 Anne Arundel  91,718                 121,096                433              \n 3 Baltimore Ci… 14,594                 128,282                288              \n 4 Baltimore Co… 100,480                170,858                467              \n 5 Calvert       20,275                 16,647                 41               \n 6 Caroline      7,128                  3,475                  13               \n 7 Carroll       44,195                 26,928                 112              \n 8 Cecil         20,750                 11,844                 35               \n 9 Charles       16,042                 37,945                 55               \n10 Dorchester    6,615                  4,784                  16               \n# ℹ 15 more rows\n# ℹ abbreviated names: ¹​`Michael Anthony Peroutka\\r\\n\\r\\n\\r\\nRepublican`,\n#   ²​`Anthony G. Brown\\r\\n\\r\\n\\r\\nDemocratic`\n\n\nWe now have a proper dataframe, albeit with some lengthy column headers.\nFrom here, we can do a little cleaning. First we’ll use clean_names() to lower the column names. Then use rename() to replace the candidate column names with simpler versions. We can just use the column positions instead of writing out the full names, which is nice.\nThen let’s use slice() to remove the last row – row number 25 – which contains totals and percentages that we don’t need. Finally, we’ll make sure the vote tallies are numbers using mutate and gsub(), which we use to replace all the commas with nothing.\n\n# Read in all html from table, get the HTML table.\nresults &lt;- ag_url |&gt;\n  read_html() |&gt;\n  html_table()\n\n# Standardize column headers, remove last row\n\nresults &lt;- results[[1]] |&gt;\n  clean_names() |&gt;\n  rename(peroutka = 2, brown = 3, write_ins = 4) |&gt;\n  slice(-25) |&gt;\n  mutate(peroutka = as.numeric(gsub(\",\",\"\", peroutka))) |&gt;\n  mutate(brown = as.numeric(gsub(\",\",\"\", brown))) |&gt;\n  mutate(write_ins = as.numeric(gsub(\",\",\"\", write_ins)))\n\n# show the dataframe\nresults\n\n# A tibble: 24 × 4\n   jurisdiction     peroutka  brown write_ins\n   &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;\n 1 Allegany            14382   6994        16\n 2 Anne Arundel        91718 121096       433\n 3 Baltimore City      14594 128282       288\n 4 Baltimore County   100480 170858       467\n 5 Calvert             20275  16647        41\n 6 Caroline             7128   3475        13\n 7 Carroll             44195  26928       112\n 8 Cecil               20750  11844        35\n 9 Charles             16042  37945        55\n10 Dorchester           6615   4784        16\n# ℹ 14 more rows\n\n\nAnd there we go. We now have a nice tidy dataframe of Maryland attorney general election results that we could ask some questions of.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Scraping data with Rvest</span>"
    ]
  },
  {
    "objectID": "census.html",
    "href": "census.html",
    "title": "20  Intro to APIs: The Census",
    "section": "",
    "text": "20.1 The ACS\nIn 2010, the Census Bureau replaced SF3 with the American Community Survey. The Good News is that the data would be updated on a rolling basis. The bad news is that it’s more complicated because it’s more like survey data with a large sample. That means there’s margins of error and confidence intervals to worry about. By default, using get_acs fetches data from the 5-year estimates (currently 2018-2022), but you can specify 1-year estimates for jurisdictions with at least 65,000 people (many counties and cities).\nHere’s an example using the 5-year ACS estimates:\nWhat is Maryland’s richest county?\nWe can measure this by median household income. That variable is B19013_001, so we can get that data like this (I’m narrowing it to the top 20 for simplicity):\nmd &lt;- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2022)\n\nGetting data from the 2018-2022 5-year ACS\n\nmd &lt;- md |&gt; arrange(desc(estimate)) |&gt; top_n(20, estimate)\n\nmd\n\n# A tibble: 20 × 5\n   GEOID NAME                             variable  estimate   moe\n   &lt;chr&gt; &lt;chr&gt;                            &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 24027 Howard County, Maryland          medincome   140971  2122\n 2 24009 Calvert County, Maryland         medincome   128078  4071\n 3 24031 Montgomery County, Maryland      medincome   125583  1757\n 4 24017 Charles County, Maryland         medincome   116882  1877\n 5 24003 Anne Arundel County, Maryland    medincome   116009  1642\n 6 24021 Frederick County, Maryland       medincome   115724  2555\n 7 24037 St. Mary's County, Maryland      medincome   113668  4210\n 8 24013 Carroll County, Maryland         medincome   111672  3322\n 9 24035 Queen Anne's County, Maryland    medincome   108332  4986\n10 24025 Harford County, Maryland         medincome   106417  2845\n11 24033 Prince George's County, Maryland medincome    97935  1296\n12 24005 Baltimore County, Maryland       medincome    88157  1370\n13 24015 Cecil County, Maryland           medincome    86869  3534\n14 24041 Talbot County, Maryland          medincome    81667  4291\n15 24047 Worcester County, Maryland       medincome    76689  3573\n16 24043 Washington County, Maryland      medincome    73017  2283\n17 24029 Kent County, Maryland            medincome    71635  9808\n18 24045 Wicomico County, Maryland        medincome    69421  2707\n19 24011 Caroline County, Maryland        medincome    65326  3919\n20 24023 Garrett County, Maryland         medincome    64447  4886\nHoward, Calvert, Montgomery, Anne Arundel, Charles. What do they all have in common? Lots of suburban flight from DC and Baltimore. But do the margins of error let us say one county is richer than the other. We can find this out visually using error bars. Don’t worry much about the code here – we’ll cover that soon enough.\nmd |&gt;\n  mutate(NAME = gsub(\" County, Maryland\", \"\", NAME)) |&gt;\n  ggplot(aes(x = estimate, y = reorder(NAME, estimate))) +\n  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) +\n  geom_point(color = \"red\") +\n  labs(title = \"Household income by county in Maryland\",\n       subtitle = \"2018-2022 American Community Survey\",\n       y = \"\",\n       x = \"ACS estimate (bars represent margin of error)\")\nAs you can see, some of the error bars are quite wide. Some are narrow. But if the bars overlap, it means the difference between the two counties is within the margin of error, and the differences aren’t statistically significant. So is the difference between Calvert and Montgomery significant? Nope. Is the difference between Howard and everyone else significant? Yes it is.\nLet’s ask another question of the ACS – did any counties lose income from the time of the global financial crisis to the current 5-year window?\nLet’s re-label our first household income data.\nmd22 &lt;- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2022)\n\nGetting data from the 2018-2022 5-year ACS\nAnd now we grab the 2010 median household income.\nmd10 &lt;- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2010)\n\nGetting data from the 2006-2010 5-year ACS\nWhat I’m going to do next is a lot, but each step is simple. I’m going to join the data together, so each county has one line of data. Then I’m going to rename some fields that repeat. Then I’m going to calculate the minimium and maximum value of the estimate using the margin of error. That’ll help me later. After that, I’m going to calculate a perent change and sort it by that change.\nmd10 |&gt;\n  inner_join(md22, by=c(\"GEOID\", \"NAME\")) |&gt;\n  rename(estimate2010=estimate.x, estimate2022=estimate.y) |&gt;\n  mutate(min2010 = estimate2010-moe.x, max2010 = estimate2010+moe.x, min2020 = estimate2022-moe.y, max2020 = estimate2022+moe.y) |&gt;\n  select(-variable.x, -variable.y, -moe.x, -moe.y) |&gt;\n  mutate(change = ((estimate2022-estimate2010)/estimate2010)*100) |&gt;\n  arrange(change)\n\n# A tibble: 24 × 9\n   GEOID NAME   estimate2010 estimate2022 min2010 max2010 min2020 max2020 change\n   &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 24011 Carol…        58799        65326   56740   60858   61407   69245   11.1\n 2 24039 Somer…        42443        52149   39092   45794   45533   58765   22.9\n 3 24019 Dorch…        45151        57490   43470   46832   54445   60535   27.3\n 4 24041 Talbo…        63017        81667   60081   65953   77376   85958   29.6\n 5 24017 Charl…        88825       116882   87268   90382  115005  118759   31.6\n 6 24035 Queen…        81096       108332   78068   84124  103346  113318   33.6\n 7 24015 Cecil…        64886        86869   63014   66758   83335   90403   33.9\n 8 24031 Montg…        93373       125583   92535   94211  123826  127340   34.5\n 9 24027 Howar…       103273       140971  101654  104892  138849  143093   36.5\n10 24045 Wicom…        50752        69421   49313   52191   66714   72128   36.8\n# ℹ 14 more rows\nSo according to this, Caroline County had the smallest change between 2010 and 2022, while all other jurisdictions saw percentage increases of at least 20 percent.\nBut did they?\nLook at the min and max values for both. Is the change statistically significant?\nThe ACS data has lots of variables, just like the decennial Census does. To browse them, you can do this:\nv22 &lt;- load_variables(2022, \"acs5\", cache=TRUE)\nAnd then view v20 to see what kinds of variables are available via the API.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Intro to APIs: The Census</span>"
    ]
  },
  {
    "objectID": "census.html#wide-results",
    "href": "census.html#wide-results",
    "title": "20  Intro to APIs: The Census",
    "section": "20.2 “Wide” Results",
    "text": "20.2 “Wide” Results\nAlthough one of the chief strengths of tidycensus is that it offers a, well, tidy display of Census data, it also has the ability to view multiple variables spread across columns. This can be useful for creating percentages and comparing multiple variables.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Intro to APIs: The Census</span>"
    ]
  },
  {
    "objectID": "census.html#sorting-results",
    "href": "census.html#sorting-results",
    "title": "20  Intro to APIs: The Census",
    "section": "20.3 Sorting Results",
    "text": "20.3 Sorting Results\nYou’ll notice that we’ve used arrange to sort the results of tidycensus functions, although that’s done after we create a new variable to hold the data. There’s another way to use arrange that you should know about, one that you can use for exploratory analysis. An example using median household income from 2022:\n\nmd22 &lt;- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2022)\n\nGetting data from the 2018-2022 5-year ACS\n\narrange(md22, desc(estimate))\n\n# A tibble: 24 × 5\n   GEOID NAME                          variable  estimate   moe\n   &lt;chr&gt; &lt;chr&gt;                         &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;\n 1 24027 Howard County, Maryland       medincome   140971  2122\n 2 24009 Calvert County, Maryland      medincome   128078  4071\n 3 24031 Montgomery County, Maryland   medincome   125583  1757\n 4 24017 Charles County, Maryland      medincome   116882  1877\n 5 24003 Anne Arundel County, Maryland medincome   116009  1642\n 6 24021 Frederick County, Maryland    medincome   115724  2555\n 7 24037 St. Mary's County, Maryland   medincome   113668  4210\n 8 24013 Carroll County, Maryland      medincome   111672  3322\n 9 24035 Queen Anne's County, Maryland medincome   108332  4986\n10 24025 Harford County, Maryland      medincome   106417  2845\n# ℹ 14 more rows\n\n\nIn this case we don’t save the sorted results to a variable, we can just see the output in the console.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Intro to APIs: The Census</span>"
    ]
  },
  {
    "objectID": "ai_and_llms.html",
    "href": "ai_and_llms.html",
    "title": "25  AI and Data Journalism",
    "section": "",
    "text": "25.1 Setup\nWe’ll be using a service called Groq for the examples here. You should sign up for a free account and create an API key. Make sure you copy that key. We’ll also need to install an R package to handle the responses:\n# install.packages(\"devtools\")\ndevtools::install_github(\"heurekalabsco/axolotr\")\n\nDownloading GitHub repo heurekalabsco/axolotr@HEAD\n\n\nrlang   (1.1.2 -&gt; 1.1.4) [CRAN]\nglue    (1.6.2 -&gt; 1.7.0) [CRAN]\ncli     (3.6.2 -&gt; 3.6.3) [CRAN]\nopenssl (2.1.1 -&gt; 2.2.1) [CRAN]\ncurl    (5.2.0 -&gt; 5.2.2) [CRAN]\nstringi (1.8.3 -&gt; 1.8.4) [CRAN]\n\n\nInstalling 6 packages: rlang, glue, cli, openssl, curl, stringi\n\n\n\n  There are binary versions available but the source versions are later:\n        binary source needs_compilation\nrlang    1.1.2  1.1.4              TRUE\nglue     1.6.2  1.7.0              TRUE\ncli      3.6.2  3.6.3              TRUE\nopenssl  2.1.1  2.2.1              TRUE\ncurl     5.2.0  5.2.2              TRUE\nstringi  1.8.3  1.8.4              TRUE\n\n\ninstalling the source packages 'rlang', 'glue', 'cli', 'openssl', 'curl', 'stringi'\n\n\n── R CMD build ─────────────────────────────────────────────────────────────────\n* checking for file ‘/private/var/folders/d3/n35gxlzs361_3bbcn6br1my80000gq/T/Rtmpcji18l/remotesf386416e54d0/heurekalabsco-axolotr-0a78e15/DESCRIPTION’ ... OK\n* preparing ‘axolotr’:\n* checking DESCRIPTION meta-information ... OK\n* checking for LF line-endings in source and make files and shell scripts\n* checking for empty or unneeded directories\nOmitted ‘LazyData’ from DESCRIPTION\n* building ‘axolotr_0.1.03.tar.gz’\nThen we can load that library and, using your API key, setup your credentials:\nlibrary(axolotr)\n\ncreate_credentials(GROQ_API_KEY = \"YOUR API KEY HERE\")\n\nCredentials updated successfully. Please restart your R session for changes to take effect.\nSee that “Please restart your R session for changes to take effect.”? Go ahead and do that; you’ll need to rerun the library() function above.\nLet’s make sure that worked. We’ll be using the Llama 3.1 model released by Meta.\ngroq_response &lt;- axolotr::ask(\n  prompt = \"Give me five names for a pet lemur\",\n  model = \"llama-3.1-8b-instant\"\n)\n\nError in Groq API call: \n\ngroq_response\n\nNULL\nI guess you’re getting a lemur?",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>AI and Data Journalism</span>"
    ]
  },
  {
    "objectID": "ai_and_llms.html#three-uses-of-ai-in-data-journalism",
    "href": "ai_and_llms.html#three-uses-of-ai-in-data-journalism",
    "title": "25  AI and Data Journalism",
    "section": "25.2 Three Uses of AI in Data Journalism",
    "text": "25.2 Three Uses of AI in Data Journalism\nThere are at least three good uses of AI in data journalism:\n\nturning unstructured information into data\nhelping with code debugging and explanation\nbrainstorming about strategies for data analysis and visualization\n\nIf you’ve tried to use a large language model to actually do data analysis, it can work, but often the results can be frustrating. Think of AI as a potentially useful assistant for the work you’re doing. If you have a clear idea of the question you want to ask or the direction you want to go, they can help. If you don’t have a clear idea or question, they probably will be less helpful. Let’s go over a quick example of each use.\n\n25.2.1 Turning Unstructured Information into Data\nNews organizations are sitting on a trove of valuable raw materials - the words, images, audio and video that they produce every day. We can (hopefully) search it, but search doesn’t always deliver meaning, let alone elevate patterns. For that, often it helps to turn that information into structured data. Let’s look at an example involving my friend Tyson Evans, who recently celebrated his 10th wedding anniversary. You can read about his wedding in The New York Times.\nThis announcement is a story, but it’s also data - or it should be.\n\n\n\n\n\n\n\n\n\nWhat if we could extract those highlighted portions of the text into, say, a CSV file? That’s something that LLMs are pretty good at. Let’s give it a shot using the full text of that announcement:\n\ntext = \"Gabriela Nunes Herman and Tyson Charles Evans were married Saturday at the home of their friends Marcy Gringlas and Joel Greenberg in Chilmark, Mass. Rachel Been, a friend of the couple who received a one-day solemnization certificate from Massachusetts, officiated. The bride, 33, will continue to use her name professionally. She is a Brooklyn-based freelance photographer for magazines and newspapers. She graduated from Wesleyan University in Middletown, Conn. She is a daughter of Dr. Talia N. Herman of Brookline, Mass., and Jeffrey N. Herman of Cambridge, Mass. The bride’s father is a lawyer and the executive vice president of DecisionQuest, a national trial consulting firm in Boston. Her mother is a senior primary care internist at Harvard Vanguard Medical Associates, a practice in Boston. The groom, 31, is a deputy editor of interactive news at The New York Times and an adjunct professor at the Columbia University Graduate School of Journalism. He graduated from the University of California, Los Angeles. He is the son of Carmen K. Evans of Climax Springs, Mo., and Frank J. Evans of St. Joseph, Mo. The groom’s father retired as the president of UPCO, a national retailer of pet supplies in St. Joseph.\"\n\nevans_response &lt;- axolotr::ask(\n  prompt = paste(\"Given the following text, extract information into a CSV file with the following structure with no yapping: celebrant1,celebrant2,location,officiant,celebrant1_age,celebrant2_age,celebrant1_parent1,celebrant1_parent2,celebrant2_parent1,celebrant2_parent2\", text),\n  model = \"llama-3.1-8b-instant\"\n)\n\nError in Groq API call: \n\nevans_response\n\nNULL\n\n\nA brief word about that “no yapping” bit; it’s a way to tell your friendly LLM to cut down on the chattiness in its response. What we care about is the data, not the narrative. And look at the results: without even providing an example or saying that the text described a wedding, the LLM did a solid job. Now imagine if you could do this with hundreds or thousands of similar announcements. You’ve just built a database.\n\n\n25.2.2 Helping with Code Debugging and Explanation\nWhen you’re writing code and run into error messages, you should read them. But if they do not make sense to you, you can ask an LLM to do some translation, which is another great use case for AI. As with any debugging exercise, you should provide some context, things like “Using R and the tidyverse …” and describing what you’re trying to do, but you also can ask LLMs to explain an error message in a different way. Here’s an example:\n\ndebug_response &lt;- axolotr::ask(\n  prompt = \"Explain the following R error message using brief, simple language and suggest a single fix. I am using the tidyverse library: could not find function '|&gt;'\",\n  model = \"llama-3.1-8b-instant\"\n)\n\nError in Groq API call: \n\ndebug_response\n\nNULL\n\n\nThe trouble is that if you run that several times, it will give you slightly different answers. Not fact machines. But you should be able to try some of the suggested solutions and see if any of them work. An even better use could be to pass in working code that you’re not fully understanding and ask the LLM to explain it to you.\n\n\n25.2.3 Brainstorming about Strategies for Data Analysis and Visualization\nLet’s say that you have some data that you want to interview, but aren’t sure how to proceed. LLMs can provide some direction, but you may not want to follow their directions exactly. You shouldn’t accept their judgments uncritically; you’ll still need to think for yourself. Here’s an example of how that might go:\n\nidea_response &lt;- axolotr::ask(\n  prompt = \"I have a CSV file of daily data on campus police incidents, including the type of incident, building location and time. Using R and the tidyverse, suggest some ways that I could find patterns in the data. Use the new-style pipe operator (|&gt;) in any code examples\",\n  model = \"llama-3.1-8b-instant\"\n)\n\nError in Groq API call: \n\nidea_response\n\nNULL\n\n\nNote that the column names may not match your data; the LLM is making predictions about your data, so you could provide the column names.\n\n\n25.2.4 Can’t I just ask it to do data analysis for me?\nWell, you can try, but I’m not confident you’ll like the results. As this story from The Pudding makes clear, the potential for using LLMs to not just assist with but perform data analysis is real. What will make the difference is how much context you can provide and how clear your ideas and questions are. You still have to do the work.",
    "crumbs": [
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>AI and Data Journalism</span>"
    ]
  },
  {
    "objectID": "textanalysis.html",
    "href": "textanalysis.html",
    "title": "26  An intro to text analysis",
    "section": "",
    "text": "26.1 Going beyond a single word\nThe next step in text analysis is using ngrams. An ngram is any combination of words that you specify. Two word ngrams are called bigrams (bi-grams). Three would be trigrams. And so forth.\nThe code to make ngrams is similar to what we did above, but involves some more twists.\nSo this block is is going to do the following:\nreleases |&gt;\n  filter(date &lt; '2022-01-01') |&gt;\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) |&gt;\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") |&gt;\n  filter(!word1 %in% stop_words$word) |&gt;\n  filter(!word2 %in% stop_words$word) |&gt;\n  mutate(bigram = paste(word1, word2, sep=\" \")) |&gt;\n  group_by(bigram) |&gt;\n  tally(sort=TRUE) |&gt;\n  mutate(percent = (n/sum(n))*100) |&gt;\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram              n percent\n   &lt;chr&gt;           &lt;int&gt;   &lt;dbl&gt;\n 1 covid 19          786   0.856\n 2 human rights      319   0.347\n 3 19 pandemic       288   0.314\n 4 health care       224   0.244\n 5 chesapeake bay    190   0.207\n 6 american rescue   166   0.181\n 7 rescue plan       166   0.181\n 8 public health     157   0.171\n 9 john sarbanes     133   0.145\n10 kweisi mfume      133   0.145\nAnd we already have a different, more nuanced result. Health was among the top single words, and we can see that “health care”, “human rights” and “chesapeake bay” are among the top 2-word phrases. What about after 2021?\nreleases |&gt;\n  filter(date &gt;= '2022-01-01') |&gt;\n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2) |&gt;\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") |&gt;\n  filter(!word1 %in% stop_words$word) |&gt;\n  filter(!word2 %in% stop_words$word) |&gt;\n  mutate(bigram = paste(word1, word2, sep=\" \")) |&gt;\n  group_by(bigram) |&gt;\n  tally(sort=TRUE) |&gt;\n  mutate(percent = (n/sum(n))*100) |&gt;\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram                 n percent\n   &lt;chr&gt;              &lt;int&gt;   &lt;dbl&gt;\n 1 project location     262   0.447\n 2 covid 19             167   0.285\n 3 health care          140   0.239\n 4 chesapeake bay       139   0.237\n 5 human rights         119   0.203\n 6 amount included      113   0.193\n 7 baltimore city       100   0.171\n 8 prince george’s       98   0.167\n 9 mental health         94   0.160\n10 location baltimore    85   0.145\nWhile “covid 19” is still a top phrase, it’s not the leading phrase any longer. “Mental health” makes an appearance, too. You’ll notice that the percentages are very small; that’s not irrelevant but in some cases it’s the differences in patterns that’s more important.\nThere are some potential challenges to doing an analysis. For one, there are variations of words that could probably be standardized - maybe using OpenRefine - that would give us cleaner results. There might be some words among our list of stop words that actually are meaningful in this context.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>An intro to text analysis</span>"
    ]
  },
  {
    "objectID": "textanalysis.html#going-beyond-a-single-word",
    "href": "textanalysis.html#going-beyond-a-single-word",
    "title": "26  An intro to text analysis",
    "section": "",
    "text": "Use the releases data we created above, and filter for pre-2022 releases.\nUnnest the tokens again, but instead we’re going to create a field called bigram, break apart summary, but we’re going to specify the tokens in this case are ngrams of 2.\nWe’re going to make things easier to read and split bigrams into word1 and word2.\nWe’re going to filter out stopwords again, but this time we’re going to do it in both word1 and word2 using a slightly different filtering method.\nBecause of some weirdness in calculating the percentage, we’re going to put bigram back together again, now that the stop words are gone.\nWe’ll then group by, count and create a percent just like we did above.\nWe’ll then use top_n to give us the top 10 bigrams.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>An intro to text analysis</span>"
    ]
  },
  {
    "objectID": "textanalysis.html#sentiment-analysis",
    "href": "textanalysis.html#sentiment-analysis",
    "title": "26  An intro to text analysis",
    "section": "26.2 Sentiment Analysis",
    "text": "26.2 Sentiment Analysis\nAnother popular use of text analysis is to measure the sentiment of a word - whether it expresses a positive or negative idea - and tidytext has built-in tools to make that possible. We use word counts like we’ve already calculated and bring in a dataframe of words (called a lexicon) along with their sentiments using a function called get_sentiments. The most common dataframe is called “bing” which has nothing to do with the Microsoft search engine. Let’s load it:\n\nbing &lt;- get_sentiments(\"bing\")\n\nbing_word_counts_2021 &lt;- unique_words_2021 |&gt;\n  inner_join(bing) |&gt;\n  count(word, sentiment, sort = TRUE)\n\nJoining with `by = join_by(word)`\n\nbing_word_counts_2022 &lt;- unique_words_2022 |&gt;\n  inner_join(bing) |&gt;\n  count(word, sentiment, sort = TRUE)\n\nJoining with `by = join_by(word)`\n\nView(bing_word_counts_2021)\nView(bing_word_counts_2022)\n\nGauging the sentiment of a word can be heavily dependent on the context, and as with other types of text analysis sometimes larger patterns are more meaningful than individual results. But the potential with text analysis is vast: knowing what words and phrases that public officials employ can be a way to evaluate their priorities, cohesiveness and tactics for persuading voters and their colleagues. And those words and phrases are data.",
    "crumbs": [
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>An intro to text analysis</span>"
    ]
  },
  {
    "objectID": "basicstats.html",
    "href": "basicstats.html",
    "title": "27  Basic Stats: Linear Regression and The T-Test",
    "section": "",
    "text": "A month into the Covid-19 pandemic, in April 2020, Reveal, an investigative reporting outfit, wrote a story based on original data analysis showing that a disproportionate share of PPP loans were going to states that Donald Trump won in 2016. In North Dakota, a state that gave a higher share of its vote to Trump than all but three states, 58 percent of small businesses got PPP loans. In Democratic-leaning New York, which was hit hard in the pandemic’s first wave, only 18 percent of small businesses received loans. They wrote:\n“Reveal’s analysis found that businesses in states that Trump won in 2016 received a far greater share of the small-business relief funds than those won by his Democratic rival, Hillary Clinton. Eight of the top 10 recipient states – ranked according to the proportion of each state’s businesses that received funding – went to Trump in 2016. Meanwhile, seven of the bottom 10 states, where the lowest proportion of businesses received funding, went to Clinton. Taken together, 32% of businesses in states that Trump won got Paycheck Protection Program dollars, we found, compared with 22% of businesses in states that went to Clinton.”\nIt continued: “The figures were so stark that they sparked concerns of political interference. Rep. Jackie Speier, a California Democrat who serves on the House Oversight and Reform Committee, said the data raise questions about whether stimulus dollars were deliberately funneled to states that voted for Trump and have Republican governors.”\nThe story didn’t present any evidence of political meddling. Instead, it offered the results of several lines of data analysis that attempted to answer this central question: did red states get a bigger slice of the PPP pie than blue states?\nMostly, it used basic descriptive statistics, calculating rates, ranking states and computing averages. But the data set it used also presents an opportunity to use two slightly more advanced statistical analysis methods to look for patterns: linear regression, to examine relationships, and a t.test, to confirm the statistical validity of an average between two groups. So, let’s do that here.\nFirst, let’s load libraries. We’re going to load janitor, the tidyverse and a new package, corrr, which will help us do linear regression a bit easier than base R.\n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(corrr)\n\nNow let’s load the data we’ll be using. It has five fields:\n\nstate_name\nvote_2016: whether Trump or Clinton won the state’s electoral vote.\npct_trump: the percentage of the vote Trump received in the state.\nbusinesses_receiving_ppe_pct: the percentage of the state’s small businesses that received a PPP loan.\nppe_amount_per_employee: the average amount of money provided by PPP per small business employee in the state.\n\n\nreveal_data &lt;- read_rds(\"data/reveal_data.rds\")\n\nreveal_data\n\n# A tibble: 51 × 5\n   state_name  vote_2016 pct_trump businesses_receiving…¹ ppe_amount_per_emplo…²\n   &lt;chr&gt;       &lt;chr&gt;         &lt;dbl&gt;                  &lt;dbl&gt;                  &lt;dbl&gt;\n 1 North Dako… Trump          63.0                     58                   7928\n 2 Nebraska    Trump          58.8                     56                   7244\n 3 South Dako… Trump          61.5                     53                   6541\n 4 Oklahoma    Trump          65.3                     50                   6499\n 5 Mississippi Trump          57.9                     49                   5674\n 6 Iowa        Trump          51.2                     48                   6642\n 7 Kansas      Trump          56.6                     47                   7087\n 8 Hawaii      Clinton        29.4                     47                   7417\n 9 Maine       Clinton        43.5                     45                   6617\n10 Arkansas    Trump          60.6                     44                   5549\n# ℹ 41 more rows\n# ℹ abbreviated names: ¹​businesses_receiving_ppe_pct, ²​ppe_amount_per_employee\n\n\n\n28 Linear Regression\nLet’s start with this question: did small businesses in states that voted more strongly for Trump get loans at higher rate than small businesses in Democratic states? We can answer it by examining the relationship or correlation between two variables, pct_trump and businesses_receiving_ppe_pct. How much do they move in tandem? Do states with more Trump support see bigger average PPP loans? Do extra Trumpy states get even more? Do super blue states get the least?\nLet’s start by plotting them to get a sense of the pattern.\n\nreveal_data |&gt;\n  ggplot() +\n  geom_point(aes(x=pct_trump,y=businesses_receiving_ppe_pct)) +\n  geom_smooth(aes(x=pct_trump,y=businesses_receiving_ppe_pct), method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nIt’s a bit messy, but we can see something of a pattern here in the blob of dots. Generally, the dots are moving from the lower left (less Trumpy states that got loans at a lower rate) to upper right (red states that got loans at a higher rate). The blue “line of best fit” shows the general direction of the relationship.\nLet’s test another variable, the average amount of money provided by PPP per small business employee in the state.\n\nreveal_data |&gt;\n  ggplot() +\n  geom_point(aes(x=pct_trump,y=ppe_amount_per_employee)) +\n  geom_smooth(aes(x=pct_trump,y=ppe_amount_per_employee), method=\"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThis one is a bit messier. There may be a slight upward slope in this blob of dots, but it’s not quite as apparent. It seems less certain that there’s a relationship between these two variables.\nWe can be a bit more precise by calculating a statistic called the correlation coefficient, also called “r”. r is a value between 1 and -1. An r of 1 indicates a strong positive correlation.\nAn increase in air temperature and air conditioning use at home is strongly-positively correlated: the hotter it gets, the more we have to use air conditioning. If we were to plot those two variables, we might not get 1, but we’d get close to it.\nAn r of -1 indicates a strong negative correlation. An increase in temperature and home heating use is strongly negatively correlated: the hotter it gets, the less heat we use indoors. We might not hit -1, but we’d probably get close to it.\nA correlation of 0 indicates no relationship.\nAll r values will fall somewhere on this scale, and how to interpret them isn’t always straightforward. They’re best used to give general guidance when exploring patterns.\nWe can calculate r with a function from the corrr package called “correlate()”. First, we remove the non-numeric values from our reveal_data (state name and a binary vote_2016 column), then we correlate.\n\nreveal_data |&gt;\n  select(-state_name, -vote_2016) |&gt;\n  correlate() |&gt;\n  select(term, pct_trump)\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 3 × 2\n  term                         pct_trump\n  &lt;chr&gt;                            &lt;dbl&gt;\n1 pct_trump                       NA    \n2 businesses_receiving_ppe_pct     0.522\n3 ppe_amount_per_employee          0.221\n\n#glimpse(reveal_data)\n\nThe table this function produces generally confirms our interpretation of the two graphs above. The relationship between a state’s pct_trump and ppe_amount_per employee is positive, but at .22 (on a scale of -1 to 1), the relationship isn’t particularly strong. That’s why the second graphic above was messier than the first.\nThe relationship between businesses in a state receiving ppe and the state’s Trump vote is a bit stronger, if still moderate, .52 (on a scale of -1 to 1). Is this finding statistically valid? We can get a general sense of that by calculating the p-value of this correlation, a test of statistical significance. For that, we can use the cor.test function.\n\ncor.test(reveal_data$pct_trump, reveal_data$businesses_receiving_ppe_pct)\n\n\n    Pearson's product-moment correlation\n\ndata:  reveal_data$pct_trump and reveal_data$businesses_receiving_ppe_pct\nt = 4.2818, df = 49, p-value = 8.607e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2875734 0.6971386\nsample estimates:\n      cor \n0.5218038 \n\n\nThis output is quite a bit uglier, but for our purposes there are two key pieces of information from this chunk of unfamiliar words. First, it shows the correlation calculated above: r 0.5218. Two, it shows the p-value, which is 0.00008607. That’s very low, as far as p-values go, which indicates that there’s a very slim chance that our finding is a statistical aberration.\nNow let’s test the other one, the relationship between the pct_trump and the ppe_amount_per_employee.\n\ncor.test(reveal_data$pct_trump, reveal_data$ppe_amount_per_employee)\n\n\n    Pearson's product-moment correlation\n\ndata:  reveal_data$pct_trump and reveal_data$ppe_amount_per_employee\nt = 1.5872, df = 49, p-value = 0.1189\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.05798429  0.46818515\nsample estimates:\n     cor \n0.221133 \n\n\nAgain, it shows our r value of .22, which was weaker. And the p-value here is a much larger 0.12. That indicates a higher chance of our finding being a statistical aberration, high enough that I wouldn’t rely on its validity.\np &lt; .05 is accepted in many scientific disciplines – and by many data journalists – as the cutoff for statistical significance. But there’s heated debate about that level, and some academics question whether p-values should be relied on so heavily.\nAnd to be clear, a low p-value does not prove that we’ve found what we set out to find. There’s nothing on this graph or in the regression model output that proves that Trump’s administration tipped the scales in favor of states that voted for it. It’s entirely possible that there’s some other variable – or variables – not considered here that explain this pattern.\nAll we know is that we’ve identified a potentially promising pattern, worthy of additional reporting and analysis to flesh out.\n\n\n29 T-tests\nLet’s suppose we want to ask a related set of questions: did Trump states get higher ppp loan amounts per employee than states won by Clinton? Or did a larger percentage of businesses in states won by Trump receive, on average, a higher rate of PPP loans on average than states won by Clinton.\nWe can do this because, in our data, we have a column with two possible categorical values, Clinton or Trump, for each state.\nWe could just calculate the averages like we’re used to doing.\n\nreveal_data |&gt;\n  group_by(vote_2016) |&gt;\n  summarise(\n    mean_ppp_amount_per_employee = mean(ppe_amount_per_employee),\n    mean_businesses_receiving_ppe_pct = mean(businesses_receiving_ppe_pct)\n  )\n\n# A tibble: 2 × 3\n  vote_2016 mean_ppp_amount_per_employee mean_businesses_receiving_ppe_pct\n  &lt;chr&gt;                            &lt;dbl&gt;                             &lt;dbl&gt;\n1 Clinton                          5704.                              28.2\n2 Trump                            6021.                              37.2\n\n\nExamining this, it appears that in both categories there’s a difference.\nThe average amount of ppp loans per employee in Clinton states is smaller than Trump states (6,000 to 5,700). And the average percentage of businesses that got loans in Trump states was larger – 37% – than Clinton states – 28%. Should we report these as meaningful findings?\nA t-test can help us answer that question. It can tell us where there’s a statistically significant difference between the means of two groups. Have we found a real difference, or have we chanced upon a statistical aberration? Let’s see by calculating it for the average loan amount.\n\nt.test(ppe_amount_per_employee ~ vote_2016, data = reveal_data)\n\n\n    Welch Two Sample t-test\n\ndata:  ppe_amount_per_employee by vote_2016\nt = -1.2223, df = 36.089, p-value = 0.2295\nalternative hypothesis: true difference in means between group Clinton and group Trump is not equal to 0\n95 percent confidence interval:\n -843.7901  209.1329\nsample estimates:\nmean in group Clinton   mean in group Trump \n             5703.571              6020.900 \n\n\nWe see our two means, for Trump and Clinton, the same as we calculated above. The t-value is approximately 1, the p-value here is .2295, both of which should which should give us pause that we’ve identified something meaningful. More on t-tests here\nLet’s try the percentage of businesses getting ppp loans.\n\nt.test(businesses_receiving_ppe_pct ~ vote_2016, data = reveal_data)\n\n\n    Welch Two Sample t-test\n\ndata:  businesses_receiving_ppe_pct by vote_2016\nt = -3.182, df = 45.266, p-value = 0.002643\nalternative hypothesis: true difference in means between group Clinton and group Trump is not equal to 0\n95 percent confidence interval:\n -14.68807  -3.30241\nsample estimates:\nmean in group Clinton   mean in group Trump \n             28.23810              37.23333 \n\n\nThis is a bit more promising. T is much stronger – about 3 – and the p-value is .002. Both of these should give us assurance that we’ve found something statistically meaningful. Again, this doesn’t prove that Trump is stacking the deck for states. It just suggests there’s a pattern worth following up on.",
    "crumbs": [
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Basic Stats: Linear Regression and The T-Test</span>"
    ]
  },
  {
    "objectID": "open-refine.html",
    "href": "open-refine.html",
    "title": "16  Data Cleaning Part III: Open Refine",
    "section": "",
    "text": "16.1 Refinr, Open Refine in R\nWhat is Open Refine?\nOpen Refine is a software program that has tools – algorithms – that find small differences in text and helps you fix them quickly. How Open Refine finds those small differences is through something called clustering. The algorithms behind clustering are not exclusive to Open Refine, so they can be used elsewhere.\nEnter refinr, a package that contains the same clustering algorithms as Open Refine but all within R. Go ahead and install it if you haven’t already by opening the console and running install.packages(\"refinr\"). Then we can load libraries as we do.\nlibrary(tidyverse)\nlibrary(refinr)\nlibrary(janitor)\nLet’s load some campaign expenditure data focused on food-related expenses in Washington, D.C. Essentially, where campaigns spend their money on D.C. restaurants.\nNow let’s try and group and count the number of expenditures by recipient. To make it a bit more manageable, let’s use another string function from stringr and filter for recipients that start with the uppercase “W” or lowercase “w” using the function str_detect() with a regular expression.\nThe filter function in the codeblock below says: look in the city column, and pluck out any value that starts with (the “^” symbol means “starts with”) a lowercase “w” OR (the vertical “|”, called a pipe, means OR) an uppercase “W”.\ndc_food |&gt;\n  group_by(recipient_name) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  filter(str_detect(recipient_name, '^w|^W')) |&gt;\n  arrange(recipient_name)\n\n# A tibble: 50 × 2\n   recipient_name      count\n   &lt;chr&gt;               &lt;int&gt;\n 1 W HOTELS                3\n 2 W. MILLAR & CO          1\n 3 W. MILLAR & CO.         5\n 4 WA METRO                1\n 5 WAGSHAL'S DELI          1\n 6 WALMART SUPERCENTER    13\n 7 WALTER'S SPORTS BAR     1\n 8 WALTERS                 1\n 9 WALTERS SPORT           1\n10 WALTERS SPORTS          1\n# ℹ 40 more rows\nThere are several problems in this data that will prevent proper grouping and summarizing. We’ve learned several functions to do this manually.\nBy using the Open Refine package for R, refinr, our hope is that it can identify and standardize the data with a little more ease.\nThe first merging technique that’s part of the refinr package we’ll try is the key_collision_merge.\nThe key collision merge function takes each string and extracts the key parts of it. It then puts every key in a bin based on the keys matching.\nOne rule you should follow when using this is: do not overwrite your original fields. Always work on a copy. If you overwrite your original field, how will you know if it did the right thing? How can you compare it to your original data? To follow this, I’m going to mutate a new field called clean_city and put the results of key collision merge there.\ncleaned_dc_food &lt;- dc_food |&gt;\n  mutate(recipient_clean=key_collision_merge(recipient_name)) |&gt;\n  select(recipient_name, recipient_clean, everything()) |&gt; \n  arrange(recipient_clean)\n\ncleaned_dc_food\n\n# A tibble: 13,108 × 50\n   recipient_name recipient_clean committee_id committee_name        report_year\n   &lt;chr&gt;          &lt;chr&gt;           &lt;chr&gt;        &lt;chr&gt;                       &lt;dbl&gt;\n 1 &PIZZA         &PIZZA          C00003418    REPUBLICAN NATIONAL …        2022\n 2 1 WEST DUPONT  1 WEST DUPONT   C00003418    REPUBLICAN NATIONAL …        2021\n 3 1 WEST DUPONT  1 WEST DUPONT   C00003418    REPUBLICAN NATIONAL …        2021\n 4 1 WEST DUPONT  1 WEST DUPONT   C00003418    REPUBLICAN NATIONAL …        2021\n 5 1 WEST DUPONT  1 WEST DUPONT   C00003418    REPUBLICAN NATIONAL …        2022\n 6 1 WEST DUPONT  1 WEST DUPONT   C00003418    REPUBLICAN NATIONAL …        2022\n 7 1 WEST DUPONT  1 WEST DUPONT   C00003418    REPUBLICAN NATIONAL …        2021\n 8 1 WEST DUPONT  1 WEST DUPONT   C00003418    REPUBLICAN NATIONAL …        2022\n 9 1 WEST DUPONT  1 WEST DUPONT   C00003418    REPUBLICAN NATIONAL …        2022\n10 1 WEST DUPONT  1 WEST DUPONT   C00003418    REPUBLICAN NATIONAL …        2022\n# ℹ 13,098 more rows\n# ℹ 45 more variables: report_type &lt;chr&gt;, image_number &lt;dbl&gt;,\n#   line_number &lt;chr&gt;, transaction_id &lt;chr&gt;, file_number &lt;dbl&gt;,\n#   entity_type &lt;chr&gt;, entity_type_desc &lt;chr&gt;,\n#   unused_recipient_committee_id &lt;chr&gt;, recipient_committee_id &lt;chr&gt;,\n#   recipient_state &lt;chr&gt;, beneficiary_committee_name &lt;chr&gt;,\n#   national_committee_nonfederal_account &lt;lgl&gt;, disbursement_type &lt;lgl&gt;, …\nTo examine changes refinr made, let’s examine the changes it made to cities that start with the letter “W”.\ncleaned_dc_food |&gt;\n  group_by(recipient_name, recipient_clean) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  filter(str_detect(recipient_clean, \"^w|^W\")) |&gt;\n  arrange(recipient_clean)\n\n`summarise()` has grouped output by 'recipient_name'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 52 × 3\n# Groups:   recipient_name [52]\n   recipient_name      recipient_clean     count\n   &lt;chr&gt;               &lt;chr&gt;               &lt;int&gt;\n 1 W HOTELS            W HOTELS                3\n 2 W. MILLAR & CO      W. MILLAR & CO.         1\n 3 W. MILLAR & CO.     W. MILLAR & CO.         5\n 4 WA METRO            WA METRO                1\n 5 WAGSHAL'S DELI      WAGSHAL'S DELI          1\n 6 WALMART SUPERCENTER WALMART SUPERCENTER    13\n 7 WALTERS             WALTERS                 1\n 8 WALTERS SPORT       WALTERS SPORT           1\n 9 WALTERS SPORTS      WALTERS SPORTS          1\n10 WALTER'S SPORTS BAR WALTERS SPORTS BAR      1\n# ℹ 42 more rows\nYou can see several changes on the second page of results, including that refinr made “PIZZA, WE THE” into “WE THE PIZZA” which is pretty smart. Other potential changes, grouping together “WASHINGTON NATIONALS” and “WASHINGTON NATIONALS BASEBALL CLUB”, didn’t happen. Key collision will do well with different cases, but all of our records are upper case.\nThere’s another merging algorithim that’s part of refinr that works a bit differently, called n_gram_merge(). Let’s try applying that one.\ncleaned_dc_food &lt;- dc_food |&gt;\n  mutate(recipient_clean=n_gram_merge(recipient_name)) |&gt;\n  select(recipient_name, recipient_clean, everything())\n\ncleaned_dc_food\n\n# A tibble: 13,108 × 50\n   recipient_name        recipient_clean committee_id committee_name report_year\n   &lt;chr&gt;                 &lt;chr&gt;           &lt;chr&gt;        &lt;chr&gt;                &lt;dbl&gt;\n 1 ST. REGIS HOTEL       ST. REGIS HOTEL C00658385    MARK GREEN FO…        2021\n 2 HARKER, GRAY          HARKER, GRAY    C00536540    JOBS, FREEDOM…        2021\n 3 REPUBLICAN NATIONAL … REPUBLICAN NAT… C00055582    NY REPUBLICAN…        2021\n 4 JOE'S SEAFOOD PRIME … JOE'S SEAFOOD,… C00551853    OORAH! POLITI…        2021\n 5 NATIONAL DEMOCRATIC … NATIONAL DEMOC… C00391029    JIM COSTA FOR…        2021\n 6 NATIONAL DEMOCRATIC … NATIONAL DEMOC… C00454694    MARCIA FUDGE …        2021\n 7 CAFE MILANO           CAFE MILANO     C00391029    JIM COSTA FOR…        2021\n 8 CAPITOL HILL CLUB     CAPITOL HILL C… C00383828    CONAWAY FOR C…        2021\n 9 CAPITOL HILL CLUB     CAPITOL HILL C… C00725465    CLIFF BENTZ F…        2021\n10 CAPITOL HILL CLUB     CAPITOL HILL C… C00383828    CONAWAY FOR C…        2021\n# ℹ 13,098 more rows\n# ℹ 45 more variables: report_type &lt;chr&gt;, image_number &lt;dbl&gt;,\n#   line_number &lt;chr&gt;, transaction_id &lt;chr&gt;, file_number &lt;dbl&gt;,\n#   entity_type &lt;chr&gt;, entity_type_desc &lt;chr&gt;,\n#   unused_recipient_committee_id &lt;chr&gt;, recipient_committee_id &lt;chr&gt;,\n#   recipient_state &lt;chr&gt;, beneficiary_committee_name &lt;chr&gt;,\n#   national_committee_nonfederal_account &lt;lgl&gt;, disbursement_type &lt;lgl&gt;, …\nTo examine changes refinr made with this algorithm, let’s again look at recipients starting with W. We see there wasn’t a substantial change from the previous method.\ncleaned_dc_food |&gt;\n  group_by(recipient_name, recipient_clean) |&gt;\n  summarise(\n    count=n()\n  ) |&gt;\n  filter(str_detect(recipient_clean, \"^w|^W\")) |&gt;\n  arrange(recipient_clean)\n\n`summarise()` has grouped output by 'recipient_name'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 51 × 3\n# Groups:   recipient_name [51]\n   recipient_name      recipient_clean     count\n   &lt;chr&gt;               &lt;chr&gt;               &lt;int&gt;\n 1 W HOTELS            W HOTELS                3\n 2 W. MILLAR & CO      W. MILLAR & CO.         1\n 3 W. MILLAR & CO.     W. MILLAR & CO.         5\n 4 WA METRO            WA METRO                1\n 5 WAGSHAL'S DELI      WAGSHAL'S DELI          1\n 6 WALMART SUPERCENTER WALMART SUPERCENTER    13\n 7 WALTERS             WALTERS                 1\n 8 WALTERS SPORT       WALTERS SPORT           1\n 9 WALTERS SPORTS      WALTERS SPORT           1\n10 WALTER'S SPORTS BAR WALTERS SPORTS BAR      1\n# ℹ 41 more rows\nThis method also made some good changes, but not in every case. No single method will be perfect and often a combination is necessary.\nThat’s how you use the Open Refine r package, refinr.\nNow let’s upload the data to the interactive version of OpenRefine, which really shines at this task.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Cleaning Part III: Open Refine</span>"
    ]
  },
  {
    "objectID": "open-refine.html#manually-cleaning-data-with-open-refine",
    "href": "open-refine.html#manually-cleaning-data-with-open-refine",
    "title": "16  Data Cleaning Part III: Open Refine",
    "section": "16.2 Manually cleaning data with Open Refine",
    "text": "16.2 Manually cleaning data with Open Refine\nOpen Refine is free software. You should download and install it; the most recent version is 3.6.0. Refinr is great for quick things on smaller datasets that you can check to make sure it’s not up to any mischief.\nFor bigger datasets, Open Refine is the way to go. And it has a lot more tools than refinr does (by design).\nAfter you install it, run it. (If you are on a Mac it might tell you that it can’t run the program. Go to System Preferences -&gt; Security & Privacy -&gt; General and click “Open Anyway”.) Open Refine works in the browser, and the app spins up a small web server visible only on your computer to interact with it. A browser will pop up automatically.\nYou first have to import your data into a project. Click the choose files button and upload a csv of the DC food-related expenditures.\n\n\n\n\n\n\n\n\n\nAfter your data is loaded into the app, you’ll get a screen to look over what the data looks like. On the top right corner, you’ll see a button to create the project. Click that.\n\n\n\n\n\n\n\n\n\nOpen Refine has many, many tools. We’re going to use one piece of it, as a tool for data cleaning. To learn how to use it, we’re going to clean the “recipient_name” field.\nFirst, let’s make a copy of the original recipient_name column so that we can preserve the original data while cleaning the new one.\nClick the dropdown arrow next to the recipient_name column, choose “edit column” &gt; “Add column based on this column”:\n\n\n\n\n\n\n\n\n\nOn the window that pops up, type “recipient_name_orig” in the “new column name” field. Then hit the OK button.\n\n\n\n\n\n\n\n\n\nNow, let’s get to work cleaning the recipient_name column.\nNext to the recipient_name field name, click the down arrow, then facet, then text facet.\n\n\n\n\n\n\n\n\n\nAfter that, a new box will appear on the left. It tells us how many unique recipient_names there are: 1,655. And, there’s a button on the right of the box that says Cluster.\n\n\n\n\n\n\n\n\n\nClick the cluster button. A new window will pop up, a tool to help us identify things that need to be cleaned, and quickly clean them.\n\n\n\n\n\n\n\n\n\nThe default “method” used is a clustering algorithim called “key collision”, using the fingerprint function. This is the same method we used with the refinr package above.\nAt the top, you’ll see which method was used, and how many clusters that algorithm identified. There are several different methods, each of which work slightly differently and produce different results.\n\n\n\n\n\n\n\n\n\nThen, below that, you can see what those clusters are. Right away, we can see how useful this program is. It identified 99 rows that have some variation on “Hawk N Dove” in the recipient_name field. It proposed changing them all to “HAWK N DOVE”.\nUsing human judgment, you can say if you agree with the cluster. If you do, click the “merge” checkbox. When it merges, the new result will be what it says in New Cell Value. Most often, that’s the row with the most common result. You also can manually edit the “New Cell Value” if you want it to be something else:\nNow begins the fun part: You have to look at all 88 clusters found and decide if they are indeed valid. The key collision method is very good, and very conservative. You’ll find that most of them are usually valid.\nBe careful! If you merge two things that aren’t supposed to be together, it will change your data in a way that could lead to inaccurate results.\nWhen you’re done, click Merge Selected and Re-Cluster.\nIf any new clusters come up, evaluate them. Repeat until either no clusters come up or the clusters that do come up are ones you reject.\nNow. Try a new method, maybe the “nearest neighbor levenshtein” method. Notice that it finds even more clusters - 167 - using a slightly different approach.\nRinse and repeat.\nYou’ll keep doing this, and if the dataset is reasonably clean, you’ll find the end.\nWhen you’re finished cleaning, click “Merge Selected & Close”.\nThen, export the data as a csv so you can load it back into R.\n\n\n\n\n\n\n\n\n\nA question for all data analysts – if the dataset is bad enough, can it ever be cleaned?\nThere’s no single definitive answer. You have to find it yourself.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Data Cleaning Part III: Open Refine</span>"
    ]
  }
]