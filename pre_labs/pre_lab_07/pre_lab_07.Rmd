---
title: "pre_lab_07.Rmd"
author: "sean mussenden"
date: "8/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Instructor demonstration points to hit
1. Ensure students install rvest package and load it.


## Chapter 15

### Task 1: Load packages
**Task** Run the following code to load packages.

```{r}
library(rvest)
library(tidyverse)
library(janitor)
```

For this example, we're going to work on loading a simple table of data from the Bureau of Labor Statistics. This is a table of industry sectors (each with a two-digit NAICS code) that we could make use of in our analysis of PPP loan data.

Recall that our PPP loan data has six-digit NAICS codes for each industry, which allows us to identify the industry for each loan.  For example 212221 is the code for "Gold Mining Industry".

A six-digit NAICS code is the most specific.  As we remove numbers from the right to create five-digit, four-digit, three-digit and two-digit codes, the industries they represent get broader. Here's an example:

* 212221 - Gold Mining Industry
* 2122 - Metal Ore Mining Industry (which includes gold mining and things like silver mining, iron mining and copper mining)
* 21 - Mining, Quarrying, and Oil and Gas Extraction Industry (which contains the metal mining industries mentioned above, but also oil drilling, coal mining and more).

It might be useful to have a lookup table of those top-level, two-digit NAICS codes (also called sector codes) for our analysis, to help us answer questions about what specific top-level industries got loans.  L

Let's suppose we can't find a table like that for download, but we do see a version on the BLS website at this URL: [https://www.bls.gov/ces/naics/](https://www.bls.gov/ces/naics/).

### Task 2: Go to the web page linked above
**Task** Visit the web page linked above. Briefly describe what you see there.
**Answer**

### Task 3: Load image
**Task** Run the following code to display an image showing what you should see on the web page.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest1.png"))
```
We could get this table into R with the following manual steps: highlighting the text, copying it into Excel, saving it as a csv, and reading it into R.  

Or, we could write a few lines of webscraping code to have R do that for us!

In this simple example, it's probably faster to do it manually than have R do it for us. And this table is unlikely to change much in the future.

Why would we ever write code to grab a single table? There's several reasons:

1. Our methods are transparent.  If a colleague wants to run our code from scratch to factcheck our work, they don't need to repeat the manual steps, which are harder to document than writing code.
2. Let's suppose we wanted to grab the same table every day, to monitor for changes (like, say, a table on a health department website that has COVID case numbers that update every day).  Writing a script once, and pressing a single button every day is going to be much more efficient than doing this manually every day.
3. If we're doing it manually, we're more likely to make a mistake, like maybe failing to copy every row from the whole table.
4. It's good practice to prepare us to do more complex scraping jobs.  As we'll see in the next chapter, if we ever want to grab the same table from hundreds of pages, writing code is much faster and easier than going to a hundred different pages ourselves and downloading data.

So, to scrape, the first thing we need to do is start with the URL. Let's store it as an object called naics_url.

### Task 4: Store URL
**Task** Run the following code to store the URL.

```{r}
naics_url <- "https://www.bls.gov/ces/naics/"
```

When we go to the web page, we can see a nicely-designed page that contains our information.  

But what we really care about, for our purposes, is the html code that creates that page.  

In our web browser, if we right-click anywhere on the page and select "view source" from the popup menu, we can see the source code.  Or you can just copy this into Google Chrome: view-source:https://www.bls.gov/ces/naics/.

### Task 3: View source
**Task** Follow the directions in the previous paragraph. Briefly describe what you see there.
**Answer**

Here's a picture of what some of the source code looks like.

### Task 4: Load image
**Task** Run the following code to display an image showing what you should see on the web page.
```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest2.png"))
```
We'll use those HTML tags -- things like `<div>` and `<a>` and `<table>` -- to grab the info we need.

Okay, step 1.  

Let's write a bit of code to tell R to go to the URL for the page and ingest all of that HTML code. In the code below, we're starting with our URL and using the read_html() function from rvest to ingest all of the page html, storing it as an object called naics_industry.

### Task 5: Run code to read in html
**Task** Run the following code to read in the html. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}
# read in the html
naics_industry <- naics_url %>%
  read_html()

# display the html below
naics_industry

```

If you're running this code in R Studio, in our environment window at right, you'll see naics_industry as a "list of 2".  

This is not a dataframe, it's a different type of data structure a "nested list."

If we click on the name "naics_industry" in our environment window, we can see that it's pulled in the html and shown us the general page structure.

### Task 6: Examine html
**Task** Follow the directions in the previous paragraph to click on naics_industry in the environment window. Briefly describe what you see there.
**Answer**

Nested within the `<html>` tag is the `<head>` and `<body>`, the two fundamental sections of most web pages. We're going to pull information out of the `<body>` tag in a bit.

### Task 7: Load image
**Task** Run the following code to display an image showing what you should see when clicking on naics in the environment window.
```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest3.png"))
```

Now, our task is to just pull out the section of the html that contains the information we need.  

But which part do we need from that mess of html code? To figure that out, we can go back to the page in a web browser like chrome, and use built in developer tools to "inspect" the html code underlying the page.  

On the page, find the data we want to grab -- "Table 2. NAICS Sectors" - and right click on the word "Sector" in the column header of the table.  That will bring up a dropdown menu. Select "Inspect", which will pop up a window called the "element inspector" that shows us where different elements on the page are located, what html tags created those elements, and other info.

### Task 8: Inspect element
**Task** Follow the directions in the previous paragraph. Briefly describe what you see there.
**Answer**

### Task 9: Load image
**Task** Run the following code to display an image showing what you should see on the web page.
```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest4.png"))
```

The entire table that we want of naics sectors is actually contained inside an html `<table>`. It has a header row `<thead>` that contains the column names and a `<tbody>` that contains one row `<tr>` per industry sector code.

Because it's inside of a table, and not some other kind of element (like a `<div>`), rvest has a special function for easily extracting and converting html tables, called html_table(). This function extracts all six html tables on the page, only one of which we actually want.

### Task 10: Run code to process tables
**Task** Run the following code to process tables. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}
# read in the html and extract all the tables
naics_industry <- naics_url %>%
  read_html() %>%
  html_table()

# display the tables below
naics_industry

```

In the environment window at right, look at naics_industry.  Note that it's now a "list of 6".  

Click on it to open it up.  

### Task 11: Inspect naics_industry.
**Task** Follow the directions in the previous paragraph. Briefly describe what you see there.
**Answer**

It should look like this.

### Task 12: Load image
**Task** Run the following code to display an image.
```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest5.png"))
```
This gets a little complicated, but what you're seeing here is a nested list that contains six different data frames -- also called tibbles --  one for each table that exists on the web page we scraped.

They're numbered 1 to 6.  The first 1 has 4 rows and 3 columns, the second has 21 rows and 2 columns, and so on.   

To examine what's in each dataframe, mouse over the right edge (next to the word columns) on each row, and click the little scroll icon.  The icon will be hidden until you mouse over it.  

Click on the scroll icon for the first dataframe examine it.

### Task 13: Click scroll icon
**Task** Follow the directions in the previous paragraphs. Briefly describe what you see there.
**Answer**

### Task 14: Load image
**Task** Run the following code to display an image showing what you should see when you follow the steps in task 13.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest6.png"))
```

That's not the one we want!  

Let's try clicking on the scroll icon for row 2.

### Task 15: Click scroll icon
**Task** Follow the directions in the previous paragraph. Briefly describe what you see there.
**Answer**

### Task 16: Load image
**Task** **Task** Run the following code to display an image showing what you should see when you follow the steps in task 15.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest7.png"))
```

That's more like it! So, all we need to do now is to store that single dataframe as an object, and get rid of the rest.  We can do that with this code, which says "keep only the second dataframe from our nested list. If we wanted to keep the third one, we'd change the number 2 to number 3.

### Task 17: Run code to keep only table we want
**Task** Run the following code to keep only the table we want. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}
# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url %>%
  read_html() %>%
  html_table()

# Just keep the second dataframe in our list

naics_industry <- naics_industry[[2]]

# show the dataframe

naics_industry

```

We now have a proper dataframe.

From here, we can do a little light cleaning. Let's use clean_names() to standardize the column names.  Then let's use slice() to remove the last row -- row number 21 -- which contains source information that will complicate our use of this table later.

### Task 18: Run code to do some light cleaning
**Task** Run the following code to do light cleaning. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}
# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry <- naics_url %>%
  read_html() %>%
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row

naics_industry <- naics_industry[[2]] %>%
  clean_names() %>%
  slice(-21)

# show the dataframe
naics_industry

```

And there we go. We now have a nice tidy dataframe of NAICS sector codes.  

In the next chapter, we'll look at a more complicated example.

## Chapter 16

In the last chapter, we demonstrated a fairly straightforward example of web scraping to grab a list of NAICS industry sector codes from the BLS website.  

We're going to graduate to a more challenging example, one that will help us gather information about the number of employees in each industry sector.

What makes this more challenging?  Well, the information we need is all contained on multiple pages, one page per sector. We need to write code to visit each page, and then merge them into a single data frame.
This is challenging stuff, so don't feel dissuaded if it all doesn't click the first time through.  Like many things, web scraping is something that gets easier with lots of practice.

First we start with libraries, as we always do.

### Task 1: Load packages
**Task** Run the following code to load packages.

```{r}
library(rvest)
library(tidyverse)
library(janitor)
```

Now, let's run the code we wrote in the last chapter, to get a tidy list of NAICS sector codes and names from [https://www.bls.gov/ces/naics/]("https://www.bls.gov/ces/naics/").

### Task 2: Run code to load the table from the last chapter
**Task** Run the following code to load table from last chapter. Briefly describe the output that appears below the codeblock.
**Answer**
```{r}
# Define url of page we want to scrape

naics_url <- "https://www.bls.gov/ces/naics/"

# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url %>%
  read_html() %>%
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row

naics_industry <- naics_industry[[2]] %>%
  clean_names() %>%
  slice(-21)

# show the dataframe
naics_industry

```

We'll use this table to help us get to our end goal: a single dataframe with the number of employees in each industry sector.

It will look like this when we're done.  

### Task 3: Load image
**Task** Run the following code to display an image showing what you should see on the web page.
```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest1a.png"))
```

Unfortunately, that information doesn't exist in a single tidy table on a single page we can scrape all at once.  We're going to have to scrape it from lots of different pages, and build it ourselves.

Let's next take a look at the web page that has detailed employment information for one of our sectors, 22, Mining, Quarrying, and Oil and Gas Extraction.  

We can find it here: [https://www.bls.gov/iag/tgs/iag22.htm](https://www.bls.gov/iag/tgs/iag22.htm).

### Task 4: Go to the web page linked above
**Task** Visit the web page linked above. Briefly describe what you see there.
**Answer**

A few scrolls down the page, there's a table that has employee statistics.  

The table is called "Employment and Unemployment". There's a row in the tabled for "Employment, all employees (seasonally adjusted)".  And in that row, there's a value for the number of employees -- in thousands -- in June 2021.  

The table shows that for the mining sector, it was 538.6  -- or 538,600 -- in June 2021.  That's the value we want to ingest in R. 

### Task 5: Load image
**Task** Run the following code to display an image showing what you should see on the web page.
```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest1.png"))
```

We don't just want it for mining.  We want it for all sectors!  

But we'll start by writing code just to get it from this one sector page, then modify that code to get it from every sector's page

First, let's define the URL of the page we want to get the information from.

### Task 6: Store URL
**Task** Run the following code to store the URL.

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"

```

Next, let's read in the html of that page, and store it as an object called employment_info.

### Task 7: Run code to read in html
**Task** Run the following code to read in the html. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"

# Get employment html
employment_info <- url %>%
  read_html()  

# Display it so we can see what it looks like
employment_info

```

Now, let's set to picking out the information we need from the raw html.

We can use the web inspector in our web browser (Chrome) to figure out where the table is located.

Go to the web page and right click on the word "Data Series" in the table, then pick "inspect" to pull up the menu.

Notice two things.  First, all of this information is contained in a proper html `<table>`.  And that table has an id property of "iag22emp1". Designers use these IDs to help style the page, to target certain elements with CSS. We can use it to scrape.

### Task 8: Load image
**Task** Run the following code to display an image showing what you should see on the web page.
```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest2.png"))
```
Recall that in the last chapter, when we used the html_table() function, it pulled in every single table on the page, six in total.  

Here, we can use that id property to pick out just the table we want, and leave all the others behind.

We do that with a new function from rvest called html_element(), employing a bit of information about that element stored in what's called the  [xpath](https://en.wikipedia.org/wiki/XPath). Xpath is a query language that helps us write programs that target specific parts of web pages.  

The syntax is a little unwieldy, I know.

But essentially what the html_element function says is "find the html element that has an id of iag22emp1, using the xpath method, and get rid of all other elements".

### Task 9: Run code to get html_element with info we need
**Task** Run the following code to get html_element with info we need. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"

# Get employment html page and select only the table with employment information
employment_info <- url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="iag22emp1"]')

# Display it so we can see what it looks like
employment_info
```

We've now isolated the table on the page that contains the information we need, and gotten rid of everything else.

From here, we can use the html_tables() function to transform it from messy html code to a proper dataframe.

### Task 10: Run code to convert to table
**Task** Run the following code to convert to table. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="iag22emp1"]') %>%
  html_table()

# Display it so we can see what it looks like
employment_info
```

Now we have a proper dataframe of 6 rows and 6 columns.  

It has much more information than we need, so let's clean it up to isolate only the "Employment, all employees (seasonally adjusted)" value for Nov. 2021.

Use clean_names() to standardize the column names, use slice() to keep only the second row, and use select() to keep two columns data_series and nov_2021.

### Task 11: Run code to keep row 2 and light cleaning
**Task** Run the following code to keep row 2 and light cleaning. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag21.htm"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="iag21emp1"]') %>%
  html_table()

# Keep only second row with seasonally adjusted, bind back to each_row_df
employment_info <- employment_info %>%
  clean_names() %>%
  slice(2) %>%
  select(data_series, nov_2021)

# Display it so we can see what it looks like
employment_info
```

Okay, so we've successfully obtained the employment numbers for one of our sectors. That's great.

But remember our original charge: to get a table with employment numbers for ALL sectors, not just one.

This is a little tricky, because, remember, the information for each sector is on a different page!

The info for mining is on this page: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm).

### Task 12: Go to the web page linked above
**Task** Visit the web page linked above. Briefly describe what you see there.
**Answer**

The info for construction is on this page: [https://www.bls.gov/iag/tgs/iag23.htm](https://www.bls.gov/iag/tgs/iag23.htm).

### Task 13: Go to the web page linked above
**Task** Visit the web page linked above. Briefly describe what you see there.
**Answer**

We have 20 sectors to get through.  

We could get the info we need by copying the codeblock we just wrote 20 times, and change the url at the top each time.  

But that's not a great approach.  

What if we needed to change the code? We'd need to change it 20 times!
In programming, there's a principle called "DRY" which stands for "Don't Repeat Yourself".  If you find yourself copying the same code over and over again, with minor changes, it's better to find a way to avoid that.

## Using for loops

Fortunately, there's a programming paradigm called "iteration" that is helpful here, using a method called a "for loop".

Every programming language has its own version of a "for loop", and R is no different.

A "for loop" says: "let's take a list of things, and do the same thing to each item on that list."   

Let's look at a very simple example to help illustrate the values of for loops.

We're going to write code to print out 10 industry sectors.  

First, let's do it the repetitive way.  We're writing the same print function over and over, just changing the sector name each time.

### Task 14: Run code to print sectors
**Task** Run the following code to print sectors. Briefly describe the output that appears below the codeblock.
**Answer**
```{r}

print("Agriculture, Forestry, Fishing and Hunting")
print("Mining, Quarrying, and Oil and Gas Extraction")
print("Utilities")
print("Construction")
print("Manufacturing")
print("Wholesale Trade")
print("Retail Trade")
print("Transportation and Warehousing")
print("Information")
print("Finance and Insurance")

```

We repeated print() 10 times, with minor modifications each time.  Lots of repetition, which we seek to avoid if possible.

Now let's look at how we might do that a little more efficiently with a "for loop."

First let's make a list of sectors, and save it as an object called "list_of_sectors." The c() function tells R that we're making a list.

### Task 15: Run code to make list of sectors
**Task** Run the following code to make list of sectors. Briefly describe the output that appears below the codeblock.
**Answer**
```{r}
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

```

And now let's write a "for loop" to print out sector on that list.

### Task 16: Run code to use for loop to print sectors
**Task** Run the following code to use a for loop to print list of sectors. Briefly describe the output that appears below the codeblock.
**Answer**
```{r}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# Make a for loop and run it
for (sector in list_of_sectors) {
  print(sector)
}


```

That's many fewer lines of code.  Let's break down what we just saw, starting with for `(sector in list_of sectors)`.

The information inside the parentheses tells R what list to use -- list_of_sectors -- and how to identify list elements later on -- sector.

It's important that the thing on the right side of "in" use the exact name of the list we want to loop through -- in this case "list_of_sectors".

If we try to feed it something different -- say "sector_list" -- it won't work, because our actual list is called something else -- "list_of_sectors". This code throws an error.

### Task 17: Run code to use for loop to print sectors that errors
**Task** Run the following code to use a for loop to print list of sectors that errors. Briefly describe the output that appears below the codeblock.
**Answer**

```{r, error=TRUE}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# For loop that refers to a list that doesn't exist!  
for (sector in sector_list) {
  print(sector)
}


```

The name on the left side of "in" -- the word we're assigning to represent each element -- is totally arbitrary.  

We could use any character string, even something simple like "x".  

What matters is that we use the same character string inside of the curly braces {}, the section of the "for loop" that tells R what to do to each element -- in this case, print it out.    

To illustrate this, note that the code works just fine if we change it to say this:

### Task 18: Run code to use for loop to print sectors
**Task** Run the following code to use a for loop to print list of sectors. Briefly describe the output that appears below the codeblock.
**Answer**
```{r}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# For loop with x that stands in for each element in our list, instead of sector
for (x in list_of_sectors) {
  print(x)
}


```

But it does NOT work if we call each element one thing -- x -- in the first line of our "for loop", and use a different name to refer to it inside of the curly braces.

In this code below, it has no idea what we mean by "sector_name", because we haven't defined that anywhere.  

### Task 19: Run code to use for loop to print sectors that errors
**Task** Run the following code to use a for loop to print list of sectors. Briefly describe the output that appears below the codeblock.
**Answer**
```{r, error=TRUE}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# For loop that includes instructions that refer to a variable that doesn't exist.
for (x in list_of_sectors) {
  print(sector_name)
}


```

We can also write for loops to iterate over a range of numbers, instead of a list of characters.  The syntax is a little different.

The code below says: "for each number in a range of numbers from 1 to 10, print the number."  

### Task 20: Run code to use for loop to print 1 to 10
**Task** Run the following code to use a for loop to print 1 to 10. Briefly describe the output that appears below the codeblock.
**Answer**
```{r}
# For loop that includes instructions that refer to a variable that doesn't exist.
for (number in 1:10) {
  print(number)
}
```

Here's a minor variation on that approach that we'll make use of below.  

Instead of giving the for loop an explicit number range, like 1:10, we can tell it to use 1 to "the number of rows in a dataframe" as our list of things to loop through.

Remember the naics_industry dataframe we loaded first? It has 20 rows.
### Task 21: Run code to display naics_industry
**Task** Run the following code to display naics_industry. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

naics_industry

```

We can use that information in our for loop by using the nrow() function, which calculates the number of rows in a dataframe.  Here's a quick demonstration of how that works.

### Task 22: Run code to display number of rows in naics_industry
**Task** Run the following code to display number of rows naics_industry. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}
nrow(naics_industry)
```

To put it all together, the code below says "make a list of numbers that starts at 1 and ends at the number of rows in the naics_industry dataframe (which is 20), then print out each of these numbers."

### Task 23: Run code to print each row number in naics_industry
**Task** Run the following code to to print each row number in naics_industry. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}
# For loop that includes instructions that refer to a variable that doesn't exist.
for (row_number in 1:nrow(naics_industry)) {
  print(row_number)
}
```

These were basic examples of how "for loops" work.  Next, we'll learn to apply "for loops" to efficentily extract information from multiple web pages.

## Looping and rvest

First, let's look at the codeblock we wrote earlier to extract the number of employees in the mining sector.

### Task 24: Run code to load table from earlier
**Task** Run the following code to load table from earlier. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag21.htm"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url %>%
  read_html() %>%
  html_element(xpath = '//*[@id="iag21emp1"]') %>%
  html_table()

# Keep only second row with seasonally adjusted, bind back to each_row_df
employment_info <- employment_info %>%
  clean_names() %>%
  slice(2) %>%
  select(data_series, nov_2021)

# Display it so we can see what it looks like
employment_info
```
This contains all the steps we needed to extract the information from one sector page. We're now going to modify this function so we can use it to extract information from each sector page, writing code that keeps us from repeating ourselves too much.

First, we need to build a list of URLs to loop through in a "for loop." We can do that using the dataframe we made in the last chapter.
### Task 25: Run code to load the table from the last chapter
**Task** Run the following code to load table from last chapter. Briefly describe the output that appears below the codeblock.
**Answer**
```{r}
# Define url of page we want to scrape

naics_url <- "https://www.bls.gov/ces/naics/"

# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url %>%
  read_html() %>%
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row

naics_industry <- naics_industry[[2]] %>%
  clean_names() %>%
  slice(-21)

# show the dataframe
naics_industry

```

This gives us the sector code and name for each industry.

Now let's have a look at the URLs for a few of the pages we want to grab data from.

* Mining, Quarrying, and Oil and Gas Extraction: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm).
* Utilities: [https://www.bls.gov/iag/tgs/iag22.htm](https://www.bls.gov/iag/tgs/iag22.htm).
* Construction: [https://www.bls.gov/iag/tgs/iag23.htm](https://www.bls.gov/iag/tgs/iag23.htm)

Notice a pattern?

They all start with "https://www.bls.gov/iag/tgs/iag".  The next bit of information is different for each one; with the two-digit sector code for each sector.  The remainder is identical in all three links, ".htm".

Because they're all the same, we can use the information in the dataframe we just loaded to make all the URLs we need.

We're going to use mutate() and paste0() to concatenate (mash together) the things that stay constant in every url (the beginning and end) with the things that are different (the sector number, stored in the column called sector).

### Task 26: Run code to build url
**Task** Run the following code to build url. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# Make a column with URL for each sector.
naics_industry <- naics_industry %>%
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm"))

# Display it
naics_industry
```

While we're at it, we're going to use the same method to programatically build the "xpath" for the table on each sector page.  

Recall that when we wrote our function that got information from just the mining page, the xpath targeted an element with an ID of "iag21emp1".  Why 21? That's the sector code for mining.  

If we look for that exact element ID on other sector pages, we won't find it! That's because it's different for each page.

On the Utilities page (sector code 22), the ID for the table we want is "iag22emp1".  On the Construction page (sector code 23), it's "iag23emp1". We can also build this programatically, because it follows a predictable pattern.

### Task 27: Run code to build id
**Task** Run the following code to build id. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# Make a column with URL and xpath ID for each sector
naics_industry <- naics_industry %>%
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm")) %>%
  mutate(sector_xpath_id =paste0("iag",sector,"emp1"))

# Display it
naics_industry
```

Lastly, we're going to use filter to remove the "Public Administration" sector, because there's no page for it. We'll have to get that information some other way.

### Task 28: Run code to filter table
**Task** Run the following code to filter table. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# Make a column with URL and xpath ID for each sector, remove the Public Administration sector
naics_industry <- naics_industry %>%
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm")) %>%
  mutate(sector_xpath_id =paste0("iag",sector,"emp1")) %>%
  filter(description != "Public Administration")
# Display it
naics_industry
```

We're left with a dataframe of 19 rows and 4 columns. It now contains everything we need.

Next, we'll construct a "for loop" to extract the info we need from each page. We're going to build it up step-by-step, beginning with the the basic elements of our "for loop".

The codeblock below says: "Make a list with the row numbers from 1 to the number of rows in our naics_industry dataframe (which is 19). Then, for each element of that list (1, 2, 3, 4, 5 and so on up to 19), use slice() to keep only the one row that matches that number and save this newly created dataframe as each_row_df. Print out the dataframe. Then go to the next element on the list and do the same thing.  Keep doing that until we hit number 19, then stop."   

We get 19 dataframes, each with one row, one for each sector.

### Task 29: Run code to run for loop and keep one row
**Task** Run the following for loop and keep one row. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(each_row_df)

}
```

We're almost to the part where we can go out and fetch the html we need. Before we do that, let's store as part of our loop an object called "url", which contains the URL of the page for each sector.

The syntax with the dollar sign is a little funky, but "each_row_df$sector_url" says "from the each_row_df dataframe, grab the information in the sector_url column." Because the column has only one row, there's one value.

We're going to do something simliar with the xpath for our employment table by using the information in the sector_xpath_id column.

That code also looks a little unwieldly.  Recall that the xpath for the mining industry was `'//*[@id="iag22emp1"]'`.  

In the code below, we're building the xpath dynamically by pasting together the parts that stay the same for each xpath -- `'//*[@id="'` and `'"]'` -- and the parts that change for each sector, pulled from the xpath_sector_id column.

To see how this is working, we're going to edit our print statement at the end a bit, printing the row_number and the dynamically created url and xpath.

### Task 30: Run code to run for loop to store url and xpath value
**Task** Run the following for loop to store url and xpath value. Briefly describe the output that appears below the codeblock.
**Answer**
```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(paste0("ROW NUMBER:", row_number," URL: ",url," XPATH:",xpath_employment_table))

}
```

Armed with the URL and xpath for each sector web page, we can now go out and get the employment table for each sector.

We'll read in the html from the url we just stored; extract the table that has the xpath ID we just created; and then transform the html table code into a proper dataframe.

The dataframe is hidden inside  a nested list, which we'll have to extract in the next step.

So, when you run this code, it will print out 19 dataframes inside of nested lists, each containing one dataframe.

### Task 31: Run code to run for loop to get tables
**Task** Run the following for loop to get tables. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table(). The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}
```
In this next step, we use employment_info <- employment_info[[1]]  to extract each dataframe from the nested list. Then we'll tidy up the dataframe a bit. We'll use the get rid of all the information we don't need in the table, by using slice() to keep only the second row. We'll also standardize the column names with clean_names().

### Task 32: Run code to run for loop to clean up tables
**Task** Run the following for loop to clean up tables. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row;
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}
```
We now have 19 dataframes, each containing one row each and two columns, one of which is the employment number for a given sector for jun_2021. But we're missing information about what industry sector these employment numbers represent.

We can add that back in by using bind_cols() to reconnect the each_row_df, which contains the sector code and the sector name.

### Task 33: Run code to run for loop to add in data
**Task** Run the following for loop to add in data. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table.
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) %>%
      bind_cols(each_row_df)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}
```
Then we'll do a little bit of cleaning.

Let's use parse_number() to remove the comma from the jun_2021 number and convert it from a character to number. We'll use rename() to make the jun_2021 column name a little more descriptive. And then we'll use select() to keep only the columns we want to keep -- the sector number, the sector name, and the jun_2021 employment number.

### Task 34: Run code to run for loop to clean up tables
**Task** Run the following for loop to clean up tables. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table; turn jun_2021 column into a proper number, and rename it.  Then select only three columns we need.
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) %>%
      bind_cols(each_row_df) %>%
      mutate(nov_2021 = parse_number(nov_2021)) %>%
      rename(nov_2021_employees = nov_2021) %>%
      select(sector,description,nov_2021_employees)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}

```


We're getting very close to the finished table we showed at the beginning.  

But right now, each bit of sector information is separated between 19 different dataframes.  

We want them in one dataframe.  

We can fix this by creating an empty dataframe called "employment_by_sector_all" using tibble(), placing it before our "for loop".

And inside our "for loop" at the end, we'll bind each employment_info dataframe to the newly created empty dataframe.  

### Task 35: Run code to run for loop to combine tables into a single table
**Task** Run the following for loop combine tables into a single table. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}

# Create an empty dataframe to hold results
employment_by_sector_all <- tibble()

# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry %>%
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url %>%
      read_html() %>%
      html_elements(xpath = xpath_employment_table) %>%
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table; turn jun_2021 column into a proper number, and rename it.  Then select only three columns we need.
    employment_info <- employment_info[[1]] %>%
      clean_names() %>%
      slice(2) %>%
      bind_cols(each_row_df) %>%
      mutate(nov_2021 = parse_number(nov_2021)) %>%
      rename(nov_2021_employees = nov_2021) %>%
      select(sector,description,nov_2021_employees)

    # Bind each individual employment info table to our employment_by_sector_all dataframe
    employment_by_sector_all <- employment_by_sector_all %>%
      bind_rows(employment_info)

}

# Display the completed dataframe
employment_by_sector_all
```

Ta da! The end result is a nice tidy dataframe with the number of employees in June 2021 for each sector.

It's always a good idea to spot check the results, especially any values that look suspiciously high or low.

The value for "Agriculture, Forestry, Fishing and Hunting" seems suspiciously low, compared with the other values.  

Let's figure out why.  

Here's the table on the mining sector page: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm)

### Task 36: Go to the web page linked above
**Task** Visit the web page linked above. Briefly describe what you see there.
**Answer**

### Task 37: Load image
**Task** Run the following code to display an image showing what you should see on the web page.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest3.png"))
```
And here's the table for the agriculture sector.

[https://www.bls.gov/iag/tgs/iag11.htm](https://www.bls.gov/iag/tgs/iag11.htm)

### Task 38: Go to the web page linked above
**Task** Visit the web page linked above. Briefly describe what you see there.
**Answer**

### Task 39: Load image
**Task** Run the following code to display an image showing what you should see on the web page.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest4.png"))
```
Unlike mining -- and every other sector page (I checked each page) -- the agriculture page is structured differently.  

In the second row of this table, it has the unemployment rate. Nowhere on the page can we find information on the number of employees.  We would need to do additional research to track down a valid number if we plan on using this table, but for now we're going to replace it with an NA using na_if.

### Task 40: Run code to remove agriculture from table
**Task** Run the following code to remove agriculture from table. Briefly describe the output that appears below the codeblock.
**Answer**

```{r}
# remove the suspicious value for agriculture.
employment_by_sector_all <- employment_by_sector_all %>%
  mutate(nov_2021_employees = na_if(nov_2021_employees,5.4))

# display it
employment_by_sector_all
```

And we're done.  

A note about advanced scraping -- every site is different. Every time you want to scrape a site, you'll be puzzling over different problems. But the steps remain the same: find a pattern, exploit it, clean the data on the fly and put it into a place to store it.
