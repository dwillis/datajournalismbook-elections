---
title: "pre_lab_12.Rmd"
author: "derek willis"
date: "11/18/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Automating analysis

Many of the data analyses that you do will be largely one-off efforts -- you're going to do the analysis and write the story and be done. Maybe you'll come back to it in a couple of months or years, but really you're just doing it once.

But what happens when you have a long-running story, where you're going to update it every week, or every month? What changes when you're writing that code?

1. How will this run again without changing anything?
2. What questions do you have that have to be answered each time?
3. What changes when you have to repeat questions to changing data?

Voter registration is a key component of elections, and it's updated regularly in most states. So it is an ideal candidate for repeating analysis -- scripting the questions we want to answer every month and doing so in a way that we can just load it without having to change anything.

Here's a real-world example: you are covering politics in Maryland and you want to see how voter registration is changing over time. You could grab the monthly PDFs, load them into Tabula and manually extract the information, load it into R and then do some calculations, but there's a better way. Instead, let's build a system that goes out each month to retrieve the data, parse it and make it easier to analyze.

Let's get some other libraries to our typical tidyverse import. We'll start with janitor to help wrangle the data, and we're also going to add a library called `tabulizer`, which does in code what you do manually using Tabula.

You install it in a slightly different way than usual: `remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"))`.

### Task 1: Load libraries and settings
**Task** Run the following code in the gray-colored codeblock below to load the libraries we'll use

```{r}
#remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"))
library(tidyverse)
library(janitor)
library(tabulizer)
library(glue)
```

## Automating downloads and imports

Now, where to find the data?

Maryland [publishes monthly voter registration data on the state Board of Elections website](https://elections.maryland.gov/voter_registration/stats.html).

Unfortunately, those monthly reports are in PDF format, as we learned in Chapter 22. Tabula makes it possible for us to handle them individually, but what if we wanted to put in place a repeatable system for parsing the data that we could just run every month?

We can!

First we'll see how it's done on a single PDF that we specify.

### Task 1: Download a PDF file via code
**Task** Run the following code to download the most recent PDF (https://elections.maryland.gov/pdf/vrar/2022_10.pdf). Check to see that it's in your pre_lab_12 directory.

```{r}
download.file("https://elections.maryland.gov/pdf/vrar/2022_10.pdf", "2022_10.pdf")
```

Having grabbed the file, we're going to try extracting the both pages from that PDF and focus on the first one, with new party registrations and removals. That's what the `tabulizer` library does using the function `extract_tables`:

### Task 2: Read the PDF using Tabulizer
**Task** Run the following code and describe the output.
**Answer**

```{r}
tables <- extract_tables("2022_10.pdf")
View(tables[[1]])
```

Ok, so the information from the first page is there, but this data needs some work to make it usable. In fact, we probably want to make two dataframes here: one for the new registrations and one for the removals. First: make it a dataframe using `as_tibble()`, then cleaning up the columns and giving them proper names and datatypes. Finally, we'll save the result as its own dataframe, `new_registrations`.

### Task 3: Make the new_registrations dataframe
**Task** Run the following code and describe the result.
**Answer** 

```{r}
# extract the first page and turn it into a dataframe, cleaning the column names
voter_reg_page_1 <- tables[[1]] %>% as_tibble(.name_repair = "unique") %>% clean_names()

# remove the first two rows
voter_reg_page_1 <- voter_reg_page_1 %>% slice(-(1:2))

# make the new dataframe, renaming the columns and ditching the columns we don't want
new_registrations <- voter_reg_page_1 %>% 
  # new registration columns
  rename(method = x2, DEM = x3, REP = x4, GRN = x5, LIB = x6, WCP = x7, UNA = x8, OTH = x9, total = x10, dupes = x11) %>% 
  # remove commas and fix the datatypes
  mutate(DEM = as.numeric(gsub(",","",DEM)), REP = as.numeric(gsub(",","",REP)), GRN = as.numeric(gsub(",","",GRN)), LIB = as.numeric(gsub(",","",LIB)), WCP = as.numeric(gsub(",","",WCP)), UNA = as.numeric(gsub(",","",UNA)), OTH = as.numeric(gsub(",","",OTH)), total = as.numeric(gsub(",","",total)), dupes = as.numeric(gsub(",","",dupes))) %>% 
  # exclude the columns we don't want
  select(-starts_with("x")) %>% 
  # add a month and year column
  mutate(month = 10, year = 2022)

View(new_registrations)
```

Ok, that's one dataframe down. The good news is that making the second dataframe involves similar code, and we can start with our `voter_reg_page_1` dataframe that we made in the previous task:

### Task 4: Make the removals dataframe
**Task** Run the following code and describe the result.
**Answer** 

```{r}
# make the new dataframe, renaming the columns and ditching the columns we don't want
removals <- voter_reg_page_1 %>% 
  # removals columns
  rename(method = x13, DEM = x14, REP = x15, GRN = x16, LIB = x17, WCP = x18, UNA = x19, OTH = x20, total = x21) %>% 
  # remove commas and fix the datatypes
  mutate(DEM = as.numeric(gsub(",","",DEM)), REP = as.numeric(gsub(",","",REP)), GRN = as.numeric(gsub(",","",GRN)), LIB = as.numeric(gsub(",","",LIB)), WCP = as.numeric(gsub(",","",WCP)), UNA = as.numeric(gsub(",","",UNA)), OTH = as.numeric(gsub(",","",OTH)), total = as.numeric(gsub(",","",total))) %>% 
  # exclude the columns we don't want
  select(-starts_with("x")) %>% 
  # exclude rows with NAs
  filter(!is.na(DEM)) %>% 
  # add a month and year column
  mutate(month = 10, year = 2022)

View(removals)
```

Now we can start to interview this data. Let's do a little exploration using percentages.

## Explore the data

Let's start with the `new_registrations` dataframe and figure out the percentage of the new registrations for Democrats, Republicans and unaffiliated voters.

### Task 5: Calculate percentages for new registrations
**Task** Run the following code and describe what you think are the most interesting percentages in the dataframe.
**Answer**

```{r}
new_registrations <- new_registrations %>% 
  mutate(pct_dem = DEM/total *100, pct_rep = REP/total *100, pct_una = UNA/total *100)

View(new_registrations)
```

### Task 6: Calculate percentages for removals
**Task** Run the following code and describe what you think are the most interesting percentages in the dataframe.
**Answer**

```{r}
removals <- removals %>% 
  mutate(pct_dem = DEM/total *100, pct_rep = REP/total *100, pct_una = UNA/total *100)

View(removals)
```

So we have a process for automatically extracting tables from this most recent PDF. What if we wanted to parse a specific PDF based on the year and month, for comparison's sake? There are a couple of ways we can do that. First, we could set those as variables in code and use them to grab the correct file. We'll use a function called `paste0` to build the URL from its parts:

### Task 7: Download a PDF file based on month and day variables
**Task** Run the following code to download the voter registration PDF from June by building the URL for it programatically (https://elections.maryland.gov/pdf/vrar/2022_06.pdf). Check to see that it's in your pre_lab_12 directory.

```{r}
year = 2022
month = "06"
url <- paste0("https://elections.maryland.gov/pdf/vrar/",year,"_",month,".pdf")
file_name <- paste0(year,"_",month,".pdf")
download.file(url, file_name)
tables <- extract_tables(file_name)
```

Then we can run the same code we did above, except we'll use the year and month variables to provide useful names and values. We'll use `paste0` to build the custom variable name based on the month and year, and the `assign` function to make it into an actual variable.

### Task 8: Parse the June 2022 voter registration data
**Task** Run the following code. What's the name of the new dataframe?

```{r}
# extract the first page and turn it into a dataframe, cleaning the column names
voter_reg_page_1 <- tables[[1]] %>% as_tibble(.name_repair = "unique") %>% clean_names()

# remove the first two rows
voter_reg_page_1 <- voter_reg_page_1 %>% slice(-(1:2))

# make the new dataframe, renaming the columns and ditching the columns we don't want
assign(paste0("new_registrations_",year,"_",month), voter_reg_page_1 %>% 
  # new registration columns
  rename(method = x2, DEM = x3, REP = x4, GRN = x5, LIB = x6, WCP = x7, UNA = x8, OTH = x9, total = x10, dupes = x11) %>% 

  # remove commas and fix the datatypes in all the columns except `method`
  mutate(across(!method, gsub, pattern=",", replacement = "")) %>% 
  mutate(across(!method, as.numeric)) %>% 

  # exclude the columns we don't want
  select(-starts_with("x")) %>% 
  # add a month and year column based on our variables
  mutate(month = month, year = year))
```

To make this even more useful for analysis, we could change the names of the columns to include the month and year, so that we could join the dataframe to other months and calculate the difference between them - find out, for example, if new registrations of unaffiliated voters were up in October compared to June. Let's do that using the above code as a template:

### Task 9: Parse the June 2022 voter registration data as use dynamically generated column names
**Task** Look at the following code, especially the assign and paste functions, then run it and look at the resulting dataframe.

```{r}
# extract the first page and turn it into a dataframe, cleaning the column names
voter_reg_page_1 <- tables[[1]] %>% as_tibble(.name_repair = "unique") %>% clean_names()

# remove the first two rows
voter_reg_page_1 <- voter_reg_page_1 %>% slice(-(1:2))

# make the new dataframe, renaming the columns and ditching the columns we don't want
assign(paste0("new_registrations_",year,"_",month), voter_reg_page_1 %>% 
  # new registration columns
  rename(method = x2, !!paste0("DEM_",month) := x3, !!paste0("REP_",month) := x4, !!paste0("GRN_",month) := x5, !!paste0("LIB_",month) := x6, !!paste0("WCP_",month) := x7, !!paste0("UNA_",month) := x8, !!paste0("OTH_",month) := x9, !!paste0("total_",month) := x10, !!paste0("dupes_",month) := x11) %>% 
  # remove commas and fix the datatypes
  mutate(across(!method, gsub, pattern=",", replacement = "")) %>% 
  mutate(across(!method, as.numeric)) %>% 
  # exclude the columns we don't want
  select(-starts_with("x")) %>% 
  # add a month and year column based on our variables
  mutate(month = month, year = year))
```

Now we can join the October and June new registration dataframes and compare them:

### Task 10: Join the October and June new registration dataframes
**Task** Run the following code. How many rows do you have compared to either the October or June dataframes?
**Answer** 

```{r}
new_reg_oct_jun <- new_registrations %>% 
  inner_join(new_registrations_2022_06, by=c('method', 'year'))
```

Looks like we'll need to fix the column referring to absentee ballots. Let's do that and then join:

### Task 11: Join the October and June new registration dataframes
**Task** Run the following code. How many rows do you have compared to either the October or June dataframes?
**Answer** 

```{r}
new_reg_oct_jun <- new_registrations %>% 
  mutate(method = if_else(method == 'MAIL-IN BALLOT (FPCA)', 'ABSENTEE BALLOT (FPCA)', method)) %>% 
  inner_join(new_registrations_2022_06, by=c('method', 'year'))
```

Now let's calculate some percentages and the difference between them.

### Task 10: Calculate percentages and differences
**Task** Run the following code. What stands out to you about the differences columns?
**Answer** 

```{r}
new_reg_oct_jun <- new_reg_oct_jun %>% 
  mutate(pct_dem = DEM/total *100, pct_rep = REP/total *100, pct_una = UNA/total *100) %>% 
  mutate(pct_dem06 = DEM_06/total_06 *100, pct_rep06 = REP_06/total_06 *100, pct_una06 = UNA_06/total_06 *100) %>%
  mutate(diff_dem = pct_dem-pct_dem06, diff_rep = pct_rep-pct_rep06, diff_una = pct_una-pct_una06)
```

## Analysis

We now have the makings of a repeatable process that we could use to make the reporting process easier. We could even add graphics to this process.

## Making updating graphics

More than numbers, we are going to want to see this data so we can spot potential stories. First, let's just make a big bar chart.

### Task 9: Make a simple bar chart
**Task** Run the following code and describe what the chart is showing. What is the trend in the data?
**Answer**

```{r fig.width=9}
new_reg_oct_jun %>% 
  filter(total > 0, method != 'TOTAL') %>% 
  ggplot() +
  geom_bar(aes(x=reorder(method, pct_dem), weight=pct_dem))+
  coord_flip() + 
  theme_minimal() +
  labs(
    title = "ERIC, Mail registrations mostly Democratic",
    y = "Democratic Registration %",
    x = "Method"
  )
```

Ok, but what about a side-by-side comparison? Can we put the October and June percentages together? We can, thanks to a library called `cowplot`:


### Task 10: Produce a side-by-side chart with two plots
**Task** Run the following code to make a better bar chart. What are the main differences between them?
**Answer** 

```{r}
library(cowplot)
plot_oct <- new_reg_oct_jun %>% 
  filter(total > 10, method != 'TOTAL') %>% 
  ggplot() +
  geom_bar(aes(x=reorder(method, pct_dem), weight=pct_dem))+
  coord_flip() + 
  theme_minimal() +
  labs(
    y = "Democratic Registration %",
    x = "Method",
    title = "October"
  )

plot_jun <- new_reg_oct_jun %>% 
  filter(total_06 > 10, method != 'TOTAL') %>% 
  ggplot() +
  geom_bar(aes(x=reorder(method, pct_dem06), weight=pct_dem06))+
  coord_flip() + 
  theme_minimal() +
  labs(
    y = "Democratic Registration %",
    x = "Method",
    title = "June"
  )

plot_grid(plot_oct, plot_jun) 
```

We could use similar code to produce multiple plots for comparing other columns, and there are other steps we could take to make this process almost totally automated: we could, for example, derive the date progeammatically based on today's date and load and process data based on time intervals that we define (1-month, 3-month, etc.). The point here is not to reduce the amount of work you do the first time, but to virtually eliminate it every other time you run the code.