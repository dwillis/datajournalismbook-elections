---
title: "pre_lab_12.Rmd"
author: "derek willis"
date: "11/09/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Automating analysis

Many of the data analyses that you do will be largely one-off efforts -- you're going to do the analysis and write the story and be done. Maybe you'll come back to it in a couple of months or years, but really you're just doing it once.

But what happens when you have a long-running story, where you're going to update it every day, or every week? What changes when you're writing that code?

1. How will this run again without changing anything?
2. What questions do you have that have to be answered each time?
3. What changes when you have to repeat questions to changing data?

The global COVID-19 pandemic is something we're going to be writing about and covering for some time. One element of it is data on vaccinations. That's reported frequently (mostly daily), and we'll be talking about it for some time. So it is an ideal candidate for repeating analysis -- scripting the questions we want to answer every week and doing so in a way that we can just load it without having to change anything.

Here's a real-world example: you are covering public health in Maryland, focusing on Prince George's County, and you want to see how many people are getting fully vaccinated over time. You could look at a county or state website, copy and paste some information into a file and then do some calculations, but there's a better way. Instead, let's build a system that goes out each day to retrieve the data, makes it easier to analyze and then calculates the percentage change in the number of people getting fully vaccinated between the latest date and a week earlier. And makes us a chart showing that.

Let's get some other libraries to our typical tidyverse import. We'll start with lubridate and janitor to help wrangle the data, and we're also going to add a library called `ggrepel`, which assists in putting tables on dots in charts.

You install it the same way you do anything else -- `install.packages("ggrepel")`.

### Task 1: Load libraries and settings
**Task** Run the following code in the gray-colored codeblock below to load the libraries we'll use

```{r}
install.packages("ggrepel")
library(tidyverse)
library(janitor)
library(lubridate)
library(ggrepel)
library(sf)
```

## Automating downloads and imports

Now, where to find the data?

Maryland [publishes data daily on vaccinations on the state Department of Heath website](https://coronavirus.maryland.gov/#Vaccine).

The dashboard is a series of HTML pages placed on the main page via iframe. Like this one: https://state-of-maryland.github.io/VaccineDashboardGraphs/VaccinationDosesDaily.html. But if you view the source on that URL, there's ... no data. It's being pulled in from another URL. If we want to automate getting this data, we'll need to find out where it lives. One big clue is in the composition of that URL: "github.io". Are these files (and maybe the data) already on GitHub? Let's find out. The easiest thing to do is to go to the GitHub user: https://github.com/state-of-maryland. There's a repository there called `VaccineCSVs` that sounds pretty good. Let's check it out.

There are a lot of files here: https://github.com/state-of-maryland/VaccineCSVs, and we want to focus our attention on the ones that are frequently updated. There are a bunch of those, some of which end in .json and .csv.xml, but we want to focus on the ones that end in .csv. Let's use this one: https://raw.githubusercontent.com/state-of-maryland/VaccineCSVs/master/MD_COVID19_TotalVaccinationsCountyFirstandSecondSingleDose.csv

We'll read it into a dataframe like usual and clean up the column names:

### Task 2: Read vaccination data into dataframe
**Task** Run the following code and describe the dataframe
**Answer**

```{r}
county_vaccinations_by_date <- read_csv("https://raw.githubusercontent.com/state-of-maryland/VaccineCSVs/master/MD_COVID19_TotalVaccinationsCountyFirstandSecondSingleDose.csv") %>%
  clean_names()
View(county_vaccinations_by_date)
```

## Exploring the data

Each row represents vaccination stats for a single county on a single day, including cumulative figures. But when we read in that data, the `vaccination_date` column is formatted as a <chr> column, not as a date. Let's fix that using lubridate's handy `mdy_hms` function, which matches the format in the dataframe. Then we'll save `vaccination_date` as a date without the time:

### Task 3: Turn the date column into an actual date
**Task** Run the following code that uses lubridate to make vaccination_date a date column

```{r}
county_vaccinations_by_date <- county_vaccinations_by_date %>%
  mutate(vaccination_date = date(mdy_hms(vaccination_date)))
```

### Task 4: Count the number of rows for each county
**Task** Run the following code and describe the dataframe it produces. Is there anything unusual there?
**Answer**

Let's see if there's anything unusual in the data by counting the number of rows for each county:

```{r}
county_totals <- county_vaccinations_by_date %>%
  group_by(county) %>%
  summarize(total = n())

View(county_totals)
```

### Task 5: Examine the NA records
**Task** Run the following code and describe the tibble it produces. What do you think this data represents?
**Answer**

There are totals for all of Maryland's jurisdictions, plus one for "Unknown" and one that's NA. All but the last one have roughly the same number of records. What's going on with the NA records?

```{r}
county_vaccinations_by_date %>%
  filter(is.na(county))
```

They don't seem to pertain to any county (or no county), and it's not clear what these records represent. Let's remove them from our dataset:

### Task 6: Remove the NA records
**Task** Run the following code to filter out the records where county is NA

```{r}
county_vaccinations_by_date <- county_vaccinations_by_date %>%
  filter(!is.na(county))
```

Now we can start to interview this data.

Let's look at the most recent date, and that's something that takes on different meaning when we're talking about updating data. We need to make this generic so that every time we pull this up and run it, it's the most recent date at the top. This time, it's very simple:

### Task 7: Remove the NA records
**Task** Run the following code to ensure that the data is ordered by vaccination_date

```{r}
county_vaccinations_by_date %>% arrange(desc(vaccination_date))
```

## Analysis

Now is when we need to start asking ourselves -- what are the questions that are going to come up day after day? What about how this most current date compares to the previous day, or the previous week or month?

What if we just ranked them? Where does this date rank? For that, we'll create a new column called Rank using mutate and we'll use a function called `min_rank` to rank them. Let's start by looking at daily `fully_vaccinated` figures in Prince George's County:

### Task 8: Produce a dataframe for Prince George's County that ranks its days in terms of number of people fully vaccinated on each day
**Task** Run the following code and describe the dataframe it produces. Is there a pattern for the most recent 7-10 days?
**Answer**

```{r}
ranked <- county_vaccinations_by_date %>%
  filter(county == "Prince George's") %>%
  mutate(rank = min_rank(desc(fully_vaccinated))) %>%
  arrange(desc(vaccination_date)) %>%
  select(county, vaccination_date, fully_vaccinated, rank)

View(ranked)
```

The most recent dates aren't among the highest-ranked, meaning that fewer people are getting fully vaccinated on a given day compared to previous dates.

Let's think about this a little more. What else could we do with this? What are the recurring questions? How about the percent change between the latest date and one week ago? To do that, we need to find the latest date, which we've arranged to be the first one in our `ranked` dataframe. But what about a week ago? Luckily, if we have one date we can calculate another one by adding or subtracting days:

### Task 8: Produce two dataframes, one for the most recent date and one for a week prior to that date, then calculate the percentage change between them
**Task** Run the following code

```{r}
latest_date <- ranked %>% slice(1)
one_week_ago <- ranked %>% filter(vaccination_date == latest_date$vaccination_date - 7)
latest_date <- latest_date %>%
  mutate(pct_change_week = (fully_vaccinated - one_week_ago$fully_vaccinated)/one_week_ago$fully_vaccinated *100)
```

The `one_week_ago$fully_vaccinated` syntax is a way to reference a specific column in a specific dataframe. In this case, it's a dataframe with exactly one row.

## Making updating graphics

More than numbers, we are going to want to see this data so we can spot potential stories. We can build this in steps. First, let's just make a big bar chart.

### Task 9: Make a simple bar chart
**Task** Run the following code and describe what the chart is showing. What is the trend in the data?
**Answer**

```{r fig.width=9}
ggplot() +
  geom_bar(data=ranked, aes(x=vaccination_date, weight=fully_vaccinated))
```

So that shows us that the trend is going down over time, which makes sense as fewer people in the county are unvaccinated. It also shows that just after vaccinations began there was an initial dip - supply problems? - before the figure leapt up in April and May.

Let's build up some more layers to highlight trends and the most recent spot.

Now, in ggplot, we can add multiple layers.

The first layer will be all the bars.

The second layer will just be the latest, and we'll make that bar red.

Then we'll add a point to the top of that line to really draw attention to it.

Then we'll use ggprepel to label it.

Then I'm going to add a smoothing line. That'll illustrate the trend clearly.

The rest is labeling and adjusting the text to make it look more like a news graphic.

### Task 10: Produce a better bar chart showing the latest date and highlighting the trend line
**Task** Run the following code to make a better bar chart

```{r fig.width=9}
ggplot() +
  geom_bar(data=ranked, aes(x=vaccination_date, weight=fully_vaccinated)) +
  geom_bar(data=latest_date, aes(x=vaccination_date, weight=fully_vaccinated), fill="red") +
  geom_point(data=latest_date, aes(x=vaccination_date, y=fully_vaccinated)) +
  geom_text_repel(data=latest_date, aes(x=vaccination_date, y=fully_vaccinated + 150, label="Latest date")) +
  geom_smooth(data=ranked, aes(x=vaccination_date, y=fully_vaccinated), method=loess, se=FALSE) +
  labs(title="Prince George's County Fully Vaccinated Tailing Off", x="Date", y="Fully Vaccinated") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 8),
    plot.subtitle = element_text(size=10),
    panel.grid.minor = element_blank()
    )
```

One thing we are missing? An automated summary. What if we programmatically wrote the description for this chart using the percent change calculation we did before?

First, we format the percent change to look more news graphic like and not with 7 significant digits.

### Task 11: Save a nicer version of the percentage change and a variable to show whether that change is an increase or decrease
**Task** Run the following code to produce those variables

```{r}
changetext <- round(latest_date$pct_change_week[[1]], digits=2)
direction <- if_else(changetext > 0, "increased", "decreased")
```

Now we're going to use a function called paste to merge some text together. We're going to paste together a sentence fragment, the percent change number and another sentence fragment together to form a sentence. We'll save it as sub, because that's what it's called in ggplot -- a subtitle.

### Task 12: Produce a description for your bar chart based on the variables in Task 11
**Task** Run the following code and put the value of `sub` in the answer.
**Answer**

```{r}
sub <- paste("The number of people in Prince George's County who got fully vaccinated on ", format(latest_date$vaccination_date, format="%B %d"), " ", direction, " by ", changetext, " percent compared to the week before", sep="")
sub
```

### Task 13: Produce a description for your bar chart based on the variables in Task 11
**Task** Run the following code and put the value of `sub` in the answer.
**Answer**

Now we can add that to our labels.

```{r fig.width=9}
ggplot() +
  geom_bar(data=ranked, aes(x=vaccination_date, weight=fully_vaccinated)) +
  geom_bar(data=latest_date, aes(x=vaccination_date, weight=fully_vaccinated), fill="red") +
  geom_point(data=latest_date, aes(x=vaccination_date, y=fully_vaccinated)) +
  geom_text_repel(data=latest_date, aes(x=vaccination_date, y=fully_vaccinated + 150, label="Latest date")) +
  geom_smooth(data=ranked, aes(x=vaccination_date, y=fully_vaccinated), method=loess, se=FALSE) +
  labs(title="Prince George's County Fully Vaccinated Tailing Off", subtitle=sub, x="Date", y="Fully Vaccinated") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 8),
    plot.subtitle = element_text(size=10),
    panel.grid.minor = element_blank()
    )
```

This is going to be a story for months, if not years. So repeating this analysis is a must for a reporter covering health care in Maryland. We've set ourselves up to do this every week when the data comes out. We just open our notebook, go to Run > Restart R and Run All Chunks and sit back and watch as it does it all again.

Then we go report.

# Automating geographic analysis

One thing that has been very apparent with the coronavirus outbreak is that this is a very geographic story. Where cases are being found and how fast is news, so it would be a good idea for us to have updating maps. But to have that, we need to have updating data.

Good news.

The New York Times is making the data behind [their interactive trackers](https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html) [available to others for free](https://github.com/nytimes/covid-19-data).

So we have a constantly updating data stream on Github, so that means we can make this work.

We can use `read_csv` to read a URL if that URL is to a csv file. And Github just happens to provide a direct link to the CSV of county COVID-19 reports. Here's what that looks like:

### Task 14: Load the NYT covid county data
**Task** Run the following code to load the data

```{r}
covid <- read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv")
head(covid)
```

If you look, we have a county and a date -- how many cases are reported in that county on that day. That means we can do some interesting progression charts.

Let's filter out Maryland first and create a line chart:

### Task 15: Filter on Maryland and create a line chart
**Task** Run the following code and describe what the resulting chart shows
**Answer**

```{r}
maryland <- covid %>% filter(state == "Maryland")

ggplot() + geom_line(data=maryland, aes(x=date, y=cases, group=county, color=county))
```

The colors can be a little hard to make out, but when you have counties with a large number of cases, you'd use a different kind of scale called a log scale. It's a way of representing a very wide range of data in a more compact way (more details here: https://en.wikipedia.org/wiki/Logarithmic_scale). [YOU REALLY SHOULD WATCH THIS](https://www.ft.com/video/9a72a9d4-8db1-4615-8333-4b73ae3ddff8). You've no doubt seen the Financial Times coronavirus trajectory tracker. Hear why they are using a log scale. And here's what our chart looks like with it. Note the y-axis scale.

### Task 16: Switch the line chart to use a log scale
**Task** Run the following code and describe what the resulting chart shows
**Answer**

```{r}
ggplot() + geom_line(data=maryland, aes(x=date, y=cases, group=county, color=county)) + scale_y_continuous(trans="log10")
```

This presents a more consistent story of the increase in cases.

## Mapping continuously

But for a map, we can't have multiple days. We need a single day. Ideally, it would be the most recent date. We can get it using the `max` function. That will give us the most recent date in Maryland in a variable called `current`. And now we can filter the most recent data for Maryland, regardless of when this runs.

I'm adding one piece to the end to make joining this to a map easier and just renaming fips to GEOID, because they are identical in both datasets and can be used for joining. Then we'll read in a shapefile with geographic data for all U.S. counties and join it with the Maryland data

### Task 17: Find the latest data for Maryland and join it to a shapefile of U.S. counties
**Task** Run the following code. How many rows in the counties dataframe?
**Answer**

```{r}
current <- maryland %>% summarize(max(date))
marylandcurrent <- maryland %>% filter(date == current[[1]]) %>% rename(GEOID = fips)
counties <- st_read("data/cb_2018_us_county_5m/cb_2018_us_county_5m.shp")
counties <- counties %>% left_join(marylandcurrent)
```

Since we have every county in the United States in our counties map layer, we can filter just Maryland:

### Task 18: Filter our data to just Maryland
**Task** Run the following code. How many rows in the mdcounties dataframe?
**Answer**

```{r}
mdcounties <- counties %>% filter(STATEFP == 24)
```

So now, we have a geographic dataframe that has both the county shapes and the number of cases in the most recent data update. We just need to see it now:

### Task 19: Make a map
**Task** Run the following code and describe what the map is showing.
**Answer**

```{r}
ggplot() +
  geom_sf(data=mdcounties, aes(fill=cases)) +
  scale_fill_viridis_c(option = "plasma", trans = "sqrt") +
  theme_void() +
  labs(title = paste("COVID-19 cases as of ", current[[1]], sep=""))
```

As long as the state continues to produce data, we could make a map every day.
