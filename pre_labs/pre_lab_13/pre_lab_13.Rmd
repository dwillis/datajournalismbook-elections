---
title: "pre_lab_13.Rmd"
author: "derek willis"
date: "2024-11-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# AI and Data Journalism

The first thing to know about the large language models that have attracted so much attention, money and coverage is this: they are not fact machines.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/llms.jpg"))
```

But they are - mostly - very useful for people who write code and for those trying to work through complex problems. That's you. At its core, what a large language model does is predict the next word in a phrase or sentence. They are probabilistic prediction machines based on a huge set of training data. This chapter goes through some tasks and examples using LLMs.

### Task 1: Load libraries and settings

**Task** Run the following code in the gray-colored codeblock below to load the libraries we'll use

We'll be using a service called Groq for the examples here. You should [sign up for a free account](https://www.groq.com) and [create an API key](https://console.groq.com/docs/quickstart). Make sure you copy that key. We'll also need to install an R package to handle the responses:

```{r}
library(devtools)
devtools::install_github("heurekalabsco/axolotr")
library(axolotr)
```

Then we can load that library and, using your API key, setup your credentials:

**Task** Run the following code, then restart R using the "Session" drop-down menu:

```{r}
create_credentials(GROQ_API_KEY = "YOUR API KEY HERE")
```

See that "Please restart your R session for changes to take effect."? Go ahead and do that; you'll need to rerun the `library()` function above.

Let's make sure that worked. We'll be using the [Llama 3.1 model released by Meta](https://ai.meta.com/blog/meta-llama-3-1/).

### Task 2: Testing out Groq

**Task** Run the following code in the gray-colored codeblock below, and tell me what you think the difference between the first 10 names and the second 10 names is, if any.

**Answer**

```{r}
groq_response <- axolotr::ask(
  prompt = "Give me 20 names for a pet lemur",
  model = "llama-3.1-8b-instant"
)

cat(groq_response)
```

I guess you're getting a lemur?

## Three Uses of AI in Data Journalism

There are at least three good uses of AI in data journalism:

-   turning unstructured information into data
-   helping with code debugging and explanation
-   brainstorming about strategies for data analysis and visualization

If you've tried to use a large language model to actually do data analysis, it *can* work, but often the results can be frustrating. Think of AI as a potentially useful assistant for the work you're doing. If you have a clear idea of the question you want to ask or the direction you want to go, they can help. If you don't have a clear idea or question, they probably will be less helpful. Let's go over a quick example of each use.

### Turning Unstructured Information into Data

News organizations are sitting on a trove of valuable raw materials - the words, images, audio and video that they produce every day. We can (hopefully) search it, but search doesn’t always deliver meaning, let alone elevate patterns. For that, often it helps to turn that information into structured data. Let's look at an example involving my friend Tyson Evans, who recently celebrated his 10th wedding anniversary. You can read about [his wedding in The New York Times](https://www.nytimes.com/2014/06/22/fashion/weddings/gabriela-herman-tyson-evans.html?unlocked_article_code=1.1E0.POK5.J1hIzAwr-D6N&smid=url-share).

This announcement is a story, but it's also data - or it should be.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/evans_wedding.png"))
```

What if we could extract those highlighted portions of the text into, say, a CSV file? That's something that LLMs are pretty good at. Let's give it a shot using the full text of that announcement.

### Task 3: Extracting data from text

**Task** Run the following code that uses Llama to extract elements from the wedding announcement into a CSV file. Did Llama get it right?

**Answer**

```{r}
text = "Gabriela Nunes Herman and Tyson Charles Evans were married Saturday at the home of their friends Marcy Gringlas and Joel Greenberg in Chilmark, Mass. Rachel Been, a friend of the couple who received a one-day solemnization certificate from Massachusetts, officiated. The bride, 33, will continue to use her name professionally. She is a Brooklyn-based freelance photographer for magazines and newspapers. She graduated from Wesleyan University in Middletown, Conn. She is a daughter of Dr. Talia N. Herman of Brookline, Mass., and Jeffrey N. Herman of Cambridge, Mass. The bride’s father is a lawyer and the executive vice president of DecisionQuest, a national trial consulting firm in Boston. Her mother is a senior primary care internist at Harvard Vanguard Medical Associates, a practice in Boston. The groom, 31, is a deputy editor of interactive news at The New York Times and an adjunct professor at the Columbia University Graduate School of Journalism. He graduated from the University of California, Los Angeles. He is the son of Carmen K. Evans of Climax Springs, Mo., and Frank J. Evans of St. Joseph, Mo. The groom’s father retired as the president of UPCO, a national retailer of pet supplies in St. Joseph."

evans_response <- axolotr::ask(
  prompt = paste("Given the following text, extract information into a CSV file with the following structure with no yapping: celebrant1,celebrant2,location,officiant,celebrant1_age,celebrant2_age,celebrant1_parent1,celebrant1_parent2,celebrant2_parent1,celebrant2_parent2. Omit quotemarks", text),
  model = "llama-3.1-8b-instant"
)

cat(evans_response)
```

A brief word about that "no yapping" bit; it's a way to tell your friendly LLM to cut down on the chattiness in its response. What we care about is the data, not the narrative. And look at the results: without even providing an example or saying that the text described a wedding, the LLM did a solid job. Now imagine if you could do this with hundreds or thousands of similar announcements. You've just built a database.

Here's another example: congressional job listings.

### Task 4: Extracting more data from text

**Task** Run the following code that uses Llama to extract elements from a congressional job listing into a CSV file. Did Llama get it right? What would you change about the prompt, given the text?

**Answer**

```{r}
text = "Congressman Dan Crenshaw (R-TX-02) seeks a highly motivated, hard-working Legislative
Assistant (LA), based in our Washington, D.C. oDice. This position reports to the Legislative
Director. The LA will handle an economic facing portfolio, including Budget and
Appropriations, Financial Services, and Tax, with other issues to follow.
Key duties include:
• Communicating clearly and consistently with the Member, Legislative Director, and
legislative, communications, and district teams on legislative updates
• Preparing and staDing the Member for meetings, and assisting in the drafting of remarks,
as needed
• Meeting with constituents, stakeholders, and advocacy groups within the portfolio of
issue areas Working with external stakeholders, Congressional Research Service (CRS),
Legislative Counsel, and committee staD to draft legislation based on the Member’s
priorities
• Ideal candidates will have strong written and oral communications skills and be detailoriented, organized, and creative. They must be able to quickly learn the member’s voice
and think in a proactive, communications-focused manner.
Other key qualities include:
• Commitment to a collaborative work environment
• Ability to work in a fast-paced environment and manage competing priorities
• Ability to communicate up and down within the team
• Ability to learn the Congressman’s voice and legislative priorities
• Interest in identifying and pursuing new opportunities and the ability to anticipate and
adapt to challenges
Hill experience is required. Compensation is competitive and commensurate with level of
experience. Qualiﬁed applicants should submit a resume and writing sample (no more
than 3 pages) as one combined PDF to Kenneth.Depew@mail.house.gov. Include
“Legislative Assistant – [Full Name]” in the subject line. Applications will be considered on
a rolling basis as received. No phone calls or drop-ins please."

housejob_response <- axolotr::ask(
  prompt = paste("Given the following text, extract information into a CSV file with the following structure with no yapping: member,party,state,district,job_title,location,hill_experience,portfolio", text),
  model = "llama-3.1-8b-instant"
)

cat(housejob_response)
```

### Helping with Code Debugging and Explanation

When you're writing code and run into error messages, you should read them. But if they do not make sense to you, you can ask an LLM to do some translation, which is another great use case for AI. As with any debugging exercise, you should provide some context, things like "Using R and the tidyverse ... " and describing what you're trying to do, but you also can ask LLMs to explain an error message in a different way. Here's an example:

### Task 5: Help me, Llama!

**Task** Run the following code. Does the response make sense? Why or why not?

**Answer**

```{r}
debug_response <- axolotr::ask(
  prompt = "Explain the following R error message using brief, simple language and suggest a single fix. I am using the tidyverse library: could not find function '|>'",
  model = "llama-3.1-8b-instant"
)

cat(debug_response)
```

The trouble is that if you run that several times, it will give you slightly different answers. Not fact machines. But you should be able to try some of the suggested solutions and see if any of them work. An even better use could be to pass in working code that you're not fully understanding and ask the LLM to explain it to you.

### Brainstorming about Strategies for Data Analysis and Visualization

Let's say that you have some data that you want to interview, but aren't sure how to proceed. LLMs can provide some direction, but you may not want to follow their directions exactly. You shouldn't accept their judgments uncritically; you'll still need to think for yourself. Here's an example of how that might go.

### Task 6: Help Me Brainstorm, Llama!

**Task** Run the following code. Does the output make sense? Would it help you to understand the data? Does it make any assumptions about what the data looks like that seem incorrect?

**Answer**

```{r}
idea_response <- axolotr::ask(
  prompt = "I have a CSV file of voter registration data, including the date of birth of the voter and the registration date, political party and the voter's address. Using R and the tidyverse, suggest some ways that I could find patterns in the data. Use the new-style pipe operator (|>) and lubridate in any code examples",
  model = "llama-3.1-8b-instant"
)

cat(idea_response)
```

Note that the column names may not match your data; the LLM is making predictions about your data, so you could provide the column names. As [this story from The Pudding](https://pudding.cool/2024/07/ai/) makes clear, the potential for using LLMs to not just assist with but perform data analysis is real. What will make the difference is how much context you can provide and how clear your ideas and questions are. You still have to do the work.

But text isn't the only thing that LLMs can help extract data from. Images, audio and video all are filled with information that can become data. Let's take a look at an example using an image.

### Task 7: What's in this picture?

Log into <https://console.groq.com/>, then switch the model to llama-3.2-11b-vision-preview:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/groq_llama32_vision.png"))
```

**Task** Follow the instructions below, then evaluate the LLM's response. Did it do well? What would you change?

Then click on the image icon next to the "Submit" button and choose the "fundraising_email.png" file from the images folder, and add it. Then, add this as the prompt: "Describe the things in this image, being as specific as possible."

**Answer**

### Task 8: Run a model on your laptop!

Go to <https://ollama.com/> and download and install the Ollama software. Then, in the Terminal (Mac) or PowerShell (Windows), type the following: ollama run llama3.2

This will take a few minutes, so take the time to read more about it: <https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/>

Now switch back to RStudio, and install the `mall` package:

**Task** Install the mall package by running this code, and then comment out the install.packages line. We'll tell the `mall` package to use the llama3.2 model via ollama.

```{r}
install.packages("mall")
library(mall)
llm_use("ollama", "llama3.2", seed = 100, temperature = 0)
```

Now we'll load some fundraising emails to work with. Our goal will be to extract any merchandise mentioned in the body of the emails (as an inducement to give money).

**Task** Load a CSV file of fundraising emails.

```{r}
emails <- read_csv("data/emails.csv")
```

Here we can use LLMs in a tidy manner, by starting with the dataframe and calling functions like we do in other instances. Let's try it out:

**Task** Run the following code and look at the .extract column. How did it do? Do the results make sense, and why or why not? 

**Answer** 

```{r}
emails |>
  llm_extract(body, "merchandise that can be won")
```
