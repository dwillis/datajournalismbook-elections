---
title: "Data Journalism with R and the Tidyverse"
author: "Matt Waite (original author); updated by Derek Willis and Sean Mussenden"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: []
biblio-style: apalike
link-citations: yes
github-repo:
description: "This is a book built on a very opinionated philosophy for journalism students doing data journalism in R with replicable methods."
---

# Introduction

If you were at all paying attention in pre-college science classes, you have probably seen this equation:

    d = rt or distance = rate*time

In English, that says we can know how far something has traveled if we know how fast it's going and for how long. If we multiply the rate by the time, we'll get the distance.

If you remember just a bit about algebra, you know we can move these things around. If we know two of them, we can figure out the third. So, for instance, if we know the distance and we know the time, we can use algebra to divide the distance by the time to get the rate.

    d/t = r or distance/time = rate

In 2012, the South Florida Sun Sentinel found a story in this formula.

People were dying on South Florida tollways in terrible car accidents. What made these different from other car fatal car accidents that happen every day in the US? Police officers driving way too fast were causing them.

But do police regularly speed on tollways or were there just a few random and fatal exceptions?

Thanks to Florida's public records laws, the Sun Sentinel got records from the toll transponders in police cars in south Florida. The transponders recorded when a car went through a given place. And then it would do it again. And again.

Given that those places are fixed -- they're toll plazas -- and they had the time it took to go from one toll plaza to another, they had the distance and the time.

[It took high school algebra to find how fast police officers were driving. And the results were shocking.](http://www.sun-sentinel.com/news/local/speeding-cops/fl-speeding-cops-20120211,0,3706919.story)

Twenty percent of police officers had exceeded 90 miles per hour on toll roads. In a 13-month period, officers drove between 90 and 110 mph more than 5,000 times. And these were just instances found on toll roads. Not all roads have tolls.

The story was a stunning find, and the newspaper documented case after case of police officers violating the law and escaping punishment. And, in 2013, they won the Pulitzer Prize for Public Service.

All with simple high school algebra.

## Modern data journalism

It's a single word in a single job description, but a Buzzfeed job posting in 2017 is another indicator in what could be a profound shift in how data journalism is both practiced and taught.

"We’re looking for someone with a passion for news and a commitment to using data to find amazing, important stories — both quick hits and deeper analyses that drive conversations," the posting seeking a data journalist says. It goes on to list five things BuzzFeed is looking for: Excellent collaborator, clear writer, deep statistical understanding, knowledge of obtaining and restructuring data.

And then there's this:

__"You should have a strong command of at least one toolset that (a) allows for filtering, joining, pivoting, and aggregating tabular data, and (b) enables reproducible workflows."__

This is not the data journalism of 20 years ago. When it started, it was a small group of people in newsrooms using spreadsheets and databases. Data journalism now encompases programming for all kinds of purposes, product development, user interface design, data visualization and graphics on top of more traditional skills like analyzing data and writing stories.

In this book, you'll get a taste of modern data journalism through programming in R, a statistics language. You'll be challenged to think programmatically while thinking about a story you can tell to readers in a way that they'll want to read. They might seem like two different sides of the brain --  mutually exclusive skills. They aren't. I'm confident you'll see programming is a creative endeavor and storytelling can be analytical.

Combining them together has the power to change policy, expose injustice and deeply inform.

## Installations

This book is all in the R statistical language. To follow along, you'll do the following:

1. Install the R language on your computer. Go to the [R Project website](https://www.r-project.org/), click download R and select a mirror closest to your location. Then download the version for your computer.

2. Install [R Studio Desktop](https://www.rstudio.com/products/rstudio/#Desktop). The free version is great.

Going forward, you'll see passages like this:

```{r eval=FALSE}
install.packages("tidyverse")
```

That is code that you'll need to run in your R Studio. When you see that, you'll know what to do.

## About this book

This book is the collection of class materials originally written for Matt Waite's Data Journalism class at the University of Nebraska-Lincoln's College of Journalism and Mass Communications. It has been substantially updated by Derek Willis and Sean Mussenden for data journalism classes at the University of Maryland Philip Merrill College of Journalism.

There's some things you should know about it:

* It is free for students.
* The topics will remain the same but the text is going to be constantly tinkered with.
* What is the work of the author is copyright Matt Waite 2020.
* The text is [Attribution-NonCommercial-ShareAlike 4.0 International](https://creativecommons.org/licenses/by-nc-sa/4.0/) Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I'm not making money on this so you can't either.  
* As such, the whole book -- authored in Bookdown -- in its original form is [open sourced on Github](https://github.com/mattwaite/datajournalismbook). Pull requests welcomed!

## What we'll cover

* Public records and open data
* R Basics
* Replication
* Data basics and structures
* Aggregates
* Mutating
* Working with dates
* Filters
* Cleaning I: Data smells
* Cleaning II: Janitor
* Cleaning III: Open Refine
* Cleaning IV: Pulling Data from PDFs
* Joins
* Basic data scraping
* Intermediate data scraping
* Getting data from APIs: Census
* Visualizing for reporting: Basics
* Visualizing for reporting: Publishing
* Geographic data basics
* Geographic queries
* Geographic visualization
* Text analysis basics
* Text analysis
* Advanced analysis: Correlations and regressions
* Advanced analysis: Logistic regression
* Writing with and about data
* Data journalism ethics

<!--chapter:end:index.Rmd-->

# Public records

Public records are the lifeblood of investigative reporting. They carry their own philosophical framework, in a manner of speaking.  

* Sunlight is the best disinfectant. Corruption hides in the shadows.
* You paid for it with your taxes. It should be yours (with exceptions).
* Journalism with a capital J is about holding the powerful accountable for their actions.

Keeping those things in mind as you navigate public records is helpful.

## Federal law

Your access to public records and public meetings is a matter of the law. As a journalist, it is your job to know this law better than most lawyers. Which law applies depends on which branch of government you are asking. In addition to documents and other kinds of information, FOIA also provides access to structured datasets of the kind we'll use in this class.

The Federal Government is covered by the Freedom of Information Act, or FOIA. FOIA is not a universal term. Do not use it if you are not talking to a federal agency. FOIA is a beacon of openness to the world. FOIA is deeply flawed and frustrating.

Why?

* There is no real timetable with FOIA. Requests can take months, even years.
* As a journalist, you can ask that your request be expedited.
* Guess what? That requires review. More delays.
* Exemptions are broad. National security, personal privacy, often overused.
* Denied? You can appeal. More delays.

The law was enacted in 1966, but it’s still poorly understood by most federal employees, if not outright flouted by political appointees. Lawsuits are common.

Post 9/11, the Bush administration rolled back many agency rules. Obama ordered a "presumption of openness" but followed it with some of the most restrictive policies ever seen. The Trump Administration, similar to the Obama administration, claims to be the most transparent administration, but has steadily removed records from open access and broadly denied access to records.

Result? FOIA is in trouble.

[SPJ is a good resource](https://www.spj.org/foi-guide-pros.asp).

## State law

States are -- generally -- more open than the federal government. The distance between the government and the governed is smaller. Some states, like Florida and Texas, are very open. Others, like Virginia and Pennsylvania, are not. Maryland is somewhere in the middle.

These laws generally give you license to view -- and obtain a copy of -- a record held by a state or local government agency.  

What is a public record? Generally speaking, public records are information stored on paper or in an electronic format held by a state or local government agency, but each state has its own list of types of records -- called "exemptions" -- that are not subject to disclosure.

If a record has both exempt and non-exempt information mixed in, most states require an agency to disclose it after removing the exempt information, a process called "redaction." Agencies aren't required to create a record in order to fill your request.

In some states but not all -- the public information law (or related case law) explicitly dictates that extracting a slice of a database doesn't constitute creation of a record. Most states can charge you a reasonable fee for time spent retrieving or copying records, though many have provisions to waive those fees for journalists. Every state law operates on a different timeline. Some only require agencies respond in a "reasonable" time, but others spell out exactly how fast an agency must respond to you, and how fast they must turn over the record.     

[The Reporters Committee For Freedom of the Press](https://www.rcfp.org/open-government-guide/) has a good resource for learning the law in your state.

Please and thank you will get you more records than any lawyer or well-written request. Be nice. Be polite. And be persistent. Following up regularly to check on status of a request lets an agency know they can't ignore you (and some will try). Hunting for records is like any other kind of reporting -- you have to do research. You have to ask questions. Ask them: What records do you keep? For how long?

When requesting data, you are going to scare the press office and you are going to confuse the agency lawyer. Request to have their data person on the phone.

A good source of info? Records retention schedules, often required by law or administrative rule at an agency. Here's an example from [Nebraska](http://www.sos.ne.gov/records-management/retention_schedules.html).

<!--chapter:end:01-publicrecords.Rmd-->

# R basics

[R](https://www.r-project.org/about.html) is a programming language, one specifically geared toward data analysis.

Like all programming languages, it has certain built-in functions.

There are many ways you can write and execute R code. The first, and most basic, is the console, shown here as part of a software tool called [RStudio (Desktop Open Source Edition)](https://www.rstudio.com/products/rstudio/) that we'll be using all semester.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/verybasics1.png"))
```

Think of the console like talking directly to the R language engine that's busy working inside your computer. You use it send R commands, making sure to use the only language it understands, which is R.  The R language engine processes those commands, and sends information back to you.  

Using the console is direct, but it has some drawbacks and some quirks we'll get into later.  Let's examine a basic example of how the console works.  

If you load up R Studio, type 2+2 into the console and hit enter it will spit out the number 4, as displayed below.

```{r}
2+2
```

It's not very complex, and you knew the answer before hand, but you get the idea. With R, we can compute things.

We can also store things for later use under a specific name. In programming languages, these are called **variables**. We can assign things to variables using this left-facing arrow: `<-`. The `<-` is a called an **assignment operator**.

If you load up R studio and type this code in the console...

```{r}
number <- 2
```

...and then type this code, it will spit out the number 4, as show below.

```{r}
number * number
```

We can have as many variables as we can name. We can even reuse them (but be careful you know you're doing that or you'll introduce errors).

If you load up R studio and type this code in the console...

```{r}
firstnumber <- 1
secondnumber <- 2
```

...and then type this, it will split out the number 6, as shown below.

```{r}
(firstnumber + secondnumber) * secondnumber
```

We can store anything in a variable. A whole table. A list of numbers. A single word. A whole book. All the books of the 18th century. Variables are really powerful. We'll explore them at length.

A quick note about the console: After this brief introduction, we won't spend much time in R Studio actually writing code directly into the console.  Instead, we'll write code in fancied-up text files -- interchangably called R Markdown or R Notebooks -- as will be explained in the next chapter. But that code we write in those text files will still *execute* in the console, so it's good to know how it works.

## About libraries

The real strength of any programming language is the external libraries (often called "packages") that power it. The base language can do a lot, but it's the external libraries that solve many specific problems -- even making the base language easier to use.

With R, there are hundreds of free, useful libraries that make it easier to do data journalism, created by a community of thousands of R users in multiple fields who contribute to open-source coding projects.

For this class, we'll make use of several external libraries.

Most of them are part of a collection of libraries bundled into one "metapackage" called the [Tidyverse](https://www.tidyverse.org/packages/) that streamlines tasks like:

* Loading data into R. (We'll use the [readr](https://readr.tidyverse.org/) Tidyverse library)
* Cleaning and reshaping the data before analysis. (We'll use the the [tidyr](https://tidyr.tidyverse.org/index.html) and [dplyr](https://dplyr.tidyverse.org/) Tidyverse libraries)
* Data analysis. (We'll use the [dplyr](https://dplyr.tidyverse.org/) Tidyverse library)
* Data visualization (We'll use the [ggplot2](https://ggplot2.tidyverse.org/) Tidyverse library)

To install packages, we use the function `install.packages()`.

You only need to install a library once, the first time you set up a new computer to do data journalism work. You never need to install it again, unless you want to update to a newer version of the package.   

To install all of the Tidyverse libraries at once, the function is `install.packages('tidyverse')`. You can type it directly in the console.

To use the R Markdown files mentioned earlier, we also need to install a Tidyverse-related library that doesn't load as part of the core Tidyverse package.  The package is called, conveniently, [rmarkdown](https://rmarkdown.rstudio.com/docs/). The code to install that is `install.packages('rmarkdown')`

<!--chapter:end:02-basics.Rmd-->

# Data journalism in the age of replication

A single word in a single job ad for [Buzzfeed News](https://www.buzzfeednews.com/) posted in 2017 offered an indication of a profound shift in how data journalism is both practiced and taught.

"We’re looking for someone with a passion for news and a commitment to using data to find amazing, important stories — both quick hits and deeper analyses that drive conversations," the posting seeking a data journalist says. It goes on to list five things BuzzFeed is looking for: Excellent collaborator, clear writer, deep statistical understanding, knowledge of obtaining and restructuring data.

And then there's this:

__"You should have a strong command of at least one toolset that (a) allows for filtering, joining, pivoting, and aggregating tabular data, and (b) enables reproducible workflows."__

The word you're seeing more and more of? Reproducible. And it started in earnest in 2017 when data journalism crossed a major threshold in American journalism: It got it's own section in the [Associated Press Stylebook](https://www.apstylebook.com/).

"Data journalism has become a staple of reporting across beats and platforms," the Data Journalism section of the Stylebook opens. "The ability to analyze quantitative information and present conclusions in an engaging and accurate way is no longer the domain of specialists alone."

The AP's Data Journalism section discusses how to request data and in what format, guidelines for scraping data from websites with automation, the ethics of using leaked or hacked data and other topics long part of data journalism conference talks.

But the third page of the section contains perhaps the most profound commandment: __"As a general rule, all assertions in a story based on data analysis should be reproducible. The methodology description in the story or accompanying materials should provide a road map to replicate the analysis."__

Reproducible research -- replication -- is a cornerstone of scientific inquiry. Researchers across a range of academic disciplines use methods to find new knowledge and publish it in peer reviewed journals. And, when it works, other researchers take that knowledge and try it with their own samples in their own locations. Replication studies exist to take something from an "interesting finding" to a "theory" and beyond.

It doesn't always work.

Replication studies aren't funded at nearly the level as new research. And, to the alarm of many, scores of studies can't be replicated by others. Researchers across disciplines are finding that when their original studies are replicated, flaws are found, or the effects found aren't as strong as the original. Because of this, academics across a number of disciplines have written about a replication crisis in their respective fields, particularly psychology, social science and medical research.

In Chapter 1 of the [New Precision Journalism](https://www.amazon.com/New-Precision-Journalism-Midland-Book/dp/0253206642), Phil Meyer wrote that "we journalists would be wrong less often if we adapted to our own use some of the research tools of the social scientists."

Meyer would go on to write about how computers pouring over datasets too large to crunch by hand had changed social science from a discipline with "a few data and a lot of interpretation" into a much more meaningful and powerful area of study. If journalists could become comfortable with data and some basic statistics, they too could harness this power.

"It used to be said that journalism is history in a hurry," Meyer wrote. "The argument of this book is that to cope with the acceleration of social change in today's world, journalism must become social science in a hurry."

He wrote that in 1971. It might as well have been yesterday.

Journalism doesn't have a history of replication, but the concerns about credibility are substantially greater. Trust in media is at an all time low and shows no signs of improving. While the politics of the day have quite a bit to do with this mistrust of media, being more transparent about what journalists do can't hurt.

The AP's commandment that "Thou must replicate your findings" could, if taken seriously by the news business, have substantial impacts on how data journalism gets done in newsrooms and how data journalism gets taught, both at professional conferences and universities.

How? Two ways.

* The predominant way that data journalism gets done in a newsroom is through simple tools like Microsoft Excel or Google Sheets. Those simple tools, on their own, lack significant logging functions that automatically keep track of steps a data journalist took to reach a given conclusion. That means journalists using those tools have to maintain separate, detailed logs of what they did so any analysis can be replicated.
* The predominant way that data journalism gets taught -- both in professional settings and at most universities -- doesn't deal with replication at all. The tools and the training stress "getting things done" -- an entirely logical focus for a deadline driven business. The choices of tools -- like spreadsheet programs -- are made to get from data to story as quick as possible, without frightening away math and tech phobic students.

If the AP's replication rules are to be followed, journalism needs to become much more serious about the tools and techniques used to do data journalism. The days of "point and click" tools to do "quick and dirty" analysis that get published are dying. The days of formal methods using documented steps are here.

## The stylebook

Troy Thibodeaux, the editor of the AP's data journalism team, said the stylebook entry started when the data team found themselves answering the same questions over and over. With a grant from the Knight Foundation, the team began to document their own standards and turn that into a stylebook section.

From the beginning, they had a fairly clear idea of what they wanted to do -- think through a project and ask what the frequently asked questions are that came up. It was not going to be a soup-to-nuts guide to how to do a data project.

When the section came out, eyebrows went up on the replication parts, surprising Thibodeaux.

"From our perspective, this is a core value for us," he said. "Just for our own benefit, we need to be able to have someone give us a second set of eyes. We benefit from that every day. We catch things for each other."

Thibodeaux said the AP data team has two audiences when it comes to replication -- they have the readers of the work, and members of the collective who may want to do their own work with the data.

"This is something that's essential to the way we work," he said. "And it's important in terms of transparency and credibility going forward. We thought it would be kind of unexceptionable."

## Replication

Meyer, now 86, said he's delighted to see replication up for discussion now, but warned that we shouldn't take it too far.

"Making the analysis replicable was something I worried about from the very beginning," he wrote in an email. So much so that in 1967, after publishing stories from his landmark survey after the Detroit riots, he shipped the data and backup materials about it to a social science data repository at the University of North Carolina.

And, in doing so, he opened the door to others replicating his results. One scholar attempted to find fault with Meyer's analysis by slicing the data ever thinner until the differences weren't significant -- gaming the analysis to criticize the stories.

Meyer believes replication is vitally important, but doesn't believe it should take on the trappings of science replication, where newsrooms take their own samples or re-survey a community. That would be prohibitively expensive.

But journalists should be sharing their data and analysis steps. And it doesn't need to be complicated, he said.

"Replication is a theoretical standard, not a requirement that every investigator duplicate his or  her own work for every project," he said. "Giving enough information in the report to enable another investigator to follow in your footsteps is enough. Just telling enough to make replication possible will build confidence."

But as simple as that sounds, it's not so simple. Ask social scientists.

Andrew Gelman, a professor of statistics and political science and director of the Applied Statistics Center at Columbia University, wrote in the journal CHANCE that difficulties with replication in empirical research are pervasive.

"When an outsider requests data from a published paper, the authors will typically not post or send their data files and code, but instead will point to their sources, so replicators have to figure out exactly what to do from there," Gelman wrote. "End-to-end replicability is not the norm, even among scholars who actively advocate for the principles of open science."

So goes science, so goes journalism.

Until a recent set of exceptions, journalists rarely shared data. The "nerd box" -- a sidebar story that explains how a news organization did what they did -- is a term that first appeared on NICAR-L, a email listserv of data journalists, in the 1990s.

It was a form born in print.

As newsrooms adapted to the internet, some news organizations began linking to their data sources if they were online. Often, the data used in stories were obtained through records requests. Sometimes, reporters created the data themselves.

Journalism, more explicitly than science, is a competitive business. There have been arguments that nerd boxes and downloadable links give too much away to competitors.

Enter the AP Stylebook.

The AP Stylebook argues explicitly for both internal and external replication. Externally, they argue that the __"methodology description in the story or accompanying materials should provide a road map to replicate the analysis"__, meaning someone else could do the replication post publication.

Internally, the AP Stylebook says: __"If at all possible, an editor or another reporter should attempt to reproduce the results of the analysis and confirm all findings before publication."__

There are two problems here.

First is that journalism, unlike science, has no history of replication. There is no "scientific method" for stories. There is no standard "research methods" class taught at every journalism school, at least not where it comes to writing stories. And, beyond that, journalism school isn't a requirement to get into the news business. In other words, journalism lacks the standards other disciplines have.

The second problem is, in many ways, worse: Except for the largest newsrooms, most news organizations lack editors who could replicate the analysis. Many don't have a second person who would know what to do.  

Not having a second set of eyes in a newsroom is a problem, Thibodeaux acknowledges. Having a data journalism team "is an incredible luxury" at the AP, he said, and their rule is nothing goes on the wire without a second set of eyes.

Thibodeaux, for his part, wants to see fewer "lone nerds in the corner" -- it's too much pressure. That person gets too much credibility from people who don't understand what they do, and they get too much blame when a mistake is made.

So what would replication look like in a newsroom? What does this mean for how newsrooms do data journalism on deadline?

## Goodbye Excel?

For decades, Excel has been the gateway drug for data journalists, the Swiss Army knife of data tools, the "One Tool You Can't Live Without." Investigative Reporters and Editors, an organization that trains investigative journalists, have built large amounts of their curricula around Excel. Of the journalism schools that teach data journalism, most of them begin and end with spreadsheets.  

The Stylebook says at a minimum, today's data journalists should keep a log that details:

* The source of the data, making sure to work on a copy of the data and not the original file.
* Data dictionaries or any other supporting documentation of the data.
* __"Description of all steps required to transform the data and perform the analysis."__

The trouble with Excel (or Google Sheets) is, unless you are keeping meticulous notes on what steps you are taking, there's no way to keep track. Many data journalists will copy and paste the values of a formula over the formula itself to prevent Excel from fouling up cell references when moving data around -- a practical step that also cuts off another path to being able to replicate the results.

An increasing number of data journalists are switching to tools like analysis notebooks, which use languages like Python and R, to document their work. The notebooks, generally speaking, allow a data journalist to mix code and explanation in the same document.

Combined with online sharing tools like GitHub, analysis notebooks seem to solve the problem of replication. But the number using them is small compared to those using spreadsheets. Recent examples of news organizations using analysis notebooks include the [Los Angeles Times](https://github.com/datadesk), the [New York Times](https://github.com/TheUpshot), [FiveThirtyEight](https://github.com/fivethirtyeight/data), and [Buzzfeed](https://github.com/BuzzFeedNews).

Peter Aldous, a data journalist at Buzzfeed recently published a story about how the online news site used machine learning to [find airplanes being used to spy on people in American cities](https://www.buzzfeednews.com/article/peteraldhous/us-marshals-spy-plane-over-mexico#.qqYnVj0B). Published with the story is the [code Aldous used to build his case](https://github.com/BuzzFeedNews/2017-08-spy-plane-finder).

"I think of it this way: As a journalist, I don't like to simply trust what people tell me. Sometimes sources lie. Sometimes they're just mistaken. So I like to verify what I'm told," he wrote in an email. "By the same token, why should someone reading one of my articles believe my conclusions, if I don't provide the evidence that explains how I reached them?"

The methodology document, associated code and source data took Aldous a few hours to create. The story, from the initial data work through the reporting required to make sense of it all, took a year. Aldous said there wasn't a discussion about if the methodology would be published because it was assumed -- "it's written into our DNA at BuzzFeed News."

"My background is in science journalism, and before that (way back in the 1980s) in science," Aldous said. "In science, there's been a shift from descriptive methods sections to publishing data and analysis code for reproducible research. And I think we're seeing a similar shift in data journalism. Simply saying what you've done is not as powerful as providing the means for others to repeat and build on your work."

Thibodeaux said that what Buzzfeed and others do with analysis notebooks and code repositories that include their data is "lovely."

"That to me is the shining city on the hill," Thibodeaux said. "We're not going to get there, and I don't think we have to for every story and every use case, and I don't think it's necessarily practical for every person working with data to get to that point."

There's a wide spectrum of approaches that still gets journalists to the essence of what the stylebook is trying to do, Thibodeaux said. There are many tools, many strategies, and the AP isn't going to advocate for any single one of them, he said. They're just arguing for transparency and replicability, even if that means doing more work.

"There's a certain burden that comes with transparency," he said. "And I think we have to accept that burden."

The question, Thibodeaux said, is what is sufficient? What's enough transparency? What does someone need for replicability?

"Maybe we do have to set a higher standard -- the more critical the analysis is to the story, and the more complex that analysis is, that's going to push the bar on what is a sufficient methodology statement," he said. "And it could end up being a whole code repo in order to just say, this isn't black magic, here's how we got it if you're so interested."

## "Receptivity ... is high"

Though written almost half a century ago, Meyer foresaw how data journalism was going to arrive in the newsroom.

"For the new methods to gain currency in journalism, two things must happen," he wrote. "Editors must feel the need strongly enough to develop the in-house capacity for systematic research ... The second need, of course, is for the editors to be able to find the talent to fill this need."

Meyer optimistically wrote that journalism schools were prepared to provide that talent -- they were not then, and only small handful are now -- but students were unlikely to be drawn to these new skills if they didn't see a chance to use those skills in their careers.

It's taken 45 years, but we are now at this point.

"The potential for receptivity, especially among the younger generation of newspaper managers, is high," Meyer wrote.

## Replication in notebooks

For our purposes in this book, replication requires two things from you, the student: What and why. What is this piece of code doing, and why are you doing that here and now? What lead you to this place? That you can copy and paste code from this book or the internet is not impressive. What is necessary for learning is that you know what a piece of code is doing a thing and why you want to do that thing here.

How will we replicate? We'll make use of special text files -- R Markdown, also known as R Notebooks -- that combine contextual text; the code we use to load, clean, analyze and visualize data; and the output of that code that allowed us to draw certain conclusions to use in stories.   

In an R Notebook, there are two blocks: A block that uses markdown, which has no special notation, and a code block. The code blocks can run mulitple languages inside R Studio. There's R, of course, but we could also run Python, a general purpose scripting language; and SQL, or Structured Query Language, the language of databases.

For the rest of the class, we're going to be working in notebooks.

In notebooks, you will both run your code and explain each step, much as I am doing here in this online book. This entire book was produced with R markdown files  

To start a notebook in R Studio, you click on the green plus in the top left corner and go down to R Notebook.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/verybasics2.png"))
```

In our first lab, we'll go through the process of editing a markdown notebook.

<!--chapter:end:03-replication.Rmd-->

# Data, structures and types

Data is everywhere. It surrounds you. Every time you use your phone or buy something online, you are creating data that is stored in cloud-computing centers. Modernity is drowning in data, and more comes along all the time. As journalists, it's critical that we learn to work with it.  
In this class we'll deal largely with two kinds of data: **unaggregated data** and **aggregated data**.

Unaggregated data, oversimplifying it a bit, is organized information that represents something tangible that exists in the real world. Aggregated data takes that raw, unaggregated information and summarizes it in some way.

In this class, we'll generally start with unaggregated ["tidy data"](https://tidyr.tidyverse.org/articles/tidy-data.html#defining-tidy-data), that is organized into a grid of rows and columns in tables.

Rows -- which run horizontally and are sometimes called observations or records -- each represent one individual element in a group.  Columns -- which run vertically and are also sometimes called variables or fields --  represent features of that group.

To illustrate this, think about how the Metropolitan Police Department collects information about crime incidents in Washington, D.C. in 2021.

Below is an image of an **unaggrgated** information about crime incidents taken from [Washington, D.C.'s open data repository](https://opendata.dc.gov/datasets/crime-incidents-in-2021/explore?location=38.893745%2C-77.019147%2C11.78&showTable=true).

```{r, echo=FALSE}
knitr::include_graphics(rep("images/data1.png"))
```

Each row (blue box) represents a single reported crime incident that -- if this data is accurate -- actually happened.  Each column (green box), represents some feature of that incident: the date on which occured, the type of crime, where it occurred. The highlighted row in blue represents incident number 21075574 (CCN), an auto theft (OFFENSE) that was reported on June, 7 2021 at 12:02 p.m. (REPORT_DATE) during the day shift (SHIFT) that occured in the 400 Block of M Street (BLOCK).  By looking where a given row intersects a given column, we can tell a little story about each incident.

This is unaggregated data. Aggregated data -- also called summary data -- will take raw data and summarize it in some way.  For example, a table that counts up all of the assaults, thefts, and homicides would represent aggregated data.  

## A Mental Picture
One of the critical components of data analysis, especially for beginners, is having a mental picture of your data. What does each row represent? What does each column in each row signify? How many rows do you have? How many columns?

All semester, we'll work with a dataset of loans made under the Paycheck Protection Act, a government program that loaned money to businesses during the Covid-19 pandemic with the goal of keeping people employed.  In the raw, unaggregated data set we'll work with, each row represents a single loan made to a single company. Each column describes a different feature of that loan: the company's name, the company's location, the industry the company operates in, the lender name, the amount of the loan, and so on.    

## Types
There are scores of data types in the world, and R has them. In this class, we're primarily going to be dealing with data frames, and each element of our data frames will have a data type.

Typically, they'll be one of four types of data:

* Numeric: a number, like the number of car accidents in a year or the number of journalism majors.
* Character: Text, like a name, a county, a state.
* Date: Fully formed dates -- 2019-01-01 -- have a special date type. Elements of a date, like a year (ex. 2019) are not technically dates, so they'll appear as numeric data types.
* Logical: Rare(ish), but every now and then we'll have a data type that's Yes or No, True or False, etc.

Is a zip code a number? Is a jersey number a number?

Trick question, because the answer is no. When we think of data types, numbers are things we do math on. If the thing you want is not something you're going to do math on -- can you add two phone numbers together? -- then make it a character type. If you don't, most every software system on the planet will drop leading zeros. For example, every zip code in Boston starts with 0. If you record that as a number, your zip code will become a four digit number, which isn't a zip code anymore.

When we load data into R, the library we'll use will try to guess the correct data type for each column.  Sometimes it gets it right, sometimes it gets it wrong.  As we'll see, we change data types during and after loading.

<!--chapter:end:04-databasics.Rmd-->

# Aggregates

## Libraries
R is a statistical programming language that is purpose built for data analysis.

Base R does a lot, but there are a mountain of external libraries that do things to make R better/easier/more fully featured. We already installed the tidyverse -- or you should have if you followed the instructions for the last assignment -- which isn't exactly a library, but a collection of libraries. Together, they make up the Tidyverse. Individually, they are extraordinarily useful for what they do. We can load them all at once using the tidyverse name, or we can load them individually. Let's start with individually.

The two libraries we are going to need for this assignment are `readr` and `dplyr`. The library `readr` reads different types of data in. For this assignment, we're going to read in csv data or Comma Separated Values data. That's data that has a comma between each column of data.

Then we're going to use `dplyr` to analyze it.

To use a library, you need to import it. Good practice -- one I'm going to insist on -- is that you put all your library steps at the top of your notebooks.

That code looks like this:

```{r}
library(readr)
```

To load them both, you need to do this:

```{r}
library(readr)
library(dplyr)
```

But, because those two libraries -- and several others that we're going to use over the course of this class -- are so commonly used, there's a shortcut to loading all of the libraries we'll need:

```{r}
library(tidyverse)
```

You can keep doing that for as many libraries as you need.

## Importing data

The first thing we need to do is get some data to work with. We do that by reading it in. In our case, we're going to read a datatable from an "rds" file, which is a format for storing data with R. Later in the course, we'll more frequently work with a format called a CSV. A CSV is a stripped down version of a spreadsheet you might open in a program like Excel, in which each column is separated by a comma. RDS files are less common when getting data from other people.  But reading in CSVs is less foolproof than reading in rds files, so for now we'll work with rds.

The rds file we're going to read in is from the [Small Business Administration's Paycheck Protection Program(https://www.sba.gov/funding-programs/loans/covid-19-relief-options/paycheck-protection-program)]. The program gave loans to businesses to keep people employed during the Covid-19 pandemic. We'll be working with a slice of the data that documents loans to Maryland businesses.

So step 1 is to import the data. The code to import the data looks like this:

`ppp_maryland_loans <- read_rds("ppp_maryland.rds")`

Let's unpack that.

The first part -- **ppp_maryland_loans** -- is the name of a variable.

A **variable** is just a name that we'll use to refer to some more complex thing. In this case, the more complex thing is the data we're importing into R that will be stored as a **dataframe**, which is one way R stores data.

We can call this variable whatever we want. The variable name doesn't matter, technically.  We could use any word.  You could use your first name, if you like. Generally, though, we want to give variables names that are descriptive of the thing they refer to. Which is why we're calling this one **ppp_maryland_loans**. Variable names, by convention are one word all lower case (or two or more words connected by an underscore). You can end a variable with a number, but you can't start one with a number.

The `<-` bit, you'll recall from the basics, is the **variable assignment operator**. It's how we know we're assigning something to a word. Think of the arrow as saying "Take everything on the right of this arrow and stuff it into the thing on the left." So we're creating an empty vessel called **ppp_maryland_loans** and stuffing all this data into it.

**read_rds()** is a function, one that only works when we've loaded the tidyverse.  A **function** is a little bit of computer code that takes in information and follows a series of pre-determined steps and spits it back out.
A recipe to make pizza is a kind of function.  We might call it **make_pizza()**.

The function does one thing. It takes a preset collection of ingredients -- flour, water, oil, cheese, tomato, salt -- and passes them through each step outlined in a recipe, in order.  Things like: mix flour and water and oil, knead, let it sit, roll it out, put tomato sauce and cheese on it, bake it in an oven, then take it out.  

The output of our **make pizza()** function is a finished pie.

We'll make use of a lot of pre-written functions from the tidyverse and other packages, and even write some of our own. Back to this line of code:

`ppp_maryland_loans <- read_rds("ppp_maryland.rds")`

Inside of the **read_rds()** function, we've put the name of the file we want to load.  Things we put inside of function, to customize what the function does, are called **arguments**.

The easiest thing to do, if you are confused about how to find your data, is to put your data in the same folder as as your notebook (you'll have to save that notebook first). If you do that, then you just need to put the name of the file in there (ppp_maryland.rds). If you put your data in a folder called "data" that sits next to your data notebook, your function would instead look like this:

```{r, eval=FALSE}

ppp_maryland_loans <- read_rds("data/ppp_maryland.rds")

```

```{r, echo=FALSE}

ppp_maryland_loans <- read_rds("pre_labs/pre_lab_01/pre_lab_01.rds")

```

In this data set, each row represents a loan application, and each column represents a feature of that loan: who it went to, how much it was for, whether the loan was forgiven.

After loading the data, it's a good idea to get a sense of its shape.  What does it look like? There are several ways we can examine it.

By looking in the R Studio environment window, we can see the number of rows (called "obs.", which is short for observations), and the number of columns(called variables).  We can double click on the dataframe name in the environment window, and explore it like a spreadsheet.  

There are several useful functions for getting a sense of the dataset right in our markdown document.

If we run `glimpse(ppp_maryland_loans)`, it will give us a list of the columns, the data type for each column and and the first few values for each column.  

```{r}
glimpse(ppp_maryland_loans)
```

If we type `head(ppp_maryland_loans)`, it will print out the columns and the first six rows of data.

```{r}
head(ppp_maryland_loans)
```
We can also click on the data name in the R Studio environment window to explore it interactively.

## Group by and count

So what if we wanted to know how many loans were made in each Maryland county?

To do that by hand, we'd have to take each of the 195,856 individual rows (or observations or records) and sort them into a pile. We'd put them in groups -- one for each county -- and then count them.

`dplyr` has a group by function in it that does just this. A massive amount of data analysis involves grouping like things together and then doing simple things like counting them, or averaging them together. So it's a good place to start.

So to do this, we'll take our dataset and we'll introduce a new operator: `|>`. The best way to read that operator, in my opinion, is to interpret that as "and then do this."

We're going to establish a pattern that will come up again and again throughout this book: `data |> function`. In English: take your data set and then do this specific action to it.

The first step of every analysis starts with the data being used. Then we apply functions to the data.

In our case, the pattern that you'll use many, many times is: `data |> group_by(COLUMN NAME) |> summarize(VARIABLE NAME = AGGREGATE FUNCTION(COLUMN NAME))`

In our dataset, the column with county information is called "project_county_name"

Here's the code to count the number of loans in each county:

```{r}
ppp_maryland_loans |>
  group_by(project_county_name) |>
  summarise(
    count_loans = n()
  )
```

So let's walk through that.

We start with our dataset -- `ppp_maryland_loans` -- and then we tell it to group the data by a given field in the data. In this case, we wanted to group together all the counties, signified by the field name `project_county_name`, which you could get from using the glimpse() function. After we group the data, we need to count them up.

In dplyr, we use the `summarize()` function, [which can do alot more than just count things](http://dplyr.tidyverse.org/reference/summarise.html).

Inside the parentheses in summarize, we set up the summaries we want. In this case, we just want a count of the number of loans for each county grouping. The line of code `count_loans = n(),` says create a new field, called `total_loans` and set it equal to `n()`. `n()` is a function that counts the number of rows or records in each group.  Why the letter n? The letter n is a common symbol used to denote a count of something. The number of things (or rows or observations or records) in a dataset? Statisticians call it n. There are n number of loans in this dataset.

When we run that, we get a list of counties with a count next to them. But it's not in any order.

So we'll add another "and then do this" symbol -- |> -- and use a new function called `arrange()`. Arrange does what you think it does -- it arranges data in order. By default, it's in ascending order -- smallest to largest. But if we want to know the county with the most loans, we need to sort it in descending order. That looks like this:

```{r}
ppp_maryland_loans |>
  group_by(project_county_name) |>
  summarise(
    count_loans = n()
  ) |>
  arrange(desc(count_loans))
```
Montgomery County has 38,781 loans, more than any other Maryland county.

We can, if we want, group by more than one thing.

The ppp loan data contains a column detailing the race of the business owner: "race". It has six possible values:  

* American Indian or Alaska Native
* Asian
* Black or African American
* Native Hawaiian or Other Pacific Islander
* White
* Unanswered

We can group by "project_county_name" and "race" to see how many loans fell in each category in each county. We'll sort alphabetically by county and then race.

```{r}
ppp_maryland_loans |>
  group_by(project_county_name,race) |>
  summarise(
    count_loans = n()
  ) |>
  arrange(project_county_name,race)
```

As you can see here, it's not clear that we'll be able to make use of the race data going forward.  By far the most common category in every county is "Unanswered". We'll avoid grouping by race in the rest of the examples in this chapter.

## Other summarization methods: summing, mean, median, min and max

In the last example, we grouped like records together and counted them, but there's so much more we can to summarize each group.

Let's say we wanted to know the total dollar amount of loans to each county? For that, we could use the `sum()` function to add up all of the loan values in the column "amount". We put the column we want to total -- "amount" -- inside the sum() function `sum(amount)`. Note that we can simply add a new summarize function here, keeping our count_loans field in our output table.

```{r}
ppp_maryland_loans |>
  group_by(project_county_name) |>
  summarise(
    count_loans = n(),
    total_loans_amount = sum(amount)
  ) |>
  arrange(desc(total_loans_amount))
```

We can also calculate the average size loan in each county -- the mean -- and the loan amount that sits at the midpoint of our data -- the median.  

```{r}
ppp_maryland_loans |>
  group_by(project_county_name) |>
  summarise(
    count_loans = n(),
    total_loans_amount = sum(amount),
    mean_loan_amount = mean(amount),
    median_loan_amount = median(amount)
  ) |>
  arrange(desc(mean_loan_amount))
```

We see something interesting here.  The mean loan amount is much higher than the median loan amount in every county.  Why? There are some very large loans -- millions of dollars -- skewing the mean higher.  Examining both the median -- which is less sensitive to extreme values -- and the mean -- which is more sensitive to extreme values --  gives you a clearer picture of the composition of the data.

Howard County has the highest average loan amount (107,217), while Carroll County has the highest median ($25,111.00)

What about the highest and lowest loan values in each county?  For that, we can use the `min()` and `max()` functions.


```{r}
ppp_maryland_loans |>
  group_by(project_county_name) |>
  summarise(
    count_loans = n(),
    total_loans_amount = sum(amount),
    mean_loan_amount = mean(amount),
    median_loan_amount = median(amount),
    min_loan_amount = min(amount),
    max_loan_amount = max(amount)
  ) |>
  arrange(desc(max_loan_amount))
```
From this, we can see that the largest loan amounts in Anne Arundel County, Baltimore County, Carroll County, Garrett County and Howard County were $10 million, the maximum loan value authorized under the program.

It would be interesting to see what these companies are that got the maximum of $10 million.  To do that, we could simply take our original data set and sort it from highest to lowest on the loan amount.

```{r}
ppp_maryland_loans |>
  arrange(desc(amount))

```
The first five rows here all represent companies that got $10 million loans. Who are they? What industries are they in?  The answers to these questions could provide a lead for further reporting.

<!--chapter:end:05-aggregates.Rmd-->

# Mutating data

Often the data you have will prompt questions that it doesn't immediately answer. The PPP loan applications include a total amount column but also columns for different categories the money would be used for: payroll, utilities, rent, etc. Amounts are great, but comparing absolute numbers to each other is only useful if you have a very small number. We need percentages!

To do that in R, we can use `dplyr` and `mutate` to calculate new metrics in a new field using existing fields of data. That's the essence of `mutate` - using the data you have to answer a new question.

So first we'll import the tidyverse so we can read in our data and begin to work with it.

```{r}
library(tidyverse)
```

Now we'll import a dataset of PPP applications from Maryland that is in the data folder in this chapter's pre-lab directory. We'll use this, a sample of the entire PPP loan data, to explore ways to create new information from existing data.

```{r}
maryland_ppp <- read.csv('data/ppp_applications_md.csv')
```

First, let's add a column called `percent_payroll` for the percentage of each loan application that payroll expenses represent. The code to calculate a percentage is pretty simple. Remember, with `summarize`, we used `n()` to count things. With `mutate`, we use very similar syntax to calculate a new value -- a new column of data -- using other values in our dataset. So in this case, we're trying to do , but we're doing it with fields.

If we look at what we got when we imported the data, you'll see there's `payroll_proceed` as the numerator, and we'll use `amount` as the denominator.

```{r}
maryland_ppp |>
  select(loan_number, amount, payroll_proceed) |>
  mutate(
  percent_payroll = payroll_proceed/amount
)
```
Now we've got our `percent_payroll` column. But what do you see right away? Do those numbers look like we expect them to? No. They're a decimal expressed as a percentage. So let's fix that by multiplying by 100.

```{r}
maryland_ppp |>
  select(loan_number, amount, payroll_proceed) |>
  mutate(
  percent_payroll = (payroll_proceed/amount)*100
)
```
Now, does this ordering do anything for us? No. Let's fix that with arrange.

```{r}
maryland_ppp |>
  select(loan_number, amount, payroll_proceed) |>
  mutate(
  percent_payroll = (payroll_proceed/amount)*100
)  |> arrange(desc(percent_payroll))
```

So now we have loans ordered by `percent_payroll` with the highest percentage first. Most loans are predominantly used for payroll expenses, so let's reverse that `arrange` function to show the lowest percentages first, then with the largest amount first.

```{r}
maryland_ppp |>
  select(loan_number, amount, payroll_proceed) |>
  mutate(
  percent_payroll = (payroll_proceed/amount)*100
)  |> arrange(percent_payroll, desc(amount))
```

There's a loan application for $1.7 million where none of it would be used for payroll, and two six-figure loans with the same percentage. Wonder what those are about?

## Another use of mutate

Take a look at the `city` column in our data.

```{r eval=FALSE}
View(maryland_ppp)
```

You'll notice that there's a mix of styles: "Baltimore" and "BALTIMORE" for example. R will think those are two different cities, and that will mean that any aggregates we create based on city won't be accurate.

So how can we fix that? Mutate - it's not just for math! And a function called `str_to_upper` that will convert a character column into all uppercase.

```{r}
maryland_ppp |>
  mutate(
    upper_city = str_to_upper(city)
)
```

We could do the same thing with the `address` column in order to standardize that for analysis, too.

## A more powerful use

Mutate is even more useful when combined with some additional functions. Let's say you want to know if the servicing lender is located in Maryland or outside the state. There are three possible answers:

1. The lender is in Maryland
2. The lender is outside Maryland
3. The data doesn't tell us (`servicing_lender_state` is blank or NA)

We can create a new column that accounts for these possibilities and populate it using mutate and `case_when`, which is like an if/else statement but for more than two options.

```{r}
maryland_with_in_out <- maryland_ppp |>
  mutate(
    in_out = case_when(
        servicing_lender_state == 'NA' ~ "NA",
        servicing_lender_state == 'MD' ~ "IN",
        servicing_lender_state != 'MD' ~ "OUT"
      )
  )
```

We can then use our new `in_out` column in group_by statements to make summarizing easier.

In this case there are no Maryland loans where `servicing_lender_state` has a value of 'NA', but you should never assume that will be the case for a dataset. If you know that the only options are the lender is in Maryland or is outside it, you can rewrite the previous code as an if/else statement:

```{r}
maryland_with_in_out <- maryland_ppp |>
  mutate(
    in_out = if_else(
        servicing_lender_state == 'MD', "IN", "OUT"
      )
  )
```

Mutate is there to make your data more useful and to make it easier for you to ask more questions of it.

<!--chapter:end:06-mutating.Rmd-->

# Working with dates

One of the most frustrating things in data is working with dates. Everyone has a different opinion on how to record them, and every software package on the planet has to sort it out. Dealing with it can be a little ... confusing. And every dataset has something new to throw at you. So consider this an introduction.

We're going to do this two ways. First I'm going to show you how to use base R to solve a tricky problem. And then we'll use a library called `lubridate` to solve a more common and less tricky problem. And then we'll use a new library to solve most of the common problems before they start.

## The hard way

First, we'll import `tidyverse` like we always do.

```{r}
library(tidyverse)
```

Now we'll import a dataset of PPP applications from Maryland that is in the data folder in this chapter's pre-lab directory. If we do this the old way -- using read.csv -- this is what we get:

```{r}
maryland_ppp <- read.csv('data/ppp_applications_md.csv')
head(maryland_ppp)
```
Note the date is a factor, not a date. We have to fix that. There's a lot of ways to fix dates. The base R way is to use formatting. The code is ... a little odd ... but it's useful to know if you get a really odd date format. What you are doing is essentially parsing the date into it's component parts then reassembling it into a date using formatting.

```{r}
new_maryland_ppp <- maryland_ppp |> mutate(
    CleanDate = as.POSIXct(date_approved, format="%Y-%m-%d %H:%M:%S")
)
head(new_maryland_ppp)
```

CleanDate is now an actual date format that includes times, so you can use `filter` to isolate applications with specific dates.

You can almost read the code that created it: The format of the date is %Y, which means a four digit year DASH %m or two digit month DASH %d or two digit day SPACE %H or two digit hour COLON %M or two digit minute COLON %S or two digit second. You can remix that as you need. If you had a date that was `20021212` then you would do `format="%Y%m%d"` and so on.

There is a [library called lubridate](https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html) that can parse some common date problems. If it's not already installed, just run `install.packages('lubridate')`

```{r}
library(lubridate)
```

Lubridate can handle this tickets data easier with one of it's many functions. The functions parse dates given a basic pattern. In this case, our data is in a very common pattern of month date year hours minutes seconds. Lubridate has a function called `ymd_hms`.

```{r}
lubridate_maryland_ppp <- maryland_ppp |> mutate(
    CleanDate = ymd_hms(date_approved)
)
head(lubridate_maryland_ppp)
```
That's less code and less weirdness, so that's good.

But to get clean data, I've installed a library and created a new field so I can now start to work with my dates. That seems like a lot, but don't think your data will always be perfect and you won't have to do these things.

Still, there's got to be a better way. And there is.

Fortunately, `readr` anticipates some date formattings and can automatically handle this (indeed it uses lubridate under the hood). The change in your code? You just use `read_csv` instead of `read.csv`

```{r}
maryland_ppp <- read_csv("data/ppp_applications_md.csv")
head(maryland_ppp)
```

And just like that, the dates are formatted correctly.

But you're not done with lubridate yet. It has some interesting pieces parts we'll use elsewhere.

What's a question you might have about PPP loan applications involving dates?

How about what month are the most applications approved? We could use formatting to create a Month field but that would group all the Aprils ever together. We could create a year and a month together, but that would give us an invalid date object and that would create problems later. Lubridate has something called a floor date that we can use.

So to follow along here, we're going to use mutate to create a month field, group by to lump them together, summarize to count them up and arrange to order them. We're just chaining things together.

```{r}
maryland_ppp |>
  mutate(Month = floor_date(date_approved, "month")) |>
  group_by(Month) |>
  summarise(total = n()) |>
  arrange(desc(total))
```

So the most applications in this dataset were issued in April of 2020. April of 2021 was second. Then May of 2020 and March and May of 2021. This isn't random; there are reasons why those months were the most common that we'll learn as we delve into the PPP loan program.

<!--chapter:end:07-workingwithdates.Rmd-->

# Filters and selections

More often than not, we have more data than we want. Sometimes we need to be rid of that data. In `dplyr`, there's two ways to go about this: filtering and selecting.

**Filtering creates a subset of the data based on criteria**. All records where the amount is greater than 150,000. All records that match "College Park". Something like that. **Filtering works with rows -- when we filter, we get fewer rows back than we start with.**

**Selecting simply returns only the fields named**. So if you only want to see city and amount, you select those fields. When you look at your data again, you'll have two columns. If you try to use one of your columns that you had before you used select, you'll get an error. **Selecting works with columns. You will have the same number of records when you are done, but fewer columns of data to work with.**

Now we'll import a dataset of PPP applications from Maryland that is in the data folder in this chapter's pre-lab directory. It has loan applications from all over the state, so one way to filter is to isolate on "county" - Maryland has both counties and Baltimore City as jurisdictions. Let's start by loading tidyverse and reading in the Maryland data:

```{r}
library(tidyverse)
```

```{r}
maryland_ppp <- read_csv('data/ppp_applications_md.csv')
```

The data we want to filter on is in `project_county_name`. So we're going to use filter and something called a comparison operator. We need to filter all records equal to "PRINCE GEORGES". The comparison operators in R, like most programming languages, are == for equal to, != for not equal to, > for greater than, >= for greater than or equal to and so on.

**Be careful: `=` is not `==` and `=` is not "equal to". `=` is an assignment operator in most languages -- how things get named.**

```{r}
prince_georges <- maryland_ppp |> filter(project_county_name == "PRINCE GEORGES")

head(prince_georges)
```

And just like that, we have just Prince Georges, which we can verify looking at the head, the first six rows.

We also have more data than we might want. For example, we may only want to work with the name, address, city, zip code and amount of Prince George's applicants.

To simplify our dataset, we can use select.

```{r}
selected_prince_georges <- prince_georges |> select(name, address, city, zip, amount)

head(selected_prince_georges)
```

And now we only have five columns of data for whatever analysis we might want to do.

## Combining filters

So let's say we wanted to know how many Prince George's applications were from non-profit organizations and applied for more than $150,000. We can do this a number of ways. The first is we can chain together a whole lot of filters.

```{r}
large_prince_georges_nonprofit <- maryland_ppp |> filter(project_county_name == "PRINCE GEORGES") |> filter(business_type == "Non-Profit Organization") |> filter(amount > 150000)

nrow(large_prince_georges_nonprofit)
```

That gives us 145 applicants. But that's silly and repetitive, no? We can do better using boolean operators -- AND and OR. In this case, AND is `&` and OR is `|`.

The difference? With AND, all three things must be true to be included. With OR, any of those three things can be true and it will be included. A Prince George's corporation will get included because it applied for more than $150k. One of the conditions is true.

Here's the difference.

```{r}
and_prince_georges <- maryland_ppp |> filter(project_county_name == "PRINCE GEORGES" & business_type == "Non-Profit Organization" & amount > 150000)

nrow(and_prince_georges)
```
So AND gives us the same answer we got before. What does OR give us?

```{r}
or_prince_georges <- maryland_ppp |> filter(project_county_name == "PRINCE GEORGES" | business_type == "Non-Profit Organization" | amount > 150000)

nrow(or_prince_georges)
```
So there's 54,321 applications from Maryland applicants who are EITHER in Prince George's OR a non-profit organization OR applied for more than $150,000. Or is inclusive; AND is exclusive.

A general tip about using filter: it's easier to work your way towards the filter syntax you need rather than try and write it once and trust the result. Each time you modify your filter, check the results to see if they make sense. This adds a little time to your process but you'll thank yourself for doing it because it helps avoid mistakes.

<!--chapter:end:08-filters.Rmd-->

# Data Cleaning Part I: Data smells

Any time you are given a dataset from anyone, you should immediately be suspicious. Is this data what I think it is? Does it include what I expect? Is there anything I need to know about it? Will it produce the information I expect?

One of the first things you should do is give it the smell test.

Failure to give data the smell test [can lead you to miss stories and get your butt kicked on a competitive story](https://source.opennews.org/en-US/learning/handling-data-about-race-and-ethnicity/).

With data smells, we're trying to find common mistakes in data. [For more on data smells, read the GitHub wiki post that started it all](https://github.com/nikeiubel/data-smells/wiki/Ensuring-Accuracy-in-Data-Journalism). Some common data smells are:

* Missing data or missing values
* Gaps in data
* Wrong type of data
* Outliers
* Sharp curves
* Conflicting information within a dataset
* Conflicting information across datasets
* Wrongly derived data
* Internal inconsistency
* External inconsistency
* Wrong spatial data
* Unusable data, including non-standard abbreviations, ambiguous data, extraneous data, inconsistent data

Not all of these data smells are detectable in code. You may have to ask people about the data. You may have to compare it to another dataset yourself. Does the agency that uses the data produce reports from the data? Does your analysis match those reports? That will expose wrongly derived data, or wrong units, or mistakes you made with inclusion or exclusion.

But with several of these data smells, we can do them first, before we do anything else.

We're going to examine three here as they apply to the PPP loan data we've been working with: wrong type, missing data and gaps in data.  

## Wrong Type

First, let's look at **Wrong Type Of Data**.

We can sniff that out by looking at the output of `readr`.

Let's load the tidyverse.  

```{r}

# Remove scientific notation
options(scipen=999)
# Load the tidyverse
library(tidyverse)

```

Then let's load the Maryland slice of PPP loan data we've used previously.

This time, we're going to load the data in a CSV format, which stands for comma separated values and is essentially a fancy structured text file. Each column in the csv is separated -- "delimited" -- by a comma from the next column. The file has a ".gz" extension on the end because it's zipped up to keep the file sizes smaller.

We're also going to introduce a new argument to our function that reads in the data, read_csv(), called "guess_max". As R reads in the csv file, it will attempt to make some calls on what "data type" to assign to each field: number, character, date, and so on. The "guess_max" argument says: look at the values in the whatever number of rows we specify before deciding which data type to assign. In this case, we'll pick 10.

```{r}
# Load the data
ppp_maryland_loans <- read_csv("data/ppp_loan_data/processed/md/ppp_loans_md.csv.zip", guess_max=10)

```

Pay attention to the red warning that signals "one or more parsing issues." It advises us to run the problems() function to see what went wrong.  Let's do that.

```{r}

problems(ppp_maryland_loans)

```

It produces a table of all the parsing problems. It has 15,708 rows, which means we have that many problems.  In almost every case here, the `readr` library has guessed that a given column was of a "logical" data type -- True or False. It did it based on very limited information -- only 1,000 rows.  So, when it hit a value that looked like a date, or a character string, it didn't know what to do.  So it just didn't read in that value correctly.

The easy way to fix this is to set the guess_max argument higher. It will take a little longer to load, but we'll use every single row in the data set to guess the column type -- 195,865

```{r}
ppp_maryland_loans <- read_csv("data/ppp_loan_data/processed/md/ppp_loans_md.csv.zip", guess_max=195865)
```
This time, we got no parsing failures.  And if we examine the data types `readr` assigned to each column using glimpse(), they generally make sense.

```{r}
glimpse(ppp_maryland_loans)
```

Things that should be characters -- like state, city, name -- are characters (chr). Things that should be numbers (dbl) -- like amount -- are numbers. Date columns -- like date_approved -- are stored as dates.

There are some minor problem.  The id column is a good example.  It read in as a number (dbl), which makes sense, because it really is just a string of numbers.  But we'd never need to do math on these values; it wouldn't make sense to add two ids together, for example.  So it is probably best stored as a character. The opposite would be more problematic.  If something that should be stored as a number we want to do math on was stored as a character, we couldn't actually use it to do math.

We can fix that pretty easily, by overwriting the column while changing the data type, using mutate()

```{r}

ppp_maryland_loans <- ppp_maryland_loans |>
  mutate(id = as.character(id))

```

When we glimpse() the dataframe again, it's been changed

```{r}

glimpse(ppp_maryland_loans)

```

## Missing Data

The second smell we can find in code is **missing data**.

We can do that by grouping and counting columns. In addition to identifying the presence of NA values, this method will also give us a sense of the distribution of values in those columns.

Let's start with the "franchise" column. The following code groups by the franchise name column, counts the number in each group, and then sorts from highest to lowest.   

There are 192,959 NA values in this column.  This makes sense. Not every business will be a franchisee of a larger company like Subway or Dunkin'. In this case, the presence of so many NAs isn't really concerning.

```{r}

ppp_maryland_loans |>
  group_by(franchise_name) |>
  summarise(
    count=n()
  ) |>
  arrange(desc(count))
```
Now let's try the "forgiveness_amount" column, which represents the amount of the loan that was forgiven, or not required to be paid back. In this case, there are 135,073 NA values. The rest have different dollar amounts.  

```{r}

ppp_maryland_loans |>
  group_by(forgiveness_amount) |>
  summarise(
    count=n()
  ) |>
  arrange(desc(count))
```
Do the 135,073 NAs represent loans that weren't forgiven?

Do they represent loans that might have been forgiven, but the data is simply missing the amount of money?  

We could check the documentation, which isn't particularly helpful.  It only says the column represents "forgiveness amount."

We could check the "forgiveness_date" column, to see how many NAs it has: 135,703, the same number as the number of NAs in "forgiveness_amount".   

```{r}

ppp_maryland_loans |>
  group_by(forgiveness_date) |>
  summarise(
    count=n()
  ) |>
  filter(is.na(forgiveness_date)) |>
  arrange(desc(count))
```
We can group by forgiveness_amount and forgiveness_date to determine whether one is NA when the other is NA.  Because this grouping has 135,073 rows, too, we know this to be the case.

```{r}
ppp_maryland_loans |>
  group_by(forgiveness_amount, forgiveness_date) |>
  summarise(
    count=n()
  ) |>
  filter(is.na(forgiveness_date)) |>
  arrange(desc(count))

```

Before we decide to base a publishable finding on this column, we should call the custodian of the data to confirm our hypothesis of NA values in these columns, that they represent loans that have not been forgiven.

## Gaps in data

Let's now look at **gaps in data**. It's been my experience that gaps in data often have to do with time, so let's first look at "date_approved", so we can see if there's any missing months, or huge differences in the number of loans by month. Let's start with Date. If we're going to work with dates, we should have `lubridate` handy for `floor_date`.

```{r}
library(lubridate)
```

The `floor_date` function will allow us to group by month, instead of a single day.

```{r}
ppp_maryland_loans |>
  mutate(month_year_approved = floor_date(date_approved, "month")) |>
  group_by(month_year_approved) |>
   summarise(
    count=n()
  ) |>
  arrange(month_year_approved)
```

So, our data starts in April 2020, the month that has more loans -- 45,040 -- than any other month.  That makes sense, as the program launched at the start of the pandemic, in April 2020.  The number of loans declines each month through August 2020, as the initial round of the program worked through the initial funding allocation.  

Then, there are no loans until January 2021. Does the four-month gap in loans in this dataset represent a problem? Are we missing a bunch of records?  

Probably not. The program was reauthorized a few times. For example, it expired in August 2020, before being reauthorized with new loans being given in January 2021.

It's good to be aware of all gaps in data, but they don't always represent a problem.

## Supicious Outliers

Any time you are going to focus on a column for analysis, you should check for suspicious values. Are there any unusually large values or unusually small values?  Are there any values that should not exist in the data?

Let's consider the loan amount column from our Maryland PPP data, and find the largest (max) and smallest (min) amount.  

```{r}

ppp_maryland_loans |>
  summarise(max_amount = max(amount),
            min_amount= min(amount))
```
The largest amount is \$10 million which is the maximum size of a loan under the program. If the max amount was larger than \$10 million, that would be suspicious.  Similarly, if the smallest amount was a negative number, that would also raise an eyebrow.  What about $6?  That does seem a little odd. Why would someone take out a loan for that amount of money?

Let's take a look at the full record set for any loan less than $100.

```{r}

ppp_maryland_loans |>
 filter(amount < 100)
```
We have two businesses, "GETGFTD LLC" and "LEGACY SPINE AND PAIN LLC" that both got very small loans of \$6 and \$78. Scan through the columns. Let's check for internal consistency.  There are other columns in the data that have related information: initial_approval_amount, current_approval_amount and payroll_proceed. Let's select just those columns

```{r}

ppp_maryland_loans |>
 filter(amount < 100) |>
  select(name, amount, initial_approval_amount, current_approval_amount, payroll_proceed)
```
Those are all the same value, so at least this number is internally consistent.  But is it correct? Did someone mistype it when entering the data?  

A call to the bank -- Bank of America in both cases -- or the companies could help resolve this mystery. Or a call to the SBA -- the owner of the data -- to ask about small values generally may help us understand if small values like this are suspicious or not.


<!--chapter:end:09-datasmells.Rmd-->

# Data Cleaning Part II: Janitor

The bane of every data analyst's existence is data cleaning.

Every developer, every data system, every agency, the all have opinions about how data gets collected. Some decisions make sense from the outside. Some decisions are based entirely on internal politics: who is creating the data, how they are creating it, why they are creating it. Is it automated? Is it manual? Are data normalized? Are there free form fields where users can just type into or does the system restrict them to choices?

Your journalistic questions -- what you want the data to tell you -- is almost never part of that equation.

So cleaning data is the process of fixing issues in your data so you can answer the questions you want to answer. Data cleaning is a critical step that you can't skip past. A standard metric is that 80 percent of the time working with data will be spent cleaning and verifying data, and 20 percent the more exciting parts like analysis and visualization.  

The tidyverse has a lot of built-in tools for data cleaning.  We're also going to make use of a new library, called `janitor` that has a bunch of great functions for cleaning data.  Let's load those now.

```{r}
library(tidyverse)
library(janitor)
```

Now let's load a tiny slice of our Maryland PPP loan dataset. To make the cleaning demonstration in this chapter easier, this dataset only has six rows and seven columns. All six of these loans are from Arnold, Maryland. The full data set has more loans for Arnold, but we're only showing these six here.

```{r}
arnold_md_loans <- read_rds("data/ppp_loan_data/processed/md/arnold_md_loans.rmd")
```

Let's glimpse it to get a sense of it, to examine the column data types and possible values.

```{r}
glimpse(arnold_md_loans)
```

And let's examine the full data set.

```{r}
arnold_md_loans
```

There are a number of issues with this data set that might get in the way of asking questions and receiving accurate answers. They are:

* The column headers are inconsistently styled (note: I've purposely dirtied these up, which is why they look different than previous versions of this data we've loaded).  The first column "1_id" starts with a number. The "NAME" column is all caps, while the rest are lowercase. And "street address" has a space in it. Those problems will make them hard to analyze, to refer to in functions we write.
* The amount column is stored as a character, not a number. If we try to do math to it -- say, calculate the average loan size -- it won't work.
* There's a fully duplicated row -- a common problem in data sets.  The first row is exactly the same as the second.
* The city field has five different forms -- including misspellings -- of Arnold. If we wanted to group and count the number of loans in Arnold, this inconsistency would not let us do that correctly.
* The zip field mixes five digit ZIP codes and nine digit ZIP codes.  If we wanted to group and count the number of loans in a given ZIP code, this inconsistency would not let us do that correctly.
* The street address field is inconsistent. It has multiple variations of Ritchie Hwy.

Let's get cleaning.  Our goal will be to build up one block of code that does all the necessary cleaning in order to answer this question: what is the total amount of loans made to businesses in Arnold, MD in ZIP code 21012?

## Cleaning headers

One of the first places we can start with cleaning data is cleaning the column names (or headers).

Every system has their own way of recording headers, and every developer has their own thoughts of what a good idea is within it. R is most happy when headers are lower case, without special characters.

If column headers start with a number, or have a space in between two words, you have to set them off with backticks when using them in a function. Generally speaking, we want one word (or words separated by an underscore), all lowercase, that don't start with numbers.  

The `janitor` library makes fixing headers trivially simple with the function `clean_names()`

```{r}
# cleaning function
cleaned_arnold_md_loans <- arnold_md_loans |>
  clean_names()

# display the cleaned dataset
cleaned_arnold_md_loans


```

This function changed `NAME` to `name`.  It put an underscore in `street_address` to get rid of the space.  And it changed `1_id` to `x1_id`. That last one was an improvement -- it no longer starts with a number -- but it's still kind of clunky.

We can use a tidyverse function `rename()` to fix that. Let's just call it `id`

```{r}
# cleaning function
cleaned_arnold_md_loans <- arnold_md_loans |>
  clean_names() |>
  rename(id = x1_id)

# display the cleaned dataset
cleaned_arnold_md_loans


```

## Changing data types

Right now, the amount column is stored as a character.  Do you see the little `<chr>` under the amount column in the table above?  If we wanted to do math to it, we'd get an error, like so.

```{r, error=TRUE}
# cleaning function
total_arnold_md_loans <- cleaned_arnold_md_loans |>
  summarise(total_amount = sum(amount))

# display the cleaned dataset
total_arnold_md_loans


```

We got an "invalid 'type' (character)" error.  So let's fix that using the mutate() function in concert with as.numeric().  We'll reuse the same column name, so it overwrites it.  

```{r}
# cleaning function
cleaned_arnold_md_loans <- arnold_md_loans |>
  clean_names() |>
  rename(id = x1_id) |>
  mutate(amount = as.numeric(amount))


# display the cleaned dataset
cleaned_arnold_md_loans

```

Notice that the amount has been converted to a `<dbl>`, which is short for double, a number format.  When we attempt to add up all of the amounts to create a total, this time it works fine.

```{r}
# cleaning function
total_arnold_md_loans <- cleaned_arnold_md_loans |>
  summarise(total_amount = sum(amount))

# display the cleaned dataset
total_arnold_md_loans


```
## Duplicates

One of the most difficult problems to fix in data is duplicate records in the data. They can creep in with bad joins, bad data entry practices, mistakes -- all kinds of reasons. A duplicated record isn't always there because of an error, but you need to know if it's there before making that determination.

So the question is, do we have any records repeated?

Here we'll use a function called `get_dupes` from the janitor library to check for fully repeated records in our cleaned data set.   

```{r}
cleaned_arnold_md_loans |>
  get_dupes()
```

In this case, the first two records in our table are fully duplicated. Every field is identical in each.

We can fix this by adding the function `distinct()` to our cleaning script.  This will keep only one copy of each unique record in our table

```{r}
# cleaning function
cleaned_arnold_md_loans <- arnold_md_loans |>
  clean_names() |>
  rename(id = x1_id) |>
  mutate(amount = as.numeric(amount)) |>
  distinct()


# display the cleaned dataset
cleaned_arnold_md_loans

```

## Cleaning strings

The rest of the problems with this data set all have to do with inconsistent format of values in a few of the columns.  To fix these problems, we're going to make use of mutate() and a new function, `case_when()` in concert with "string functions" -- special functions that allow us to clean up columns stored as character strings. The tidyverse package `stringr` has lots of useful string functions, more than we'll learn in this chapter.

Let's start by cleaning up the zip field. Remember, three of rows had a five-digit ZIP code, while two had a nine-digit ZIP code, separated by a hyphen.

We're going to write code that tells R to keep the first five digits on the left, and get rid of anything after that by using `mutate()` in concert with `str_sub()`, from the `stringr` package.

```{r}
# cleaning function
cleaned_arnold_md_loans <- arnold_md_loans |>
  clean_names() |>
  rename(id = x1_id) |>
  mutate(amount = as.numeric(amount)) |>
  distinct() |>
  mutate(zip = str_sub(zip, start=1L, end=5L))


# display the cleaned dataset
cleaned_arnold_md_loans

```

Let's break down this line of code. It says: take the value in each zip column and extract the first character on the left (1L) through the fifth character on the left (5L), and then use that five-digit zip to overwrite the zip column.

We'll use a different set of functions to standardize how we standardize the different flavors of the word "Arnold" in the city column.  Let's start by changing every value to title case -- first letter uppercase, subsequent letters lowercase -- using the `str_to_title()` function from `stringr`.  

```{r}
# cleaning function
cleaned_arnold_md_loans <- arnold_md_loans |>
  clean_names() |>
  rename(id = x1_id) |>
  mutate(amount = as.numeric(amount)) |>
  distinct() |>
  mutate(zip = str_sub(zip, start=1L, end=5L)) |>
  mutate(city = str_to_title(city))


# display the cleaned dataset
cleaned_arnold_md_loans

```

That was enough to standardize two values (ARNOLD and arnold).  The only ones that remain are the two clear misspellings (Arnld and Anold). To fix those, we're going to do some manual editing.  And for that, we're going to use `case_when()`, a function that let's us say if a value meets a certain condition, then change it, and if it doesn't, don't change it.   

```{r}
# cleaning function
cleaned_arnold_md_loans <- arnold_md_loans |>
  clean_names() |>
  rename(id = x1_id) |>
  mutate(amount = as.numeric(amount)) |>
  distinct() |>
  mutate(zip = str_sub(zip, start=1L, end=5L)) |>
  mutate(city = str_to_title(city)) |>
  mutate(city = case_when(
    city == "Anold" ~ "Arnold",
    TRUE ~ city
  ))


# display the cleaned dataset
cleaned_arnold_md_loans

```

This is a little complex, so let's break it down.  

What the code above says, in English, is this:  Look at all the values in the city column. If the value is "Anold", then (that's what the "~" means, then) replace it with the word "Arnold". If it's anything other than that (that's what "TRUE" means, otherwise), then keep the existing value in that column.

We could fix "Arnld" by adding another line inside that function, that looks identical:  ```city == "Arnld" ~ "Arnold"```. Like so.  

```{r}
# cleaning function
cleaned_arnold_md_loans_ <- arnold_md_loans |>
  clean_names() |>
  rename(id = x1_id) |>
  mutate(amount = as.numeric(amount)) |>
  distinct() |>
  mutate(zip = str_sub(zip, start=1L, end=5L)) |>
  mutate(city = str_to_title(city)) |>
  mutate(city = case_when(
    city == "Anold" ~ "Arnold",
    city == "Arnld" ~ "Arnold",
    TRUE ~ city
  ))


# display the cleaned dataset
cleaned_arnold_md_loans_

```

Instead of specifying the exact value, we can also solve the problem by using something more generalizable, using a function called str_detect(), which allows us to search parts of words.

The second line of our case_when() function below now says, in English: look in the city column. If you find that one of the values starts with "Arnl"  (the "^" symbol means "starts with"), then (the tilde ~ means then) change it to "Arnold".  

```{r}
# cleaning function
cleaned_arnold_md_loans <- arnold_md_loans |>
  clean_names() |>
  rename(id = x1_id) |>
  mutate(amount = as.numeric(amount)) |>
  distinct() |>
  mutate(zip = str_sub(zip, start=1L, end=5L)) |>
  mutate(city = str_to_title(city)) |>
  mutate(city = case_when(
    city == "Anold" ~ "Arnold",
    str_detect(city,"^Arnl") ~ "Arnold",
    TRUE ~ city
  ))


# display the cleaned dataset
cleaned_arnold_md_loans

```

That only changed one value "Arnld".  Imagine that there were other rows with values like "Arnlid","Arnl" or "Arnlod".  By using str_detect(city,"^Arnl"), we pick up any values that start with "Arnl", so it would change all four of these. If we used city == "Arnld", it would only pick up one.  

Lastly, there's the issue with inconsistent spelling of Ritchie Hwy in the street address column.  Do we need to clean this?

Remember the motivating question that's driving us to do this cleaning: What's the total amount of loans (at least in this tiny slice of data) for Arnold, MD in ZIP code 21012?

We don't need the street_address field to answer that question.  So we're not going to bother cleaning it.  

That's a good approach for the future. A good rule of thumb is that you should only spend time cleaning fields that are critical to the specific analysis you want to do.  

<!--chapter:end:10-janitor.Rmd-->

# Data Cleaning Part III: Open Refine

Gather 'round kids and let me tell you a tale about your author. In college, your author (Matt Waite) got involved in a project where he mapped crime in the city, looking specifically in the neighborhoods surrounding campus. This was in the mid 1990s. Computers were under powered. Tools were pretty primitive. I was given a database of nearly 50,000 calls for service.

And then I learned that addresses were not stored in a standard way. However the officer wrote it down, that's how it was recorded.

What did that mean?

It meant the Lincoln (Nebraska) Police Department came up with dozens of ways to say a single place. And since the mapping software needed the addressed to be in a specific form, I had to fix them. For example, I will go to my grave knowing that Lincoln High School's street address is 2229 J Street. Police officers wrote down LHS, L.H.S., Lincoln HS, Lincoln H.S., LHS (J Street), 2229 J, 2229 J ST, St., Street and on and on and on. That one was relatively easy. The local convenience store chain, with 8 locations around the city, was harder. I had to use the patrol district to locate them.

It took me four months to clean up more than 30,000 unique addresses and map them.

I tell you this because if I had Open Refine, it would have taken me a week, not four months.

Every time I talk about Open Refine, I remember this, and I get mad.

Fortunately (unfortunately?) several columns in the PPP loan data we're working with are flawed in exactly the same way. There are dozens of variations on just "Baltimore".    

We're going to explore two ways into Open Refine: Through R, and through Open Refine itself.

## Refinr, Open Refine in R

What is Open Refine?

Open Refine is a software program that has tools -- algorithms -- that find small differences in text and helps you fix them quickly. How Open Refine finds those small differences is through something called clustering. The algorithms behind clustering are not exclusive to Open Refine, so they can be used elsewhere.

Enter `refinr`, a package that contains the same clustering algorithms as Open Refine but all within R. Go ahead and install it if you haven't already by opening the console and running `install.packages("refinr")`. Then we can load libraries as we do.

```{r}
library(tidyverse)
library(refinr)
library(janitor)
```

Let's load our full Maryland PPP loans data.

```{r include=FALSE}

md_loans <- read_rds("data/ppp_loan_data/processed/md/ppp_loans_md.rds")

```

Now let's try and group and count the number of loans by city. To make it a bit more managable, let's use another string function from `stringr` and filter for cities that start with the uppercase "A" or lowercase "a" using the function `str_detect()` with a regular expression.  

The filter function in the codeblock below says: look in the city column, and pluck out any value that starts with (the "^" symbol means "starts with") a lowercase "a" OR (the vertical "|", called a pipe, means OR) an uppercase "A".  

```{r}
md_loans |>
  group_by(city) |>
  summarise(
    count=n()
  ) |>
  filter(str_detect(city, "^a|^A")) |>
  arrange(city)
```

There are lots of problems in this data that will prevent proper grouping and summarizing. We've learned several functions to do this manually.

By using the Open Refine package for R, `refinr`, our hope is that it can identify and standardize the data with a little more ease.

The first merging technique that's part of the `refinr` package we'll try is the `key_collision_merge`.

The key collision merge function takes each string and extracts the key parts of it. It then puts every key in a bin based on the keys matching.

One rule you should follow when using this is: **do not overwrite your original fields**. Always work on a copy. If you overwrite your original field, how will you know if it did the right thing? How can you compare it to your original data? To follow this, I'm going to mutate a new field called clean_city and put the results of key collision merge there.

```{r}
cleaned_md_loans <- md_loans |>
  mutate(city_clean=key_collision_merge(city)) |>
  select(id:city, city_clean, everything())

cleaned_md_loans

```
To examine changes `refinr` made, let's examine the changes it made to cities that start with the letter "A".

```{r}
cleaned_md_loans |>
  group_by(city_clean, city) |>
  summarise(
    count=n()
  ) |>
  filter(str_detect(city, "^a|^A")) |>
  arrange(city)
```
It got a bunch of things right.  It merged three variations of "Aberdeen" -- "aberdeen","Aberdeen" and "ABERDEEN" -- and it didn't merge it with "Aberdeen Proving Ground", which are  two distinct places. But it wasn't smart enough to convert "Abereen" to "Aberdeen".

It also merged "Annapolis" and "ANNAPOLIS" under "Annapolis", and was smart enough not to merge it with "Annapolis Junction", which is not the same city. But it wasn't smart enough to merge "Annapoils" or "Annalpolis".

There's another merging algorithim that's part of refinr that works a bit differently, called `n_gram_merge()`.  Let's try applying that one.

```{r}
cleaned_md_loans <- md_loans |>
  mutate(city_clean=n_gram_merge(city)) |>
  select(id:city, city_clean, everything())

cleaned_md_loans

```

To examine changes `refinr` made with this algorithm, let's again look at cities that start with the letter "A". Examining Aberdeen and Annapolis, we see there wasn't a substantial change from the previous method.

```{r}
cleaned_md_loans |>
  group_by(city_clean, city) |>
  summarise(
    count=n()
  ) |>
  filter(str_detect(city, "^a|^A")) |>
  arrange(city)
```
That's how you use the Open Refine r package, refinr.  

Now let's upload the data to the interactive version of OpenRefine, which really shines at this task.

## Manually cleaning data with Open Refine

Open Refine is free software. [You should download and install it](https://openrefine.org/). Refinr is great for quick things on smaller datasets that you can check to make sure it's not up to any mischief.

For bigger datasets, Open Refine is the way to go. And it has a lot more tools than refinr does (by design).

After you install it, run it. Open Refine works in the browser, and the app spins up a small web server visible only on your computer to interact with it. A browser will pop up automatically.

You first have to import your data into a project. Click the choose files button and upload a csv of the Maryland loans.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/open1.png"))
```

After your data is loaded into the app, you'll get a screen to look over what the data looks like. On the top right corner, you'll see a button to create the project. Click that.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/open2.png"))
```

Open Refine has many, many tools.  We're going to use one piece of it, as a tool for data cleaning. To learn how to use it, we're going to clean the "city" field.  

First, let's make a copy of the original city column so that we can preserve the original data while cleaning the new one.

Click the dropdown arrow next to the city column, choose "edit column" > Add column based on this column.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/open2a.png"))
```
On the window that pops up, type "city_original" in the "new column name" field. Then hit the OK button.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/open2b.png"))
```

Now, let's get to work cleaning the city column.  

Next to the city field name, click the down arrow, then facet, then text facet.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/open3.png"))
```

After that, a new box will appear on the left. It tells us how many unique cities there are: 1,977. And, there's a button on the right of the box that says Cluster.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/open4.png"))
```

Click the cluster button.  A new window will pop up, a tool to help us identify things that need to be cleaned, and quickly clean them.  

```{r, echo=FALSE}
knitr::include_graphics(rep("images/open5.png"))
```

The default "method" used is a clustering algorithim called "key collision", using the fingerprint function. This is the same method we used with the refinr package above.

At the top, you'll see which method was used, and how many clusters that algorithm identified. There are several different methods, each of which work slightly differently and produce different results.


```{r, echo=FALSE}
knitr::include_graphics(rep("images/open6.png"))
```

Then, below that, you can see what those clusters are. Right away, we can see how useful this program is.  It identified 9,903 rows that have some variation on "Silver Spring" in the city field: Silver Spring, SILVER SPRING, silver spring, Silver spring, silver Spring and so on.  It proposed changing them all to "Silver Spring".

Using human judgement, you can say if you agree with the cluster. If you do, click the "merge" checkbox. When it merges, the new result will be what it says in New Cell Value. Most often, that's the row with the most common result.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/open7.png"))
```

Now begins the fun part: You have to look at all 533 clusters found and decide if they are indeed valid. The key collision method is very good, and very conservative. You'll find that most of them are usually valid.

Be careful! If you merge two things that aren't supposed to be together, it will change your data in a way that could lead to inaccurate results.

When you're done, click Merge Selected and Re-Cluster.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/open8.png"))
```

If any new clusters come up, evaluate them. Repeat until either no clusters come up or the clusters that do come up are ones you reject.

Now. Try a new method, maybe the "nearest neighbor levenshtein" method. Notice that it finds even more variations of Silver Spring, using a slightly different approach.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/open9.png"))
```

Rinse and repeat.

You'll keep doing this, and if the dataset is reasonably clean, you'll find the end.

When you're finished cleaning, click "Merge Selected & Close".

```{r, echo=FALSE}
knitr::include_graphics(rep("images/open10.png"))
```

Then, export the data as a csv so you can load it back into R.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/open11.png"))
```

A question for all data analysts -- if the dataset is bad enough, can it ever be cleaned?

There's no good answer. You have to find it yourself.

<!--chapter:end:11-openrefine.Rmd-->

# Cleaning Data Part IV: PDFs

The next circle of Hell on the Dante's Inferno of Data Journalism is the PDF. Governments everywhere love the PDF and publish all kinds of records in a PDF. The problem is a PDF isn't a data format -- it's a middle finger, saying I've Got Your Accountability Right Here, Pal.

It's so ridiculous that there's a constellation of tools that do nothing more than try to harvest tables out of PDFs. There are online services like [CometDocs](https://www.cometdocs.com/) where you can upload your PDF and point and click your way into an Excel file. There are mobile device apps that take a picture of a table and convert it into a spreadsheet. But one of the best is a tool called [Tabula](https://tabula.technology/). It was build by journalists for journalists.

There is a version of Tabula that will run inside of R -- a library called Tabulizer -- but the truth is I'm having the hardest time installing it on my machine, which leads me to believe that trying to install it across a classroom of various machines would be disastrous. The standalone version works just fine, and it provides a useful way for you to see what's actually going on.

Unfortunately, harvesting tables from PDFs with Tabula is an exercise in getting your hopes up, only to have them dashed. We'll start with an example.

## Easy does it

Tabula works best when tables in PDFs are clearly defined and have nicely-formatted information. Here's a perfect example: [active voters by county in Maryland](https://elections.maryland.gov/press_room/2020_stats/Eligible%20Active%20Voters%20by%20County%20-%20PG20.pdf).

[Download and install Tabula](https://tabula.technology/). Tabula works much the same way as Open Refine does -- it works in the browser by spinning up a small webserver in your computer.

When Tabula opens, you click browse to find the PDF on your computer somewhere, and then click import. After it imports, click autodetect tables. You'll see red boxes appear around what Tabula believes are the tables. You'll see it does a pretty good job at this.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/md_voters.png"))
```

Now you can hit the green "Preview & Export Extracted Data" button on the top right. You should see something very like this:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/md_voters2.png"))
```

You can now export that extracted table to a CSV file using the "Export" button. And then we can read it into R:

```{r}
voters_by_county <- read_csv("data/tabula-Eligible Active Voters by County - PG20.csv")

voters_by_county
```

Boom - we're good to go.

## When it looks good, but needs a little fixing

Here's a slightly more involved PDF. Check out the table on page 4 of [this SBA PDF](https://www.sba.gov/sites/default/files/2021-06/PPP_Report_Public_210531-508.pdf).

```{r, echo=FALSE}
knitr::include_graphics(rep("images/ppp_1.png"))
```

Looks like a spreadsheet, right? Save that PDF file to your computer in a place where you'll remember it (like a Downloads folder).

Now let's repeat the steps we did to import the PDF into Tabula and autodetect the tables. Page 4 should look like this:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/ppp_2.png"))
```

We just want the table on page 4, which shows 2021 loan activity by type of lender, so hit the "Clear All Selections" button to remove the red boxes. Now, in Tabula, let's draw a box around the table on page 4. Click and drag to draw the box.

Now you can hit the green "Preview & Export Extracted Data" button on the top right. You should see something very like this:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/ppp_3.png"))
```

You can now export that extracted table to a CSV file using the "Export" button. And then we can read it into R:

```{r}
lender_types <- read_csv("data/tabula-PPP_Report_Public_210531-508.csv")

lender_types
```

## Cleaning up the data in R

The good news is that we have data we don't have to retype. The bad news is, it's hardly in importable shape.

See the "Net Dollars" column? Thanks to the dollar signs, R doesn't recognize those values as numbers. The column names seem ok, but having spaces in them is a pain. Let's fix that by re-importing it and calling `mutate` so that the new `net_dollars` column is numeric.

```{r}
lender_types <- read_csv("data/tabula-PPP_Report_Public_210531-508.csv", skip=1, col_names=c("type", "count", "approved", "net_dollars"))
lender_types <- lender_types |> mutate(net_dollars=as.numeric(parse_number(net_dollars)))
lender_types
```

All things considered, that was pretty easy. Many - most? - electronic PDFs aren't so easy to parse. Sometimes you'll need to open the exported CSV file and clean things up before importing into R. Other times you'll be able to do that cleaning in R itself.

Here's the sad truth: THIS IS PRETTY GOOD. It sure beats typing it out. And since many government processes don't change all that much, you can save the code to process subsequent versions of PDFs.

<!--chapter:end:12-pdfs.Rmd-->

# Combining and joining

Often, as data journalists, we're looking at data across time or at data stored in multiple tables. And to do that, we need to often need to merge that data together.

Depending on what we have, we may just need to stack data on top of each other to make new data. If we have 2019 data and 2018 data and we want that to be one file, we stack them. If we have a dataset of cows in counties and a dataset of populations in county, we're going to join those two together on the county -- the common element.  

Let's explore.

## Combining data (stacking)

Let's say that we have county population estimates for three different years - [2010](https://umd.box.com/s/vsuyt7v1gtb2u0aerliv5eixfmzfgt17), [2015](https://umd.box.com/s/2qijnwfygsrfin9d2o1krhzw5o8vza30) and [2020](https://umd.box.com/s/uqkeaabkvs4ge0l10j3w3sa76q3z1v16) - in three different files. They have the same record layout and the same number of counties. We can combine them into a single dataframe.

Let's do what we need to import them properly. I've merged it all into one step for each of the three datasets.

```{r}
library(tidyverse)
```

```{r}
popestimate_2010 <- read_csv("data/popestimate_2010.csv")
```

```{r}
popestimate_2015 <- read_csv("data/popestimate_2015.csv")
```

```{r}
popestimate_2020 <- read_csv("data/popestimate_2020.csv")
```

All three of these datasets have the same number of columns, all with the same names, so if we want to merge them together to compare them over time, we need to stack them together. The verb here, in R, is `bind_rows`. You tell the function what you want to combine and it does it, assuming that you've got column names in common containing identically formatted data.

Since we have three dataframes, we're going to need to pass them as a list, meaning they'll be enclosed inside the `list` function.

```{r}
estimates <- bind_rows(list(popestimate_2010, popestimate_2015, popestimate_2020))
```

And boom, like that, we have 9,852 rows of data together instead of three dataframes. There are plenty of uses for `bind_rows`: any regularly updated data that comes in the same format like crime reports or award recipients or player game statistics.

## Joining data

More difficult is when you have two separate tables that are connected by a common element or elements.

Let's start by reading in the Maryland PPP loan applications data:

```{r}
maryland_ppp <- read_csv('data/ppp_applications_md.csv')
```

One of the columns we have is called `naics_code`, which is a six-digit number used by the federal government "in classifying business establishments for the purpose of collecting, analyzing, and publishing statistical data related to the U.S. business economy." More details at [the official NAICS site](https://www.census.gov/naics/).

But unless you have a particular interest in memorizing what those more than 1,000 codes actually mean, the codes themselves don't help you understand what type of business any individual applicant has. Luckily, we can merge that information into our data using a join.

Using a [CSV file of the 2017 NAICS codes](https://umd.box.com/s/thou3merdzwzbdimbvd06zkax1zndcsk), let's read the file into R:

```{r}
naics_codes <- read_csv('data/naics_codes.csv')
```

To put the Maryland applications and NAICS codes together, we need to use something called a join. There are different kinds of joins. It's better if you think of two tables sitting next to each other. A `left_join` takes all the records from the left table and only the records that match in the right one. A `right_join` does the same thing. An `inner_join` takes only the records where they are equal. There's one other join -- a `full_join` which returns all rows of both, regardless of if there's a match -- but I've never once had a use for a full join.

In the best-case scenario, the two tables we want to join share a common column. In this case, both of our tables have a column called `naics_code` that has the same characteristics: both are six-digit numbers.

We can do this join multiple ways and get a similar result. We can put the Maryland file on the left and the NAICS codes on the right and use a left join to get them all together. And we use `by=` to join by the correct column. And to avoid rendering hundreds of rows of data, I'm going to count the rows at the end. The reason I'm going this is important: **Rule 1 in joining data is having an idea of what you are expecting to get**. So with a left join with applications on the left, I have 195,869 applications, so I expect to get 195,869 rows when I'm done.

```{r}
maryland_ppp |> left_join(naics_codes, by="naics_code") |> select(name, naics_code, title) |> nrow()
```

Remove the nrow and run it again for yourself. By default, `dplyr` will do a "natural" join, where it'll match all the matching columns in both tables. So if we take out the by, it'll use all the common columns between the tables. That may not be right in every instance but let's try it. If it works, we should get 195,869 rows.

```{r}
maryland_ppp |> left_join(naics_codes) |> select(name, naics_code, title)
```

Since we only have one column in common between the two tables, the join only used that column. And we got the same answer. If we had more columns in common, you could see in your results columns with .X after them - that's a sign of duplicative columns between two tables, and you may decide you don't need both moving forward.

Let's save our joined data to a new dataframe, but this time let's remove the select function so we don't limit the columns to just three.

```{r}
maryland_ppp_with_naics <- maryland_ppp |> left_join(naics_codes)
```

Now, with our joined data, we can answer questions in a more useful way. But joins can do even more than just provide lookups; they can bring in additional data to enable you to ask more sophisticated questions.

Let's try adding zip code demographic data to the mix. Using [a file from the state's data catalog](https://umd.box.com/s/2xsq2rpkmg4ct3a77vt8j5bu4vu1z0tf), we can read it into R:

```{r}
maryland_zcta <- read_csv('data/maryland_zcta.csv')
```

You'll want to keep open the [data documentation](https://geodata.md.gov/imap/rest/services/Demographics/MD_CensusData/FeatureServer/1) that hosts the data, because the field names are abbreviations.

Note that this file describes Zip Code Tabulation Areas, which are very similar to but not always identical to zip codes (there's a [good explanation from the Census Bureau](https://www.census.gov/programs-surveys/geography/guidance/geo-areas/zctas.html)). Bottom line: "In most instances the ZCTA code is the same as the ZIP Code for an area."

Again, we can use a `left_join` to make our demographic data available. This time we'll need to specify the two fields to join because they do not have identical names. We'll use `zip` from our PPP data and `ZCTA5N` from the demographic data:

```{r, error=TRUE}
maryland_ppp_with_naics_and_demographics <- maryland_ppp_with_naics |> left_join(maryland_zcta, by=c("zip"="ZCTA5N"))
```

You probably got an error when running that:


Error: Can't join on `x$zip` x `y$zip` because of incompatible types.
ℹ `x$zip` is of type <character>>.
ℹ `y$zip` is of type <double>>

Let's unpack this: R is saying it can't join those two columns because one of them is a character column and the other is a double (number) column. When joining, _the two columns need to have the same data type_.

We can fix that using our pal `mutate`. Let's change the ZCTA data to be a character, since zip codes aren't really useful as numbers (nobody needs to know the average zip code value).

```{r}
maryland_zcta <- maryland_zcta |> mutate(across(ZCTA5N, as.character))
```

Now we can re-run the join:

```{r}
maryland_ppp_with_naics_and_demographics <- maryland_ppp_with_naics |> left_join(maryland_zcta, by=c("zip"="ZCTA5N"))

maryland_ppp_with_naics_and_demographics
```

Now, if you use the tiny black right arrows to see what's in those demographic columns, you'll see ... a lot of NAs. What's going on there? Are there zip codes in the PPP data that aren't in the ZCTA data? If you go back using the left arrows, you can see that the PPP zip codes often are zip+4, and we're using the ZCTA5N field to join, which is a 5-character field. But not all of the PPP codes are zip+4. So we'll need to use `mutate` once more to give us a PPP zip field that is exactly 5 characters. Let's do this on the original `maryland` dataframe so we can then do the join using our new `zip5` field:

```{r}
maryland_ppp_with_naics <- maryland_ppp_with_naics |> mutate(zip5 = str_sub(zip, 1, 5))
maryland_ppp_with_naics_and_demographics <- maryland_ppp_with_naics |> left_join(maryland_zcta, by=c("zip5"="ZCTA5N"))
```

Now we've got PPP data and demographic data by zip code. That means we can draw from both datasets in asking our questions. For example, we could see the mean and median loan amount in zip codes with different demographic characteristics. Let's start with zip codes that have more than 50 percent non-Hispanic Black population.

We get this by using filter followed by summarize. In this case, we want PNHB > 50:

```{r}
maryland_ppp_with_naics_and_demographics |>
  filter(PNHB > 50) |>
  summarize(
    count = n(),
    avgamount = mean(amount),
    medamount = median(amount))
```

According to our query, there were 44,032 loans approved in zip codes with more than 50 percent non-Hispanic Black population, and the average amount was $53,836 and the median amount was $20,000.

Let's change that to zip codes with more than 50 percent non-Hispanic white population:

```{r}
maryland_ppp_with_naics_and_demographics |>
  filter(PNHW > 50) |>
  summarize(
    count = n(),
    avgamount = mean(amount),
    medamount = median(amount))
```

And we get a greater number of loans with a higher average and median amount. But that's not the end of the story, or of our questions. We'll need to ask more.

Let's break this down one more step. What if we added `rural_urban_indicator` -- if the loan recipient is located in a rural or urban area -- as a group_by. Does that change anything for the previous queries?

```{r}
maryland_ppp_with_naics_and_demographics |>
  filter(PNHB > 50) |>
  group_by(rural_urban_indicator) |>
  summarize(
    count = n(),
    avgamount = mean(amount),
    medamount = median(amount))
```

```{r}
maryland_ppp_with_naics_and_demographics |>
  filter(PNHW > 50) |>
  group_by(rural_urban_indicator) |>
  summarize(
    count = n(),
    avgamount = mean(amount),
    medamount = median(amount))
```

In both cases, urban applicants got more approvals for higher average and median amounts, and the gap between majority-white zip codes and majority-Black zip codes was larger for urban applicants than for rural ones.

Joining datasets allows you to expand the range and sophistication of questions you're able to ask. It is one of the most powerful tools in a journalist's toolkit.

<!--chapter:end:13-merging.Rmd-->

# Scraping data with Rvest

Sometimes, governments put data online on a page or in a searchable database. And when you ask them for a copy of the data underneath the website, they say no.

Why? Because they have a website. That's it. That's their reason. They say they  don't have to give you the data because they've already given you the data, never mind that they haven't given to you in a form you can actually load into R with ease.

Lucky for us, there's a way for us to write code to get data even when an agency hasn't made it easy: webscraping.

One of the most powerful tools you can learn as a data journalist is how to scrape data from the web. Scraping is the process of programming a computer to act like a human that opens a web browser, goes to a website, ingests the HTML from that website into R and turns it into data.

The degree of difficulty here goes from "Easy" to "So Hard You Want To Throw Your Laptop Out A Window." And the curve between the two can be steep. You can learn how to scrape "Easy" in a day. The hard ones take a little more time, but it's often well worth the effort because it lets you get stories you couldn't get without it.  

In this chapter, we'll show you an easy one.  And in the next chapter, we'll so you a moderately harder one.

Let's start easy.

We're going to use a library called `rvest`, which you can install it the same way we've done all installs: go to the console and `install.packages("rvest")`.

Like so many R package names, rvest is a bad pun.  You're supposed to read it to sound like "harvest", as in "harvesting" information from a website the same way you'd harvest crops in a field.   

We'll load these packages first:

```{r}
library(rvest)
library(tidyverse)
library(janitor)
```

For this example, we're going to work on loading a simple table of data from the Bureau of Labor Statistics. This is a table of industry sectors (each with a two-digit NAICS code) that we could make use of in our analysis of PPP loan data.

Recall that our PPP loan data has six-digit NAICS codes for each industry, which allows us to identify the industry for each loan.  For example 212221 is the code for "Gold Mining Industry".

A six-digit NAICS code is the most specific.  As we remove numbers from the right to create five-digit, four-digit, three-digit and two-digit codes, the industries they represent get broader. Here's an example:

* 212221 - Gold Mining Industry
* 2122 - Metal Ore Mining Industry (which includes gold mining and things like silver mining, iron mining and copper mining)
* 21 - Mining, Quarrying, and Oil and Gas Extraction Industry (which contains the metal mining industries mentioned above, but also oil drilling, coal mining and more).

It might be useful to have a lookup table of those top-level, two-digit NAICS codes (also called sector codes) for our analysis, to help us answer questions about what specific top-level industries got loans.  L

Let's suppose we can't find a table like that for download, but we do see a version on the BLS website at this URL: [https://www.bls.gov/ces/naics/](https://www.bls.gov/ces/naics/).

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest1.png"))
```
We could get this table into R with the following manual steps: highlighting the text, copying it into Excel, saving it as a csv, and reading it into R.  

Or, we could write a few lines of webscraping code to have R do that for us!

In this simple example, it's probably faster to do it manually than have R do it for us. And this table is unlikely to change much in the future.

Why would we ever write code to grab a single table? There's several reasons:

1. Our methods are transparent.  If a colleague wants to run our code from scratch to factcheck our work, they don't need to repeat the manual steps, which are harder to document than writing code.
2. Let's suppose we wanted to grab the same table every day, to monitor for changes (like, say, a table on a health department website that has COVID case numbers that update every day).  Writing a script once, and pressing a single button every day is going to be much more efficient than doing this manually every day.
3. If we're doing it manually, we're more likely to make a mistake, like maybe failing to copy every row from the whole table.
4. It's good practice to prepare us to do more complex scraping jobs.  As we'll see in the next chapter, if we ever want to grab the same table from hundreds of pages, writing code is much faster and easier than going to a hundred different pages ourselves and downloading data.

So, to scrape, the first thing we need to do is start with the URL. Let's store it as an object called naics_url.

```{r}
naics_url <- "https://www.bls.gov/ces/naics/"
```

When we go to the web page, we can see a nicely-designed page that contains our information.  

But what we really care about, for our purposes, is the html code that creates that page.  

In our web browser, if we right-click anywhere on the page and select "view source" from the popup menu, we can see the source code.  Or you can just copy this into Google Chrome: view-source:https://www.bls.gov/ces/naics/.

Here's a picture of what some of the source code looks like.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest2.png"))
```
We'll use those HTML tags -- things like `<div>` and `<a>` and `<table>` -- to grab the info we need.

Okay, step 1.  

Let's write a bit of code to tell R to go to the URL for the page and ingest all of that HTML code. In the code below, we're starting with our URL and using the read_html() function from rvest to ingest all of the page html, storing it as an object called naics_industry.

```{r}
# read in the html
naics_industry <- naics_url |>
  read_html()

# display the html below
naics_industry

```

If you're running this code in R Studio, in our environment window at right, you'll see naics_industry as a "list of 2".  

This is not a dataframe, it's a different type of data structure a "nested list."

If we click on the name "naics_industry" in our environment window, we can see that it's pulled in the html and shown us the general page structure. Nested within the `<html>` tag is the `<head>` and `<body>`, the two fundamental sections of most web pages. We're going to pull information out of the `<body>` tag in a bit.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest3.png"))
```

Now, our task is to just pull out the section of the html that contains the information we need.  

But which part do we need from that mess of html code? To figure that out, we can go back to the page in a web browser like chrome, and use built in developer tools to "inspect" the html code underlying the page.  

On the page, find the data we want to grab -- "Table 2. NAICS Sectors" - and right click on the word "Sector" in the column header of the table.  That will bring up a dropdown menu. Select "Inspect", which will pop up a window called the "element inspector" that shows us where different elements on the page are located, what html tags created those elements, and other info.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest4.png"))
```

The entire table that we want of naics sectors is actually contained inside an html `<table>`. It has a header row `<thead>` that contains the column names and a `<tbody>` that contains one row `<tr>` per industry sector code.

Because it's inside of a table, and not some other kind of element (like a `<div>`), rvest has a special function for easily extracting and converting html tables, called html_table(). This function extracts all six html tables on the page, only one of which we actually want

```{r}
# read in the html and extract all the tables
naics_industry <- naics_url |>
  read_html() |>
  html_table()

# display the tables below
naics_industry

```

In the environment window at right, look at naics_industry.  Note that it's now a "list of 6".  

Click on it to open it up.  It should look like this.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest5.png"))
```
This gets a little complicated, but what you're seeing here is a nested list that contains six different data frames -- also called tibbles --  one for each table that exists on the web page we scraped.

They're numbered 1 to 6.  The first 1 has 4 rows and 3 columns, the second has 21 rows and 2 columns, and so on.   

To examine what's in each dataframe, mouse over the right edge (next to the word columns) on each row, and click the little scroll icon.  The icon will be hidden until you mouse over it.  

Click on the scroll icon for the first dataframe examine it.  

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest6.png"))
```

That's not the one we want!  

Let's try clicking on the scroll icon for row 2.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/rvest7.png"))
```

That's more like it! So, all we need to do now is to store that single dataframe as an object, and get rid of the rest.  We can do that with this code, which says "keep only the second dataframe from our nested list. If we wanted to keep the third one, we'd change the number 2 to number 3.

```{r}
# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url |>
  read_html() |>
  html_table()

# Just keep the second dataframe in our list

naics_industry <- naics_industry[[2]]

# show the dataframe

naics_industry

```

We now have a proper dataframe.

From here, we can do a little light cleaning. Let's use clean_names() to standardize the column names.  Then let's use slice() to remove the last row -- row number 21 -- which contains source information that will complicate our use of this table later.

```{r}
# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry <- naics_url |>
  read_html() |>
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row

naics_industry <- naics_industry[[2]] |>
  clean_names() |>
  slice(-21)

# show the dataframe
naics_industry

```

And there we go. We now have a nice tidy dataframe of NAICS sector codes.  

In the next chapter, we'll look at a more complicated example.

<!--chapter:end:14-rvest.Rmd-->

---
output:
  html_document: default
  pdf_document: default
---
# Advanced rvest

In the last chapter, we demonstrated a fairly straightforward example of web scraping to grab a list of NAICS industry sector codes from the BLS website.  

We're going to graduate to a more challenging example, one that will help us gather information about the number of employees in each industry sector.

What makes this more challenging?  Well, the information we need is all contained on multiple pages, one page per sector. We need to write code to visit each page, and then merge them into a single data frame.

This is challenging stuff, so don't feel dissuaded if it all doesn't click the first time through.  Like many things, web scraping is something that gets easier with lots of practice.

First we start with libraries, as we always do.

```{r}
library(tidyverse)
library(rvest)
library(janitor)
```

Now, let's run the code we wrote in the last chapter, to get a tidy list of NAICS sector codes and names from [https://www.bls.gov/ces/naics/]("https://www.bls.gov/ces/naics/").


```{r}
# Define url of page we want to scrape

naics_url <- "https://www.bls.gov/ces/naics/"

# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url |>
  read_html() |>
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row

naics_industry <- naics_industry[[2]] |>
  clean_names() |>
  slice(-21)

# show the dataframe
naics_industry

```

We'll use this table to help us get to our end goal: a single dataframe with the number of employees in each industry sector.

It will look like this when we're done.  

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest1a.png"))
```

Unfortunately, that information doesn't exist in a single tidy table on a single page we can scrape all at once.  We're going to have to scrape it from lots of different pages, and build it ourselves.

Let's next take a look at the web page that has detailed employment information for one of our sectors, 22, Mining, Quarrying, and Oil and Gas Extraction.  

We can find it here: [https://www.bls.gov/iag/tgs/iag22.htm](https://www.bls.gov/iag/tgs/iag22.htm).

A few scrolls down the page, there's a table that has employee statistics.  

The table is called "Employment and Unemployment". There's a row in the tabled for "Employment, all employees (seasonally adjusted)".  And in that row, there's a value for the number of employees -- in thousands -- in June 2021.  

The table shows that for the mining sector, it was 538.6  -- or 538,600 -- in June 2021.  That's the value we want to ingest in R.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest1.png"))
```


We don't just want it for mining.  We want it for all sectors!  

But we'll start by writing code just to get it from this one sector page, then modify that code to get it from every sector's page

First, let's define the URL of the page we want to get the information from.

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"

```

Next, let's read in the html of that page, and store it as an object called employment_info.

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"

# Get employment html
employment_info <- url |>
  read_html()  

# Display it so we can see what it looks like
employment_info

```

Now, let's set to picking out the information we need from the raw html.

We can use the web inspector in our web browser (Chrome) to figure out where the table is located.

Go to the web page and right click on the word "Data Series" in the table, then pick "inspect" to pull up the menu.

Notice two things.  First, all of this information is contained in a proper html `<table>`.  And that table has an id property of "iag22emp1". Designers use these IDs to help style the page, to target certain elements with CSS. We can use it to scrape.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest2.png"))
```
Recall that in the last chapter, when we used the html_table() function, it pulled in every single table on the page, six in total.  

Here, we can use that id property to pick out just the table we want, and leave all the others behind.
We do that with a new function from rvest called html_element(), employing a bit of information about that element stored in what's called the  [xpath](https://en.wikipedia.org/wiki/XPath). Xpath is a query language that helps us write programs that target specific parts of web pages.  

The syntax is a little unwieldy, I know.

But essentially what the html_element function says is "find the html element that has an id of iag22emp1, using the xpath method, and get rid of all other elements".

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"

# Get employment html page and select only the table with employment information
employment_info <- url |>
  read_html() |>
  html_element(xpath = '//*[@id="iag22emp1"]')

# Display it so we can see what it looks like
employment_info
```

We've now isolated the table on the page that contains the information we need, and gotten rid of everything else.

From here, we can use the html_tables() function to transform it from messy html code to a proper dataframe.

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag22.htm"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url |>
  read_html() |>
  html_element(xpath = '//*[@id="iag22emp1"]') |>
  html_table()

# Display it so we can see what it looks like
employment_info
```

Now we have a proper dataframe of 6 rows and 6 columns.  

It has much more information than we need, so let's clean it up to isolate only the "Employment, all employees (seasonally adjusted)" value for June 2021.

Use clean_names() to standardize the column names, use slice() to keep only the second row, and use select() to keep two columns data_series and jul_2021.

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag21.htm"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url |>
  read_html() |>
  html_element(xpath = '//*[@id="iag21emp1"]') |>
  html_table()

# Keep only second row with seasonally adjusted, bind back to each_row_df
employment_info <- employment_info |>
  clean_names() |>
  slice(2) |>
  select(data_series, jul_2021)

# Display it so we can see what it looks like
employment_info
```

Okay, so we've successfully obtained the employment numbers for one of our sectors. That's great.

But remember our original charge: to get a table with employment numbers for ALL sectors, not just one.

This is a little tricky, because, remember, the information for each sector is on a different page!

The info for mining is on this page: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm).

The info for construction is on this page: [https://www.bls.gov/iag/tgs/iag23.htm](https://www.bls.gov/iag/tgs/iag23.htm).

We have 20 sectors to get through.  

We could get the info we need by copying the codeblock we just wrote 20 times, and change the url at the top each time.  

But that's not a great approach.  

What if we needed to change the code? We'd need to change it 20 times!

In programming, there's a principle called "DRY" which stands for "Don't Repeat Yourself".  If you find yourself copying the same code over and over again, with minor changes, it's better to find a way to avoid that.

## Using for loops

Fortunately, there's a programming paradigm called "iteration" that is helpful here, using a method called a "for loop".

Every programming language has its own version of a "for loop", and R is no different.

A "for loop" says: "let's take a list of things, and do the same thing to each item on that list."   

Let's look at a very simple example to help illustrate the values of for loops.

We're going to write code to print out 10 industry sectors.  

First, let's do it the repetitive way.  We're writing the same print function over and over, just changing the sector name each time.
```{r}

print("Agriculture, Forestry, Fishing and Hunting")
print("Mining, Quarrying, and Oil and Gas Extraction")
print("Utilities")
print("Construction")
print("Manufacturing")
print("Wholesale Trade")
print("Retail Trade")
print("Transportation and Warehousing")
print("Information")
print("Finance and Insurance")

```

We repeated print() 10 times, with minor modifications each time.  Lots of repetition, which we seek to avoid if possible.

Now let's look at how we might do that a little more efficiently with a "for loop."

First let's make a list of sectors, and save it as an object called "list_of_sectors." The c() function tells R that we're making a list.

```{r}
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

```

And now let's write a "for loop" to print out sector on that list.

```{r}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# Make a for loop and run it
for (sector in list_of_sectors) {
  print(sector)
}


```

That's many fewer lines of code.  Let's break down what we just saw, starting with for `(sector in list_of sectors)`.

The information inside the parentheses tells R what list to use -- list_of_sectors -- and how to identify list elements later on -- sector.

It's important that the thing on the right side of "in" use the exact name of the list we want to loop through -- in this case "list_of_sectors".

If we try to feed it something different -- say "sector_list" -- it won't work, because our actual list is called something else -- "list_of_sectors". This code throws an error.

```{r, error=TRUE}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# For loop that refers to a list that doesn't exist!  
for (sector in sector_list) {
  print(sector)
}


```

The name on the left side of "in" -- the word we're assigning to represent each element -- is totally arbitrary.  

We could use any character string, even something simple like "x".  

What matters is that we use the same character string inside of the curly braces {}, the section of the "for loop" that tells R what to do to each element -- in this case, print it out.    

To illustrate this, note that the code works just fine if we change it to say this:

```{r}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# For loop with x that stands in for each element in our list, instead of sector
for (x in list_of_sectors) {
  print(x)
}


```

But it does NOT work if we call each element one thing -- x -- in the first line of our "for loop", and use a different name to refer to it inside of the curly braces.

In this code below, it has no idea what we mean by "sector_name", because we haven't defined that anywhere.  

```{r, error=TRUE}
# Define list of sectors
list_of_sectors <- c("Agriculture, Forestry, Fishing and Hunting", "Mining, Quarrying, and Oil and Gas Extraction", "Utilities", "Construction", "Manufacturing",
"Wholesale Trade", "Retail Trade", "Transportation and Warehousing", "Information", "Finance and Insurance")

# For loop that includes instructions that refer to a variable that doesn't exist.
for (x in list_of_sectors) {
  print(sector_name)
}


```

We can also write for loops to iterate over a range of numbers, instead of a list of characters.  The syntax is a little different.

The code below says: "for each number in a range of numbers from 1 to 10, print the number."  

```{r}
# For loop that includes instructions that refer to a variable that doesn't exist.
for (number in 1:10) {
  print(number)
}
```

Here's a minor variation on that approach that we'll make use of below.  

Instead of giving the for loop an explicit number range, like 1:10, we can tell it to use 1 to "the number of rows in a dataframe" as our list of things to loop through.

Remember the naics_industry dataframe we loaded first? It has 20 rows.

```{r}

naics_industry

```

We can use that information in our for loop by using the nrow() function, which calculates the number of rows in a dataframe.  Here's a quick demonstration of how that works.

```{r}
nrow(naics_industry)
```

To put it all together, the code below says "make a list of numbers that starts at 1 and ends at the number of rows in the naics_industry dataframe (which is 20), then print out each of these numbers."

```{r}
# For loop that includes instructions that refer to a variable that doesn't exist.
for (row_number in 1:nrow(naics_industry)) {
  print(row_number)
}
```

These were basic examples of how "for loops" work.  Next, we'll learn to apply "for loops" to efficentily extract information from multiple web pages.

## Looping and rvest

First, let's look at the codeblock we wrote earlier to extract the number of employees in the mining sector.

```{r}

# Define url of the page we want to get
url <- "https://www.bls.gov/iag/tgs/iag21.htm"

# Get employment html page and select only the table with employment information, then transform it from html to a table.
employment_info <- url |>
  read_html() |>
  html_element(xpath = '//*[@id="iag21emp1"]') |>
  html_table()

# Keep only second row with seasonally adjusted, bind back to each_row_df
employment_info <- employment_info |>
  clean_names() |>
  slice(2) |>
  select(data_series, jul_2021)

# Display it so we can see what it looks like
employment_info
```
This contains all the steps we needed to extract the information from one sector page. We're now going to modify this function so we can use it to extract information from each sector page, writing code that keeps us from repeating ourselves too much.  
First, we need to build a list of URLs to loop through in a "for loop." We can do that using the dataframe we made in the last chapter.

```{r}
# Define url of page we want to scrape

naics_url <- "https://www.bls.gov/ces/naics/"

# Read in all html from table, store all tables on page as nested list of dataframes.
naics_industry  <- naics_url |>
  read_html() |>
  html_table()

# Just keep the second dataframe in our list, standardize column headers, remove last row

naics_industry <- naics_industry[[2]] |>
  clean_names() |>
  slice(-21)

# show the dataframe
naics_industry

```

This gives us the sector code and name for each industry.

Now let's have a look at the URLs for a few of the pages we want to grab data from.

* Mining, Quarrying, and Oil and Gas Extraction: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm).
* Utilities: [https://www.bls.gov/iag/tgs/iag22.htm](https://www.bls.gov/iag/tgs/iag22.htm).
* Construction: [https://www.bls.gov/iag/tgs/iag23.htm](https://www.bls.gov/iag/tgs/iag23.htm)

Notice a pattern?

They all start with "https://www.bls.gov/iag/tgs/iag".  The next bit of information is different for each one; with the two-digit sector code for each sector.  The remainder is identical in all three links, ".htm".

Because they're all the same, we can use the information in the dataframe we just loaded to make all the URLs we need.

We're going to use mutate() and paste0() to concatenate (mash together) the things that stay constant in every url (the beginning and end) with the things that are different (the sector number, stored in the column called sector).

```{r}

# Make a column with URL for each sector.
naics_industry <- naics_industry |>
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm"))

# Display it
naics_industry
```

While we're at it, we're going to use the same method to programatically build the "xpath" for the table on each sector page.  

Recall that when we wrote our function that got information from just the mining page, the xpath targeted an element with an ID of "iag21emp1".  Why 21? That's the sector code for mining.  

If we look for that exact element ID on other sector pages, we won't find it! That's because it's different for each page.

On the Utilities page (sector code 22), the ID for the table we want is "iag22emp1".  On the Construction page (sector code 23), it's "iag23emp1". We can also build this programatically, because it follows a predictable pattern.

```{r}

# Make a column with URL and xpath ID for each sector
naics_industry <- naics_industry |>
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm")) |>
  mutate(sector_xpath_id =paste0("iag",sector,"emp1"))

# Display it
naics_industry
```

Lastly, we're going to use filter to remove the "Public Administration" sector, because there's no page for it. We'll have to get that information some other way.

```{r}

# Make a column with URL and xpath ID for each sector, remove the Public Administration sector
naics_industry <- naics_industry |>
  mutate(sector_url = paste0("https://www.bls.gov/iag/tgs/iag",sector,".htm")) |>
  mutate(sector_xpath_id =paste0("iag",sector,"emp1")) |>
  filter(description != "Public Administration")
# Display it
naics_industry
```

We're left with a dataframe of 19 rows and 4 columns. It now contains everything we need.

Next, we'll construct a "for loop" to extract the info we need from each page. We're going to build it up step-by-step, beginning with the the basic elements of our "for loop".

The codeblock below says: "Make a list with the row numbers from 1 to the number of rows in our naics_industry dataframe (which is 19). Then, for each element of that list (1, 2, 3, 4, 5 and so on up to 19), use slice() to keep only the one row that matches that number and save this newly created dataframe as each_row_df. Print out the dataframe. Then go to the next element on the list and do the same thing.  Keep doing that until we hit number 19, then stop."   

We get 19 dataframes, each with one row, one for each sector.

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry |>
      slice(row_number)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(each_row_df)

}
```

We're almost to the part where we can go out and fetch the html we need. Before we do that, let's store as part of our loop an object called "url", which contains the URL of the page for each sector.

The syntax with the dollar sign is a little funky, but "each_row_df$sector_url" says "from the each_row_df dataframe, grab the information in the sector_url column." Because the column has only one row, there's one value.

We're going to do something simliar with the xpath for our employment table by using the information in the sector_xpath_id column.

That code also looks a little unwieldly.  Recall that the xpath for the mining industry was `'//*[@id="iag22emp1"]'`.  

In the code below, we're building the xpath dynamically by pasting together the parts that stay the same for each xpath -- `'//*[@id="'` and `'"]'` -- and the parts that change for each sector, pulled from the xpath_sector_id column.

To see how this is working, we're going to edit our print statement at the end a bit, printing the row_number and the dynamically created url and xpath.
```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry |>
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(paste0("ROW NUMBER:", row_number," URL: ",url," XPATH:",xpath_employment_table))

}
```

Armed with the URL and xpath for each sector web page, we can now go out and get the employment table for each sector.

We'll read in the html from the url we just stored; extract the table that has the xpath ID we just created; and then transform the html table code into a proper dataframe.

The dataframe is hidden inside  a nested list, which we'll have to extract in the next step.

So, when you run this code, it will print out 19 dataframes inside of nested lists, each containing one dataframe.

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry |>
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table(). The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url |>
      read_html() |>
      html_elements(xpath = xpath_employment_table) |>
      html_table()

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}
```
In this next step, we use employment_info <- employment_info[[1]]  to extract each dataframe from the nested list. Then we'll tidy up the dataframe a bit. We'll use the get rid of all the information we don't need in the table, by using slice() to keep only the second row. We'll also standardize the column names with clean_names().

```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry |>
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url |>
      read_html() |>
      html_elements(xpath = xpath_employment_table) |>
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row;
    employment_info <- employment_info[[1]] |>
      clean_names() |>
      slice(2)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}
```
We now have 19 dataframes, each containing one row each and two columns, one of which is the employment number for a given sector for jul_2021. But we're missing information about what industry sector these employment numbers represent.

We can add that back in by using bind_cols() to reconnect the each_row_df, which contains the sector code and the sector name.   
```{r}

# For loop, iterating over each row in our naics industry dataframe

for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry |>
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url |>
      read_html() |>
      html_elements(xpath = xpath_employment_table) |>
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table.
    employment_info <- employment_info[[1]] |>
      clean_names() |>
      slice(2) |>
      bind_cols(each_row_df)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}
```
Then we'll do a little bit of cleaning.

Let's use parse_number() to remove the comma from the jul_2021 number and convert it from a character to number. We'll use rename() to make the jul_2021 column name a little more descriptive. And then we'll use select() to keep only the columns we want to keep -- the sector number, the sector name, and the jul_2021 employment number.

```{r}

# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry |>
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url |>
      read_html() |>
      html_elements(xpath = xpath_employment_table) |>
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table; turn jul_2021 column into a proper number, and rename it.  Then select only three columns we need.
    employment_info <- employment_info[[1]] |>
      clean_names() |>
      slice(2) |>
      bind_cols(each_row_df) |>
      mutate(jul_2021 = parse_number(jul_2021)) |>
      rename(jul_2021_employees = jul_2021) |>
      select(sector,description,jul_2021_employees)

    # To help us see what's happening as we build this, we're going to print the thing we're creating.  
    print(employment_info)


}

```


We're getting very close to the finished table we showed at the beginning.  

But right now, each bit of sector information is separated between 19 different dataframes.  

We want them in one dataframe.  

We can fix this by creating an empty dataframe called "employment_by_sector_all" using tibble(), placing it before our "for loop".

And inside our "for loop" at the end, we'll bind each employment_info dataframe to the newly created empty dataframe.  

```{r}

# Create an empty dataframe to hold results
employment_by_sector_all <- tibble()

# For loop, iterating over each row in our naics industry dataframe
for(row_number in 1:nrow(naics_industry)) {

    # Keep only the row for a given row number, get rid of every other row
    each_row_df <- naics_industry |>
      slice(row_number)

    # Define url of page to get
    url <- each_row_df$sector_url

    # Define id of table to ingest
    xpath_employment_table <- paste0('//*[@id="',each_row_df$sector_xpath_id,'"]')

    # Get employment table from each page by going to each url defined above, reading in the html with read_html(), extracting the table with the id generated by the xpath code using html_elements), and then turning the html into a proper dataframe using html_table().  The dataframe is in a nested list, which we'll have to extract in the next step.
    employment_info <- url |>
      read_html() |>
      html_elements(xpath = xpath_employment_table) |>
      html_table()

    # Grab the dataframe out of the list (it's the first and only element inside the list); clean up the field names with clean_names(); use slice(2) to keep only the second row; use bind_cols() to append the sector code and name to this table; turn jul_2021 column into a proper number, and rename it.  Then select only three columns we need.
    employment_info <- employment_info[[1]] |>
      clean_names() |>
      slice(2) |>
      bind_cols(each_row_df) |>
      mutate(jul_2021 = parse_number(jul_2021)) |>
      rename(jul_2021_employees = jul_2021) |>
      select(sector,description,jul_2021_employees)

    # Bind each individual employment info table to our employment_by_sector_all dataframe
    employment_by_sector_all <- employment_by_sector_all |>
      bind_rows(employment_info)

}

# Display the completed dataframe
employment_by_sector_all
```

Ta da! The end result is a nice tidy dataframe with the number of employees in June 2021 for each sector.

It's always a good idea to spot check the results, especially any values that look suspciously high or low.

The value for "Agriculture, Forestry, Fishing and Hunting" seems suspiciously low, compared with the other values.  

Let's figure out why.  

Here's the table on the mining sector page: [https://www.bls.gov/iag/tgs/iag21.htm](https://www.bls.gov/iag/tgs/iag21.htm)

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest3.png"))
```
And here's the table for the agriculture sector.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/advrvest4.png"))
```
Unlike mining -- and every other sector page (I checked each page) -- the agriculture page is structured differently.  

In the second row of this table, it has the unemployment rate. Nowhere on the page can we find information on the number of employees.  We would need to do additional research to track down a valid number if we plan on using this table, but for now we're going to replace it with an NA using na_if.

```{r}
# remove the suspicious value for agriculture.
employment_by_sector_all <- employment_by_sector_all |>
  mutate(jul_2021_employees = na_if(jul_2021_employees,7.5))

# display it
employment_by_sector_all
```

And we're done.  

A note about advanced scraping -- every site is different. Every time you want to scrape a site, you'll be puzzling over different problems. But the steps remain the same: find a pattern, exploit it, clean the data on the fly and put it into a place to store it.

<!--chapter:end:15-advancedrvest.Rmd-->

# Intro to APIs: The Census

There is truly an astonishing amount of data collected by the US Census Bureau. First, there's the Census that most people know -- the every 10 year census. That's the one mandated by the Constitution where the government attempts to count every person in the US. It's a mind-boggling feat to even try, and billions get spent on it. That data is used first for determining how many representatives each state gets in Congress. From there, the Census gets used to divide up billions of dollars of federal spending.

To answer the questions the government needs to do that, a ton of data gets collected. That, unfortunately, means the Census is exceedingly complicated to work with. The good news is, the Census has an API -- an application programming interface. What that means is we can get data directly through the Census Bureau via calls over the internet.

Let's demonstrate.

We're going to use a library called `tidycensus` which makes calls to the Census API in a very tidy way, and gives you back tidy data. That means we don't have to go through the process of importing the data from a file. I can't tell you how amazing this is, speaking from experience. The documentation for this library is [here](https://walker-data.com/tidycensus/). Another R library for working with Census APIs (there is more than one) is [this one](https://github.com/hrecht/censusapi) from Hannah Recht, a journalist with Kaiser Health News.

First we need to install `tidycensus` using the console: `install.packages("tidycensus")`

```{r}
library(tidyverse)
library(tidycensus)
```

To use the API, you need an API key. To get that, you need to [apply for an API key with the Census Bureau](https://api.census.gov/data/key_signup.html). It takes a few minutes and you need to activate your key via email. Once you have your key, you need to set that for this session. Just FYI: Your key is your key. Do not share it around.

```{r, include=FALSE}
census_api_key("549950d36c22ff16455fe196bbbd01d63cfbe6cf")
```

```{r, eval=FALSE}
census_api_key("YOUR KEY HERE", install=TRUE)
```

The two main functions in tidycensus are `get_decennial`, which retrieves data from the 2000 and 2010 Censuses (and soon the 2020 Census), and `get_acs`, which pulls data from the American Community Survey, a between-Censuses annual survey that provides estimates, not hard counts, but asks more detailed questions. If you're new to Census data, there's [a very good set of slides from Kyle Walker](http://walker-data.com/umich-workshop/census-data-in-r/slides/#1), the creator of tidycensus, and he's working on a [book](https://walker-data.com/census-r/index.html) that you can read for free online.

It's important to keep in mind that Census data represents people - you, your neighbors and total strangers. It also requires some level of definitions, especially about race & ethnicity, that may or may not match how you define yourself or how others define themselves.

So to give you some idea of how complicated the data is, let's pull up just one file from the decennial Census. We'll use Summary File 1, or SF1. That has the major population and housing stuff.

```{r}
sf1 <- load_variables(2010, "sf1", cache = TRUE)

sf1
```

Note: There are thousands of variables in SF1. That's not a typo. Open it in your environment by double clicking. As you scroll down, you'll get an idea of what you've got to choose from.

If you think that's crazy, try the SF3 file from 2000.

```{r}
sf3 <- load_variables(2000, "sf3", cache = TRUE)

sf3
```

Yes. That's more than 16,000 variables to choose from. I told you. Astonishing.

So let's try to answer a question using the Census. What is the fastest growing state since 2000?

To answer this, we need to pull the total population by state in each of the decennial census. Here's 2000.

```{r, eval=FALSE}
p00 <- get_decennial(geography = "state", variables = "P001001", year = 2000)

```

Now 2010.

```{r, eval=FALSE}
p10 <- get_decennial(geography = "state", variables = "P001001", year = 2010)
```

Let's take a peek at 2010.

```{r, eval=FALSE}
p10
```

As you can see, we have a GEOID, NAME, then variable and value. Variable and value are going to be the same. Because those are named the same thing, to merge them together, we need to rename them.

```{r, eval=FALSE}
p10 |> select(GEOID, NAME, value) |> rename(Population2010=value) -> p2010

p00 |> select(GEOID, NAME, value) |> rename(Population2000=value) -> p2000
```

Now we join the data together.

```{r, eval=FALSE}
alldata <- p2000 |> inner_join(p2010)
```

And now we calculate the percent change.

```{r, eval=FALSE}
alldata |> mutate(change = ((Population2010-Population2000)/Population2000)*100) |> arrange(desc(change))
```

And just like that: Nevada.

You may be asking: hey, wasn't there a 2020 Census? Where's that data? The answer is that it's coming - the Census Bureau has a [schedule of releases](https://www.census.gov/programs-surveys/popest/about/schedule.html).

## The ACS

In 2010, the Census Bureau replaced SF3 with the American Community Survey. The Good News is that the data would be updated on a rolling basis. The bad news is that it's more complicated because it's more like survey data with a large sample. That means there's margins of error and confidence intervals to worry about. By default, using `get_acs` fetches data from the 5-year estimates (currently 2015-2019), but you can specify 1-year estimates for jurisdictions with at least 65,000 people (many counties and cities).

Here's an example using the 5-year ACS estimates:

What is Maryland's richest county?

We can measure this by median household income. That variable is `B19013_001`, so we can get that data like this (I'm narrowing it to the top 20 for simplicity):
```{r}
md <- get_acs(geography = "county",
              variables = c(medincome = "B19013_001"),
              state = "MD",
              year = 2019)

md <- md |> arrange(desc(estimate)) |> top_n(20, estimate)

md

```

Howard, Calvert, Montgomery, Anne Arundel, Charles. What do they all have in common? Lots of suburban flight from DC and Baltimore. But do the margins of error let us say one county is richer than the other. We can find this out visually using error bars. Don't worry much about the code here --  we'll cover that soon enough.

```{r}
md |>
  mutate(NAME = gsub(" County, Maryland", "", NAME)) |>
  ggplot(aes(x = estimate, y = reorder(NAME, estimate))) +
  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) +
  geom_point(color = "red") +
  labs(title = "Household income by county in Maryland",
       subtitle = "2015-2019 American Community Survey",
       y = "",
       x = "ACS estimate (bars represent margin of error)")
```

As you can see, some of the error bars are quite wide. Some are narrow. But if the bars overlap, it means the difference between the two counties is within the margin of error, and the differences aren't statistically significant. So is the difference between Calvert and Montgomery significant? Nope. Is the difference between Howard and everyone else significant? Yes it is.

Let's ask another question of the ACS -- did any counties lose income from the time of the global financial crisis to the current 5-year window?

Let's re-label our first household income data.

```{r}
md19 <- get_acs(geography = "county",
              variables = c(medincome = "B19013_001"),
              state = "MD",
              year = 2019)
```

And now we grab the 2010 median household income.

```{r}
md10 <- get_acs(geography = "county",
              variables = c(medincome = "B19013_001"),
              state = "MD",
              year = 2010)
```

What I'm going to do next is a lot, but each step is simple. I'm going to join the data together, so each county has one line of data. Then I'm going to rename some fields that repeat. Then I'm going to calculate the minimium and maximum value of the estimate using the margin of error. That'll help me later. After that, I'm going to calculate a perent change and sort it by that change.

```{r}
md10 |>
  inner_join(md19, by=c("GEOID", "NAME")) |>
  rename(estimate2010=estimate.x, estimate2019=estimate.y) |>
  mutate(min2010 = estimate2010-moe.x, max2010 = estimate2010+moe.x, min2019 = estimate2019-moe.y, max2019 = estimate2019+moe.y) |>
  select(-variable.x, -variable.y, -moe.x, -moe.y) |>
  mutate(change = ((estimate2019-estimate2010)/estimate2010)*100) |>
  arrange(change)
```

So according to this, Somerset and Caroline counties lost ground from the financial meltdown to now.

But did they?

Look at the min and max values for both. Is the change statistically significant?

The ACS data has lots of variables, just like the decennial Census does. To browse them, you can do this:

```{r}
v19 <- load_variables(2019, "acs5", cache=TRUE)
```

And then view `v19` to see what kinds of variables are available via the API.

## "Wide" Results

Although one of the chief strengths of tidycensus is that it offers a, well, tidy display of Census data, it also has the ability to view multiple variables spread across columns. This can be useful for creating percentages and comparing multiple variables.


## Sorting Results

You'll notice that we've used `arrange` to sort the results of tidycensus functions, although that's done after we create a new variable to hold the data. There's another way to use `arrange` that you should know about, one that you can use for exploratory analysis. An example using median household income from 2019:

```{r}
md19 <- get_acs(geography = "county",
              variables = c(medincome = "B19013_001"),
              state = "MD",
              year = 2019)
arrange(md19, desc(estimate))
```

In this case we don't save the sorted results to a variable, we can just see the output in the console.

<!--chapter:end:16-census.Rmd-->

# Visualizing your data for reporting

Visualizing data is becoming a much greater part of journalism. Large news organizations are creating graphics desks that create complex visuals with data to inform the public about important events.

To do it well is a course on its own. And not every story needs a feat of programming and art. Sometimes, you can help yourself and your story by just creating a quick chart, which helps you see patterns in the data that wouldn't otherwise surface.

Good news: one of the best libraries for visualizing data is in the tidyverse and it's pretty simple to make simple charts quickly with just a little bit of code. It's called [ggplot2](https://ggplot2.tidyverse.org/).

Let's revisit some data we've used in the past and turn it into charts. First, let's load libraries. When we load the tidyverse, we get ggplot2.

```{r}
library(tidyverse)
```

The dataset we'll use is the PPP loan data for Maryland.  Let's load it.

```{r}
ppp_maryland <- read_csv("pre_labs/pre_lab_09/data/ppp_loans_md.csv.zip")
```

## Bar charts

The first kind of chart we'll create is a simple bar chart.

It's a chart designed to show differences between things -- the magnitude of one thing, compared to the next thing, and the next, and the next.

So if we have thing, like a county, or a state, or a group name, and then a count of that group, we can make a bar chart.

So what does the chart of the top 10 maryland counties with the most total PPP loans look like?

First, we'll create a dataframe of those top 10, called maryland_ppp_top_counties.

```{r}
maryland_ppp_top_counties <- ppp_maryland |>
  group_by(project_county_name) |>
  summarise(
    total_loans = n()
  ) |>
  arrange(desc(total_loans)) |>
  head(10)

maryland_ppp_top_counties
```

Now let's create a bar chart using ggplot.

With ggplot, the first thing we'll always do is draw a blank canvas that will house our chart. We start with our dataframe name, and then (|>) we invoke the ggplot() function to make that blank canvas.  All this does is make a gray box, the blank canvas that will hold our chart.

```{r}
maryland_ppp_top_counties |>
  ggplot()
```

Next we need to tell ggplot what kind of chart to make.

In ggplot, we work with two key concepts called  geometries (abbreivated frequently as geom) and asthetics (abbreviated as aes).

Geometries are the shape that the data will take; think of line charts, bar charts, scatterplots, histograms, pie charts and other common graphics forms.  

Asesthetics help ggplot know what component of our data to visualize -- why we'll visualize values from one column instead of another.

In a bar chart, we first pass in the data to the geometry, then set the aesthetic.

In the codeblock below, we've added a new function, geom_bar().  

Using geom_bar() -- as opposed to geom_line() -- says we're making a bar chart.  

Inside of that function, the asthetic, aes, says which columns to use in drawing the chart.

We're setting the values on the x axis (horizontal) to be the name of the county. We set weight to total loans, and it uses that value to "weight" or set the height of each bar.

One quirk here with ggplot.  

After we've invoked the ggplot() function, you'll notice we're using a + symbol.  It means the same thing as  |> -- "and then do this".  It's just a quirk of ggplot() that after you invoke the ggplot() function, you use + instead of |>.  It makes no sense to me either, just something to live with.

```{r}
maryland_ppp_top_counties |>
  ggplot() +
  geom_bar(aes(x=project_county_name, weight=total_loans))
```

This is a very basic chart.  But it's hard to derive much meaning from this chart, because the counties aren't ordered from highest to lowest by total_loans. We can fix that by using the reorder() function to do just that:

```{r}
maryland_ppp_top_counties |>
  ggplot() +
  geom_bar(aes(x=reorder(project_county_name,total_loans), weight=total_loans))
```

This is a little more useful. But the bottom is kind of a mess, with overlapping names.  We can fix that by flipping it from a vertical bar chart (also called a column chart) to a horizontal one. coord_flip() does that for you.

```{r}
maryland_ppp_top_counties |>
  ggplot() +
  geom_bar(aes(x=reorder(project_county_name,total_loans), weight=total_loans)) +
  coord_flip()
```

Is this art? No. Does it quickly tell you something meaningful?  It does.

We're mainly going to use these charts to help us in reporting, so style isn't that important.  

But it's worth mentioning that we can pretty up these charts for publication, if we wanted to, with some more code. To style the chart, we can change or even modify the "theme", a kind of skin that makes the chart look better.

It's kind of like applying CSS to html. Here I'm changing the theme slightly to remove the gray background with one of ggplot's built in themes, theme_minimal()

```{r}
maryland_ppp_top_counties |>
  ggplot() +
  geom_bar(aes(x=reorder(project_county_name,total_loans), weight=total_loans)) +
  coord_flip() +
  theme_minimal()
```
The ggplot universe is pretty big, and lots of people have made and released cool themes for you to use.  Want to make your graphics look kind of like [The Economist's](https://www.economist.com/) graphics?  There's a theme for that.  

First, you have to install and load a package that contains lots of extra themes, called [ggthemes](https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/).

```{r}
#install.packages('ggthemes')
library(ggthemes)
```

And now we'll apply the economist theme from that package with theme_economist()
```{r}
maryland_ppp_top_counties |>
  ggplot() +
  geom_bar(aes(x=reorder(project_county_name,total_loans), weight=total_loans)) +
  coord_flip() +
  theme_economist()
```
Those axis titles are kind of a mess. Let's change "count" on the x axis to "total loans" and change "reorder(project_county_name,total_loans)" to "county".  And while we're at it, let's add a basic title and a source as a caption. We'll use a new function, labs(), which is short for labels.

```{r}
maryland_ppp_top_counties |>
  ggplot() +
  geom_bar(aes(x=reorder(project_county_name,total_loans), weight=total_loans)) +
  coord_flip() +
  theme_economist() +
  labs(
    title="Maryland Counties with Most PPP Loans",
    x = "total loans",
    y = "county",
    caption = "source: SBA PPP loan database"

  )
```
Viola.  Not super pretty, but good enough to show an editor to help them understand the conclusions you reached with your data analysis.

## Line charts

Let's look at how to make another common chart type that will help you understand patterns in your data.

Line charts can show change over time. It works much the same as a bar chart, code wise, but instead of a weight, it uses a y.

So, let's create a dataframe with a count of Maryland loans for each date in our dataframe.
```{r}

ppp_maryland_loans_by_date <- ppp_maryland |>
  group_by(date_approved) |>
  summarise(
    total_loans=n()
  )

ppp_maryland_loans_by_date
```

And now let's make a line chart to look for patterns in this data.  

We'll put the date on the x axis and total loans on the y axis.

```{r}
ppp_maryland_loans_by_date |>
  ggplot() +
  geom_line(aes(x=date_approved, y=total_loans))


```

It's not super pretty, but there's an obvious pattern! There are a ton of loans right at the beginning of the program. There's a trickle for the next few months, and then no loans at all for several months.  

At the beginning of 2021, there's another spike, and a pretty steady level with some fluctuations until July 2021.  We know from previous chapters the explanation for this: there was a flood of loans when the program was first authorized, but it eventually ran out of money, and then it was later reauthorized.

Right now, it's kind of hard to see specifics, though.  Exactly when did loans fall to zero? August 2020?

We can't really tell.  So let's modify the x axis to have one tick mark and label per month. We can do that with a function called scale_x_date().

We'll set the date_breaks to appear for every month; if we wanted every week, we'd say date_breaks = "1 week". We can set the date to appear as month abbreviated name (%b) and four-digit year (%Y).

```{r}
ppp_maryland_loans_by_date |>
  ggplot() +
  geom_line(aes(x=date_approved, y=total_loans)) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b-%Y")


```

Those are a little hard to read, so we can turn them 45 degrees to remove the overlap using the theme() function for styling.  With "axis.text.x = element_text(angle = 45,  hjust=1)" we're saying, turn the date labels 45 degrees.

```{r}
ppp_maryland_loans_by_date |>
  ggplot() +
  geom_line(aes(x=date_approved, y=total_loans)) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b-%Y") +
  theme(
    axis.text.x = element_text(angle = 45,  hjust=1)
  )

```

Again, this isn't as pretty as we could make it.  But by charting this, we can quickly see a pattern that can help guide our reporting.

We're just scratching the surface of what ggplot can do, and chart types. There's so much more you can do, so many other chart types you can make.  But the basics we've shown here will get you started.

<!--chapter:end:17-visualizingforreporting.Rmd-->

# Visualizing your data for publication

Doing data visualization well, and at professional level, takes time, skill and practice to perfect. Understanding it and doing it at a complex level is an entire class on it's own. It uses some of the same skills here -- grouping, filtering, calculating -- but then takes that data and turns it into data pictures.

But simple stuff -- and even some slightly complicated stuff -- can be done with tools made for people who aren't data viz pros.

The tool we're going to use is called [Datawrapper](https://www.datawrapper.de/).

First, let's get some data and work with it. Let's use a cleaned-up version of 2021 PPP loan totals by type of lender. Let's look at it.

```{r}
library(tidyverse)
```

```{r}
lender_types <- read_csv("data/lender_totals_2021.csv")
```

```{r}
head(lender_types)
```

## Datawrapper

Making charts in Datawrapper is preposterously simple, which is the point. There are dozens of chart types, and dozens of options. To get from a csv to a chart to publication is very, very easy.

First, go to [datawrapper.de](https://www.datawrapper.de/) and sign up for an account. It's free.

Once logged in, you'll click on New Chart.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/ppp_datawrapper1.png"))
```

The first thing we'll do is upload our CSV that we created before. Click on XLS/CSV and upload the file.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/datawrapper2.png"))
```

Next up is to check and see what Datawrappper did with our data when we uploaded it. As you can see from the text on the left, if it's blue, it's a number. If it's green, it's a date. If it's black, it's text. Red means there's a problem. This data is very clean, so it imports cleanly. Click on the "Proceed" button.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/ppp_datawrapper3.png"))
```
Now we make a chart. Bar chart comes up by default, which is good, because with totals, that's what we have.

Click on Refine. The first option we want to change is the Number Format, because we have currency figures to display and we want to make it as easy as possible for readers to understand what we're displaying. Datawrapper has an excellent reference for its [custom formats](https://academy.datawrapper.de/article/207-custom-number-formats-that-you-can-display-in-datawrapper) that you can consult. Let's choose `$0.[00]a`, which adds a dollar sign and abbreviates larger amounts:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/ppp_datawrapper4.png"))
```

Now we need to annotate our charts. Every chart needs a title, a source line and a credit line. Most need chatter (called description here). Click on the "Annotate" tab to get started.

Really think about the title and description: the title is like a headline and the description is provides some additional context. Another way to think about it: the title is the most important lesson from the graphic, and the description could be the next most important lesson or could provide more context to the title.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/ppp_datawrapper5.png"))
```

To publish, we click the "Publish & Embed" tab. Some publication systems allow for the embedding of HTML into a post or a story. Some don't. The only way to know is to ask someone at your publication. Every publication system on the planet, though, can publish an image. So there's always a way to export your chart as a PNG file, which you can upload like any photo.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/ppp_datawrapper7.png"))
```

### Making a Map

Let's create a choropleth map - one that shows variations between the total number of approved PPP applications across Maryland counties. First, we'll need to generate that data using our collection of Maryland PPP loans.

```{r}
maryland_ppp <- read_rds("data/maryland_ppp.rds")
```

First, we'll create a dataframe that has county (jurisdiction) level counts of approved applications. Run the following code to do that. There are some mistakes in the data, so we're going to remove those with a filter after doing the counting.

```{r}
md_counties <- maryland_ppp |>
    group_by(project_county_name) |>
    summarise(count=n())
```

Let's deal with those seemingly extraneous records that don't appear to be MD PPP applications:

```{r}
md_counties <- maryland_ppp |>
    group_by(project_county_name) |>
    summarise(count=n()) |>
    filter(count > 6)
```

In order to make a map, we need to be able to tell Datawrapper that a certain column contains geographic information (besides the name of the county). The easiest way to do that for U.S. maps is to use something called a [FIPS Code](https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html). You should read about them so you understand what they are, and think of them as a unique identifier for some geographical entity like a state or county. Our maryland_ppp dataframe doesn't have a FIPS code for each county, but this is a solved problem thanks to the Tigris library. Let's install it:

```{r}
#install.packages('tigris',repos = "http://cran.us.r-project.org")
library(tigris)
```

Once we've done that, we have access to a dataframe containing all fips codes. Let's isolate the Maryland codes:

```{r}
all_fips <- fips_codes |> as_tibble()
md_fips <- all_fips |> filter(state == 'MD')
```

Looks good, but there are two issues: Datawrapper expects a 5-digit FIPS code (the state code plus the county code, so "24001" for Allegany County) and the county names don't match the `project_county_name` in the PPP data. Let's fix the first issue - adding a full FIPS code based on its components using the function `str_c`, which concatenates multiple strings:

```{r}
md_fips <- md_fips |> mutate(fips_code = str_c(state_code, county_code))
```

Now we'll deal with the county names in `md_counties`. They are all caps and contain no punctuation, we could:

1. Change counties in `md_counties` to match `md_fips`
2. Change counties in `md_fips` to match `md_counties`

Let's do the latter. We'll use mutate to create an uppercase version of the name, remove " COUNTY", replace the quotemarks with nothing and change St. Mary's so it matches the PPP data:

Change the county names in `md_fips`

```{r}
md_fips <- md_fips |> mutate(match_county = str_to_upper(county)) |>
   mutate(match_county = str_replace(match_county, ' COUNTY', '')) |>
   mutate(match_county = str_replace(match_county, "'", "")) |>
   mutate(match_county = str_replace(match_county, "ST. MARY", "SAINT MARY"))
```

Now we'll join `md_counties` and `md_fips` together using our new `match_county` column:

```{r}
md_counties_with_fips <- md_counties |>
  left_join(md_fips, by=c('project_county_name'='match_county'))
```

Then we'll write `md_counties_with_fips` to a CSV in the data folder using write_csv:

```{r}
write_csv(md_counties_with_fips, "data/md_counties.csv")
```

Go back to Datawrapper and click on "New Map". Click on "Choropleth map" and then choose "USA >> Counties (2018)" for the map base and click the Proceed button.

Now we can upload the `md_counties.csv` file we just saved using the Upload File button. It should look like the following image:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/ppp_datawrapper8.png"))
```

We'll need to make sure that Datawrapper understands what the data is and where the FIPS code is. Click on the "Match" tab and make sure that yours looks like the image below:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/ppp_datawrapper9.png"))
```

Click the "Proceed" button (you should have to click it twice, since the first time it will tell you that there's no data for 3,199 counties - the rest of the U.S.). That will take you to the Visualize tab.

You'll see that the map currently is of the whole nation, and we only have Maryland data. Let's fix that.

Look for "Hide regions without data" under Appearance, and click the slider icon to enable that feature. You should see a map zoomed into Maryland with some counties in various colors.

But it's a little rough visually, so let's clean that up.

Look for the "Show color legend" label and add a caption for the legend, which is the horizontal bar under the title. It represents the extent of the data from smallest number of loans to largest. Then click on the "Annotate" tab to add a title, description, data source and byline. The title should represent the headline, while the description should be a longer phrase that tells people what they are looking at.

That's better, but check out the tooltip by hovering over a county. It's not super helpful. Let's change the tooltip behavior to show the county name and a better-formatted number.

Click the "Customize tooltips" button so it expands down. Change {{ fips }} to {{ county }} and {{ count }} to {{ FORMAT(count, "0,0.[00]")}}

Ok, that looks better. Let's publish!

Click the "Proceed" button until you get to the "Publish & Embed" tab, then click "Publish Now".

<!--chapter:end:18-visualizingforpublication.Rmd-->

# Geographic data basics

Up to now, we've been looking at patterns in data for what is more than this, or what's the middle look like. We've calculated metrics like per capita rates, or looked at how data changes over time.

Another way we can look at the data is geographically. Is there a spatial pattern to our data? Can we learn anything by using distance as a metric? What if we merge non-geographic data into geographic data?

The bad news is that there isn't a One Library To Rule Them All when it comes to geo queries in R. But there's one emerging, called Simple Features, that is very good.

Go to the console and install it with `install.packages("sf")`

To understand geographic queries, you have to get a few things in your head first:

1. Your query is using planar space. Usually that's some kind of projection of the world. If you're lucky, your data is projected, and the software will handle projection differences under the hood without you knowing anything about it.
2. Projections are cartographers making opinionated decisions about what the world should look like when you take a spheroid -- the earth isn't perfectly round -- and flatten it. Believe it or not, every state in the US has their own geographic projection. There's dozens upon dozens of them.
3. Geographic queries work in layers. In most geographic applications, you'll have multiple layers. You'll have a boundary file, and a river file, and a road file, and a flood file and combined together they make the map. But you have to think in layers.
4. See 1. With layers, they're all joined together by the planar space. So you don't need to join one to the other like we did earlier -- the space has done that. So you can query how many X are within the boundaries on layer Y. And it's the plane that holds them together.

```{r, echo=FALSE}
knitr::include_graphics(rep("images/geolayers.jpg"))
```

## Importing and viewing data

Let's start with the absolute basics of geographic data: loading and viewing. Load libraries as usual.

```{r}
library(tidyverse)
library(sf)
library(janitor)
```

First: an aside on geographic data. There are many formats for geographic data, but data type you'll see the most is called the shapefile. It comes from a company named ERSI, which created the most widely used GIS software in the world. For years, they were the only game in town, really, and the shapefile became ubiquitous, especially so in government and utilities.

So more often than not, you'll be dealing with a shapefile. But a shapefile isn't just a single file -- it's a collection of files that combined make up all the data that allow you to use it. There's a .shp file -- that's the main file that pulls it all together -- but it's important to note if your shapefiles has a .prj file, which indicates that the projection is specified.

The data we're going to be working with is a file from the Department of Homeland Security that is every hospital in the US and the number of beds they have. I'm writing this during the days of coronavirus, and suddenly the number of hospital beds is a top concern. So let's look at where hospital beds are and how many of them are there.

When you do, it should look something like this:

```{r, echo=FALSE}
knitr::include_graphics(rep("images/geolayers2.png"))
```

Simlar to `readr`, the `sf` library has functions to read geographic data. In this case, we're going to use `st_read` to read in our hospitals data. And then glimpse it to look at the columns.

```{r}
hospitals <- st_read("data/Hospitals/Hospitals.shp")

glimpse(hospitals)
```

This looks like a normal dataframe, and mostly it is.  We have one row per hospital, and each column is some feature of that hospital: the name, the address, it's open/closed status and more.  What sets this data apart from other dataframes we've used is the last column, "geometry", which is of a new data type.  It's not a character or a number, it's a "POINT", which is composed of a longitude value and a latitude value.  When we plot these on a grid of latitude and longitude, it will place a point where those two numbers intersect.

Let's look at just Maryland hospitals. Good news -- `sf` plays very nicely with the tidyverse, so we can filter data the way we are accustomed.

```{r}
md_hospitals <- hospitals |>
  filter(STATE == "MD")

md_hospitals
```

We have 72 hospitals, according to this data.

What kind of hospitals do we have?

```{r}
md_hospitals |>
  group_by(TYPE) |>
  summarise(
    count=n()
  ) |>
  arrange(desc(count))
```
Let's narrow our data to only look at the 50 "General Acute Care hospitals."

```{r}

md_hospitals <- hospitals |>
  filter(STATE == "MD") |>
  filter(TYPE == "GENERAL ACUTE CARE")

md_hospitals

```

That gives us 50 hospitals in Maryland. Where are they?

We can simply plot them on a longitude-latitude grid using ggplot.

```{r}
md_hospitals |>
  ggplot() +
  geom_sf() +
  theme_minimal()
```

Each point is a hospital. Each hospital has been plotted according to its degrees of longitude and latitude.

If you know anything about the state of Maryland, you can kinda pick out the shape of the state there. The point in the top left is in Western Maryland. The point at the extreme bottom right is on the Eastern Shore. But this map is not exactly ideal. It would help to have a state and county map layered underneath of it, to help make sense of the spatial nature of this data.

This is where layering becomes more clear. First, we want to go out and get another shapefile, this one showing Maryland county outlines.

Instead of loading it from our local machine, like we did above, we're going to use a package to directly download it from the U.S. Census.  The package is called `tigris` and it's developed by the same person who made `tidycensus`.

In the console, install tigris with the install packages function

Then load it:

```{r}
library(tigris)
```

Now, let's use the counties() function from tigris to pull down a shapefile of all U.S. counties.

```{r}

counties <- counties()

glimpse(counties)
```

This looks pretty similar to our hospital shapefile, in that it looked mostly like a normal dataframe with the exception of the new geometry column.

But instead of POINT, this geometry is of the datatype "MULTIPOLYGON".  Points are shape data represented by one pair of longitude or latitude coordinates. Polygons are made up of LOTS of pairs of longitude and latitude coordinates, connected by a boundary line into a complex shape.  

If you've ever filled in a "connect the dots" picture by drawing lines between points, in order to reveal a hidden shape, then you're familiar with the concept.

This county shapefile has all 3233 U.S. counties.  We only want the Maryland counties, so we're going to filter the data to only keep Maryland counties.  There is no STATE column, but there is a STATEFP column, with each number representing a state.  Maryland's FP number is 24.

```{r}
md_counties <- counties |>
  filter(STATEFP == "24")

```

To see what this looks like, let's plot it out with ggplot. We can pretty clearly see the shapes of Maryland counties.

```{r}
md_counties |>
  ggplot() +
  geom_sf() +
  theme_minimal()
```

With this county map, we can layer our hospital data.

Something to note: The layers are rendered in the order they appear. So the first geom_sf is rendered first. The second geom_sf is rendered ON TOP OF the first one.

We're also going to change things up a bit to put the datasets we want to display INSIDE of the geom_sf() function, instead of starting with a dataframe.  We have two to plot now, so it's easier this way.

```{r}

ggplot() +
  geom_sf(data=md_counties) +
  geom_sf(data=md_hospitals) +
  theme_minimal()
```
What can we tell from this?

Well, hospitals are clustered around the state's most populous areas, the Baltimore to Washington corridor. There are fewer hospitals in rural Eastern and Western counties. And two counties have no hospital at all.

This is a pretty blunt visualization. Not all hospitals are equal. Some have more beds than the others, and bed space is a critical factor in how full hospitals get during COVID-19 surges.  

We can get a sense of where the largest hospitals are, by changing the color of the points according to the number of beds. We do this by setting the aesthetic -- or aes -- to use the BEDS column inside of the geom_sf function.  To make the differences easier to see, we're going to change the fill of the counties white, too, and use a special color palette, viridis magma. We're also going to make the points slightly bigger.


```{r}

ggplot() +
  geom_sf(data=md_counties, fill="white") +
  geom_sf(data=md_hospitals, aes(color=BEDS), size=2) +
  scale_colour_viridis_b(option="magma") +
  theme_minimal()

```

With these changes, what else can we make out here?  Well, not only are most hospitals clustered in the center of Maryland, the largest ones are too. Rural areas have fewer and typically smaller hospitals.


<!--chapter:end:19-geographicbasics.Rmd-->

# Geographic analysis

In the previous chapter, we looked at Maryland's hospitals and used layers to show where hospitals sit on a map of Maryland's counties, and to show a bit of a pattern regarding concentration of the largest hospitals. Let's go little further.

First, let's load the libraries we'll need. We're also going to load tidycensus and set an API key for tidycensus.

```{r}
library(tidyverse)
library(sf)
library(janitor)
library(tidycensus)
census_api_key("549950d36c22ff16455fe196bbbd01d63cfbe6cf")

```

And now let's load the dataframe of hospital information from the previous chapter, and filter for the 50 General Acute Care hospitals in Maryland.

```{r}
md_hospitals <- st_read("data/Hospitals/Hospitals.shp") |>
  filter(STATE == "MD") |>
  filter(TYPE == "GENERAL ACUTE CARE")

md_hospitals
```

For the rest of this chapter, we're going to work on building a map that will help us gain insight into geographic patterns in hospital bed availability by county in Maryland. Our question: by examining the number of hospital beds per 100,000 people in each county, what regional geographic patterns can we identify?

Each hospital exists inside of a county, so we're going to first calculate the total number of beds in each county. We do this by first converting the md_hospitals data to a standard dataframe (instead of a spatial dataframe) using as_tibble(), then grouping by county and totaling the number of beds. Finally, let's sort by total_beds to see which county has the most.

```{r}

md_beds_by_county <- md_hospitals |>
  as_tibble() |>
  group_by(COUNTY) |>
  summarise(
    total_beds = sum(BEDS)
  ) |>
  arrange(desc(total_beds))

md_beds_by_county
```
So, what do we see here? Baltimore City has the most, followed by Montgomery, Baltimore County and Prince George's.  All big counties.

Next, we'll go out and get population data for each county from tidycensus.  The variable for total population is B01001_001.  

```{r}

md_county_population <- get_acs(geography = "county",
              variables = c(population = "B01001_001"),
              state = "MD")

md_county_population
```

Ultimately, we're going to join this county population table with our beds by county table, and then calculate a beds per 100,000 people statistic.  But remember, we then want to visualize this data by drawing a county map that helps us pick out trends. Thinking ahead, we know we'll need a county map shapefile.  Fortunately, we can pull this geometry information right from tidycensus at the same time that we pull in the population data by adding "geometry = TRUE" to our get_acs function.

```{r}

md_county_population <- get_acs(geography = "county",
              variables = c(population = "B01001_001"),
              state = "MD",
              geometry = TRUE)

md_county_population
```

We now have a new column, geometry, that contains the "MULTIPOLYGON" data that will draw an outline of each county when we go to draw a map.

The next step will be to join our population data to our hospital bed data on the county column.

But there's a problem.  The column in our population data that has county names is called "NAME", and it has the full name of the county spelled out in title case -- first word capitalized and has "County" and "Maryland" in it.  The beds data just has the uppercase name of the county.  For example, the population data has "Anne Arundel County, Maryland" and the beds data has "ANNE ARUNDEL"

```{r}

md_county_population

md_beds_by_county
```

If they're going to join properly, we need to clean one of them up to make it match the other.  

Let's clean the population table. We're going to rename the "NAME" column to "COUNTY", then convert it to uppercase while also removing ", Maryland" and "County". Next we'll remove any white spaces after that first cleaning step that, if left in, would prevent a proper join. We're also going to rename the column that contains the population information from "estimate" to "population" and select only the county name and the population columns, along with the geometry. That leaves us with this tidy table.

```{r}
md_county_population <- md_county_population |>
  rename(COUNTY = NAME) |>
  mutate(COUNTY = toupper(str_remove_all(COUNTY,", Maryland|County"))) |>
  mutate(COUNTY = str_trim(COUNTY,side="both")) |>
  rename(population = estimate) |>
  select(COUNTY, population, geometry)

md_county_population

```

Now we can join them.  

```{r}
md_beds_per_100k <- md_county_population |>
  left_join(md_beds_by_county)

md_beds_per_100k
```

We have two NAs after we join, for Queen Anne's County and Carolina County.  That's not an error.  There are no General Acute Care hospitals in those counties, according to our data (it's why our beds table has 22 rows, not 24).  So let's convert those values to 0 using replace_na().

```{r}
md_beds_per_100k <- md_county_population |>
  left_join(md_beds_by_county) |>
  mutate(total_beds = replace_na(total_beds,0))

md_beds_per_100k
```

Our final step before visualization, let's calculate the number of beds per 100,000 for each county and sort from highest to lowest to see waht trends we can identify just from the table.

```{r}
md_beds_per_100k <- md_county_population |>
  left_join(md_beds_by_county) |>
  mutate(total_beds = replace_na(total_beds,0)) |>
  mutate(beds_per_100k = total_beds/population*100000) |>
  arrange(desc(beds_per_100k))

md_beds_per_100k
```

Let's take a look at the result of this table.  Baltimore City is still up there at the top, even when measuring by beds per 100k.  But there are some surpising ones at the top, some of Maryland's smallest counties! Allegany, Talbot, Somerset may not have that many beds, but they also don't have a lot of people.  

Okay, now let's visualize.  We're going to build a choropleth map, with the color of each county -- the fill -- set according to the number of beds per 100K on a color gradient.  

```{r}
ggplot() +
  geom_sf(data=md_beds_per_100k, aes(fill=beds_per_100k)) +
  theme_minimal()
```
This map is okay, but the color scale makes it hard to draw fine-grained differences. Let's try applying the magma color scale we learned in the last chapter.

```{r}
ggplot() +
  geom_sf(data=md_beds_per_100k, aes(fill=beds_per_100k)) +
  theme_minimal() +
  scale_fill_viridis_b(option="magma")
```
The highest ranking counties stand out nicely in this version, but it's still hard to make out fine-grained differences between counties.

So let's change the color scale to a "log" scale, which will help us see those differences a bit more clearly.
```{r}
ggplot() +
  geom_sf(data=md_beds_per_100k, aes(fill=beds_per_100k)) +
  theme_minimal() +
  scale_fill_viridis_b(option="magma",trans = "log")
```
Some interesting regional patterns finally emerge.  

The Eastern Shore and Western Maryland have more beds per capita than Central Maryland (with the exception of Baltimore City). And Southern Maryland -- PG, Charles, Calvert and St. Mary's -- has by far the fewest beds per capita of any other region.

<!--chapter:end:20-geographicanalysis.Rmd-->

# Automating analysis

Many of the data analyses that you do will be largely one-off efforts -- you're going to do the analysis and write the story and be done. Maybe you'll come back to it in a couple of months or years, but really you're just doing it once.

But what happens when you have a long-running story, where you're going to update it every day, or every week? What changes when you're writing that code?

1. How will this run again without changing anything?
2. What questions do you have that have to be answered each time?
3. What changes when you have to repeat questions to changing data?

The global COVID-19 pandemic is something we're going to be writing about and covering for some time. One element of it -- one that materialized in Nebraska as I am writing this -- is a tsunami of first time joblessness claims for unemployment assistance. That data is regularly published, and we're going to be talking about it weekly for a long time. So it's the ideal candidate for repeating analysis -- scripting the questions we want to answer every week and doing so in a way that we can just load it without having to change anything.

Let's get some new libraries to our typical tidyverse import. First, I'm going to add a library called `readxl`, which does what you think it does. It reads Microsoft Excel files. The next one I'm going to add is `DT`. It stands for datatables, and it makes your dataframes into html tables that are browsable and searchable. This is a bigger issue for me -- the author who is turning these into html pages -- than you, working in your notebooks. We're also going to add a library called ggrepel, which assists in putting tables on dots in charts.

You install them the same way you do anything else -- `install.packages("readxl")` and `install.packages("DT")` and `install.packages("ggrepel")`.


```{r}
library(tidyverse)
library(janitor)
library(readxl)
library(DT)
library(ggrepel)
```

## Automating downloads an imports

Nebraska [publishes data weekly on first time unemployment claims on the state Department of Labor website](https://neworks.nebraska.gov/gsipub/index.asp?docid=710).

There's four datasets. We're looking at the weekly initial claims from 2012 to present. If you click it, you'll get an Excel spreadsheet. That's a problem, given that we've been working with CSVs all along. So the problem we have before us is this: Have to download it first, then open an Excel file with multiple sheets and lots of header and footers.

There's a multitude of ways to get data from a website, but base R has a simple function to just download a file and name it. Couldn't be easier. Right or control click on the link for the 2012 to 2020 initial claims data and copy the link location. Then use this function:

```{r}
download.file("https://neworks.nebraska.gov/admin/gsipub/htmlarea/uploads/NE%20UI%20Weekly%20Initial%20Claims.xlsx", destfile = "weeklyinitialclaims.xlsx")
```

Open the file in Excel so you can see what we're working with. The first sheet is just definitions and explanations. Then each sheet is a year (or year to date) of data. Some have footers. Some don't. Some have a headline. Some don't. So we have some work ahead of us.

There are better ways to do this, and if your author was better at this, I'd tell you about it. But for beginners and people who aren't totally sure of themselves, explicit is better than implicit. So instead of using programming magic, we're going to use copy and paste. My programmer friends just died inside a little, but done is better than clever.

What we need to do, in words, is this:

1. Read a specific sheet
2. Skip the top row
3. Clean the column names with janitor.
4. Select just the first two rows, which will help us chop out some garbage.
5. We need to find if the sheet has that source line at the end of the data, so we'll use `str_detect` in `stringr` and filter it out.
6. We'll use janitor to remove any empty rows and columns
7. And lastly, we'll convert all the numbers in initial_claims to actual numbers, because the Source line made them character on import.

It's a lot, but not really. Each step is very simple, and is designed to solve one problem. And after we do it with one, we'll do it with the next, and the next, and the next, so on and so forth.

```{r}
weeklyclaims20 <- read_excel("weeklyinitialclaims.xlsx", sheet=2, skip=1) |>
  clean_names() |>
  select(1:2) |>
  filter(!str_detect(initial_claims, "Source")) |>  
  remove_empty(c("cols", "rows")) |>
  mutate(initial_claims = as.numeric(initial_claims))

weeklyclaims19 <- read_excel("weeklyinitialclaims.xlsx", sheet=3, skip=1) |>
  clean_names() |> select(1:2) |>
  filter(!str_detect(initial_claims, "Source")) |>
  remove_empty(c("cols", "rows")) |>
  mutate(initial_claims = as.numeric(initial_claims))

weeklyclaims18 <- read_excel("weeklyinitialclaims.xlsx", sheet=4, skip=1) |>
  clean_names() |>
  select(1:2) |>
  filter(!str_detect(initial_claims, "Source")) |>
  remove_empty(c("cols", "rows")) |>
  mutate(initial_claims = as.numeric(initial_claims))

weeklyclaims17 <- read_excel("weeklyinitialclaims.xlsx", sheet=5, skip=1) |>
  clean_names() |>
  select(1:2) |>
  filter(!str_detect(initial_claims, "Source")) |>
  remove_empty(c("cols", "rows")) |>
  mutate(initial_claims = as.numeric(initial_claims))

weeklyclaims16 <- read_excel("weeklyinitialclaims.xlsx", sheet=6, skip=1) |>
  clean_names() |>
  select(1:2) |>
  filter(!str_detect(initial_claims, "Source")) |>
  remove_empty(c("cols", "rows")) |>
  mutate(initial_claims = as.numeric(initial_claims))

weeklyclaims15 <- read_excel("weeklyinitialclaims.xlsx", sheet=7, skip=1) |>
  clean_names() |>
  select(1:2) |>
  filter(!str_detect(initial_claims, "Source")) |>
  remove_empty(c("cols", "rows")) |>
  mutate(initial_claims = as.numeric(initial_claims))

weeklyclaims14 <- read_excel("weeklyinitialclaims.xlsx", sheet=8, skip=1) |>
  clean_names() |>
  select(1:2) |>
  filter(!str_detect(initial_claims, "Source")) |>
  remove_empty(c("cols", "rows")) |>
  mutate(initial_claims = as.numeric(initial_claims))

weeklyclaims13 <- read_excel("weeklyinitialclaims.xlsx", sheet=9, skip=1) |>
  clean_names() |>
  select(1:2) |>
  filter(!str_detect(initial_claims, "Source")) |>
  remove_empty(c("cols", "rows")) |>
  mutate(initial_claims = as.numeric(initial_claims))

weeklyclaims12 <- read_excel("weeklyinitialclaims.xlsx", sheet=10, skip=1) |>
  clean_names() |>
  select(1:2) |>
  filter(!str_detect(initial_claims, "Source")) |>
  remove_empty(c("cols", "rows")) |>
  mutate(initial_claims = as.numeric(initial_claims))
```

Now we need to combine all those tables together. We can do that with rbind and some convenient overwriting of a dataframe to add new data each time.

```{r}
weeklyclaims <- rbind(weeklyclaims12, weeklyclaims13)
weeklyclaims <- rbind(weeklyclaims, weeklyclaims14)
weeklyclaims <- rbind(weeklyclaims, weeklyclaims15)
weeklyclaims <- rbind(weeklyclaims, weeklyclaims16)
weeklyclaims <- rbind(weeklyclaims, weeklyclaims17)
weeklyclaims <- rbind(weeklyclaims, weeklyclaims18)
weeklyclaims <- rbind(weeklyclaims, weeklyclaims19)
weeklyclaims <- rbind(weeklyclaims, weeklyclaims20)
```

## Exploring the data

Let's take a look at what we have, using datatables. The formatDate business just makes the date look nicer.

```{r}
datatable(weeklyclaims) |> formatDate(1, "toLocaleDateString")
```

Let's just look at the most recent week, and that's something that takes on different meaning when we're talking about updating data. We need to make this generic so that every time we pull this up and run it, it's the most recent week at the top. This time, it's very simple.

```{r}
weeklyclaims20 |> arrange(desc(week_ending_date))
```

## Analysis

Now is when we need to start asking ourselves -- what are the questions that are going to come up week after week. What about how this most current week compares to all weeks going back to 2012? What if we just ranked them? Where does this week rank? For that, we'll create a new column called Rank using mutate and we'll use a function called `min_rank` to rank them. I'm going to save them to a dataframe and use data

```{r}
ranked <- weeklyclaims |> mutate(Rank = min_rank(desc(initial_claims))) |> arrange(desc(week_ending_date))

datatable(ranked) |> formatDate(1, "toLocaleDateString")
```

Let's think about this a little more. What else could we do with this. What are the recurring questions? How about the percent change between this week and last week?

To do that, we need our dates to be next to each other -- side by side. Then we can do new minus old divided by old. To do that, we're going to use a function from `tidyr` called pivot_wider, which will transform our data from one row per week to one row, with the weeks as columns.

```{r}
change <- weeklyclaims20 |> pivot_wider(names_from = week_ending_date, values_from = initial_claims)

head(change)
```

Now the problem we have is ... which column is the last one, and which one is the previous one? I'll be honest, this isn't easy in R. But the trick is to reverse the order of the columns. Then, your newest one is column 1 and the next newest is 2.  

```{r}
changecalc <- ((rev(change)[1] - rev(change)[2])/rev(change)[2])*100

changecalc
```

So whatever the date, that'll always return the percent change between the most recent date and the previous week.

## Making updating graphics

More than numbers, we are going to want to see this data. We can build this in steps. First, let's just make a big bar chart.

```{r fig.width=9}
ggplot() +
  geom_bar(data=ranked, aes(x=week_ending_date, weight=initial_claims))
```

So that shows us that the trend is going down over time, and that there's some regular spikes around the holidays. Which tells us this data is seasonal, but we knew that going in.

Let's build up some more layers to highlight trends and the most recent spot.

First, we'll slice out a dataframe that's just the most recent data.

```{r}
latest <- ranked |> slice(1)
```

Now, in ggplot, we can add multiple layers.

The first layer will be all the bars.

The second layer will just be the latest.

Then we'll add a point to the top of that line to really draw attention to it.

Then we'll use ggprepel to label it.

Then I'm going to add a smoothing line. That'll illustrate the trend clearly.

The rest is labeling and adjusting the text to make it look more like a news graphic.

```{r fig.width=9}
ggplot() +
  geom_bar(data=ranked, aes(x=week_ending_date, weight=initial_claims)) +
  geom_bar(data=latest, aes(x=week_ending_date, weight=initial_claims), fill="red") +
  geom_point(data=latest, aes(x=week_ending_date, y=initial_claims)) +
  geom_text_repel(data=latest, aes(x=week_ending_date, y=initial_claims + 150, label="This week")) +
  geom_smooth(data=ranked, aes(x=week_ending_date, y=initial_claims), method=loess, se=FALSE) +
  labs(title="Nebraska jobless claims on the rise", x="Date", y="Claims") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 8),
    plot.subtitle = element_text(size=10),
    panel.grid.minor = element_blank()
    )
```

One thing we are missing? Context. What if we programmatically wrote the chatter for this chart using the percent change calculation we did before?

First, we format the change to look more news graphic like and not with 7 significant digits.

```{r}
changetext <- round(changecalc[[1]], digits=2)
```

Now we're going to use a function called paste to merge some text together. We're going to paste together a sentence fragment, the percent change number and another sentence fragment together to form a sentence. We'll save it as sub, because that's what it's called in ggplot -- a subtitle.

```{r}
sub <- paste("First time unemployment claims rose by ", changetext, " percent over last week", sep="")
```

Here's our sentence:

```{r}
sub
```

Now we can add that to our labels.

```{r fig.width=9}
ggplot() +
  geom_bar(data=ranked, aes(x=week_ending_date, weight=initial_claims)) +
  geom_bar(data=latest, aes(x=week_ending_date, weight=initial_claims), fill="red") +
  geom_point(data=latest, aes(x=week_ending_date, y=initial_claims)) +
  geom_text_repel(data=latest, aes(x=week_ending_date, y=initial_claims + 150, label="This week")) +
  geom_smooth(data=ranked, aes(x=week_ending_date, y=initial_claims), method=loess, se=FALSE) +
  labs(title="Nebraska jobless claims on the rise", subtitle=sub, x="Date", y="Claims") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 8),
    plot.subtitle = element_text(size=10),
    panel.grid.minor = element_blank()
    )
```

This is going to be a story for months, if not years. So repeating this analysis is a must for a reporter covering the economy in Nebraska. We've set ourselves up to do this every week when the data comes out. We just open our notebook, go to Run > Restart R and Run All Chunks and sit back and watch as it does it all again.

Then we go report.

## The State vs the Feds

As I write this, the state hasn't updated their data but the feds have, and the feds have more. The problem? It's a mess. And it's not automatable. So to get more data, you need to go to [the Department of Labor website](https://oui.doleta.gov/unemploy/claims.asp) and fill out the form.

Leave the years, select state, pick yours (I'm taking Nebraska) and select spreadsheet, but know that it's a lie. If you try to load it using read_excel, you'll get an error. Why? Because what is being downloaded is an HTML file. So you can use `rvest` to read and parse it.

However, that isn't so simple.

```{r}
library(rvest)
library(lubridate)
```

First, read the file you downloaded. You'll need to add the `fill=TRUE` to html_table() to solve a problem with some of the data.

```{r, error=TRUE}
claims <- read_html("~/Downloads/r539cy.xls") |>
  html_nodes("table") |>
  html_table(fill=TRUE)
```

Similar to previous efforts, we get a list. The first element is a dataframe, so let's get that.

```{r, error=TRUE}
claims <- claims[[1]]
```

Now if you look at this data, you'll see that the header row is empty, and the header names are in the first row. Also, none of this data is formatted correctly. We need to fix that.

To do this, we're going to fix this in steps after we create a new dataframe called `cleanclaims`

1. We'll remove empty columns.
2. We'll rename the columns with what they are, using `janitor` style naming conventions.
3. We'll filter out the old header row.
4. We'll mutate each field to format them correctly. The date columns will use `lubridate`'s `mdy` function. Numbers will use `readr`'s `parse_number` function to solve the comma separator issue.

```{r, error=TRUE}
cleanclaims <- claims |>
  remove_empty("cols") |>
  rename("state" = 1, "filed_week_ended"= 2, "initial_claims"=3, "reflecting_week_ended"=4, "continued_claims"=5, "covered_employment"=6, "insured_unemployment_rate"=7) |>
  filter(state != "State") |>
  mutate(filed_week_ended = mdy(filed_week_ended), initial_claims=parse_number(initial_claims), reflecting_week_ended=mdy(reflecting_week_ended), continued_claims=parse_number(continued_claims), covered_employment=parse_number(covered_employment), insured_unemployment_rate=parse_number(insured_unemployment_rate))
```

If you open `cleanclaims`, you may notice something:

```{r, error=TRUE}
cleanclaims |> arrange(desc(filed_week_ended)) |> head()
```

See it? The latest data isn't in there in my version. [It's in a press release](https://www.dol.gov/sites/dolgov/files/OPA/newsreleases/ui-claims/20200551.pdf).

So how do we add it? We use `add_row` from the tidyverse (specifically the `tibble` library).

```{r, error=TRUE}
updatedcleanclaims <- cleanclaims |> add_row(state="Nebraska", filed_week_ended=as.Date("2020-03-28"), initial_claims=24572, reflecting_week_ended=as.Date("2020-03-21"))
```

Now we can repeat the analysis from above.

First we rank.

```{r, error=TRUE}
fedranked <- updatedcleanclaims |> mutate(Rank = min_rank(desc(initial_claims))) |> arrange(desc(filed_week_ended))
```

We get the latest week.

```{r, error=TRUE}
fedlatest <- fedranked |> slice(1)
```

The previous week ...

```{r, error=TRUE}
fedprevious <- fedranked |> slice(2)
```

And for this one, let's grab the pre-crisis number as well as a basis point.

```{r, error=TRUE}
pre <- fedranked |> filter(filed_week_ended==as.Date("2020-03-14"))
```

Now we can do some percent change math similar to above. First we calculate the current change.

```{r, error=TRUE}
change <- round((((fedlatest$initial_claims - fedprevious$initial_claims)/fedprevious$initial_claims)*100), digits=0)
```

Then the change from the pre-crisis week.

```{r, error=TRUE}
prechange <- round((((fedlatest$initial_claims - pre$initial_claims)/pre$initial_claims)*100), digits=0)
```

And we roll it all into a subhead that we can use in the chart with a little paste-fu.

```{r, error=TRUE}
subhed <- paste("Applications for unemployment jumped ", change, " percent from last week to this week and are up ", prechange, " percent since March 14.", sep="")
```

And we make the graphic.

```{r, error=TRUE, fig.width=9}
ggplot() +
  geom_line(data=fedranked, aes(x=filed_week_ended, y=initial_claims, group=1)) +
  geom_point(data=fedlatest, aes(x=filed_week_ended, y=initial_claims)) +
  geom_text(data=fedlatest, aes(x=filed_week_ended-500, y=initial_claims + 500, label=initial_claims)) +
  geom_point(data=fedprevious, aes(x=filed_week_ended, y=initial_claims)) +
  geom_text(data=fedprevious, aes(x=filed_week_ended-500, y=initial_claims + 500, label=initial_claims)) +
  geom_smooth(data=fedranked, aes(x=filed_week_ended, y=initial_claims), method=loess, se=FALSE) +
  labs(title="Another record for jobless claims in Nebraska", subtitle=subhed, x="Date", y="Claims", caption = "Source: US Dept. of Labor  |  Graphic by Matt Waite") +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 16, face = "bold"),
    axis.title = element_text(size = 8),
    plot.subtitle = element_text(size=10),
    panel.grid.minor = element_blank()
    )
```

<!--chapter:end:21-automatinganalysis.Rmd-->

# Automating geographic analysis

One thing that has been very apparent with the coronavirus outbreak is that this is a very geographic story. Where cases are being found and how fast is news, so it would be a good idea for us to have updating maps. But to have that, we need to have updating data.

Good news.

The New York Times is making the data behind [their interactive trackers](https://www.nytimes.com/interactive/2020/us/coronavirus-us-cases.html) [available to others for free](https://github.com/nytimes/covid-19-data).

So we have a constantly updating data stream on Github, so that means we can make this work.

Let's get our libraries first:

```{r}
library(tidyverse)
library(sf)
```

We can use `read_csv` to read a URL if that URL is to a csv file. And Github just happens to provide a direct link to the CSV of county COVID-19 reports. Here's what that looks like:

```{r}
covid <- read_csv("https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv")
```

Let's look at what the New York Times is providing us:

```{r}
head(covid)
```

If you look, we have a county and a date -- how many cases are reported in that county on that day. That means we can do some interesting progression charts.

Let's filter out Nebraska first.

```{r}
nebraska <- covid |> filter(state == "Nebraska")
```

And we can create line chart like this:

```{r}
ggplot() + geom_line(data=nebraska, aes(x=date, y=cases, group=county, color=county))
```

The little county on the bottom that curves sharply up? That's my home county, Washington County. One day of this writing, they added 10 cases in one day in one nursing home. Grim stuff.

The curve you see for Douglas County is a classic exponential curve. Because the number of cases here are small, we can get away with it for a little while. But when looking at much larger places, you'd use a log scale. [YOU REALLY SHOULD WATCH THIS](https://www.ft.com/video/9a72a9d4-8db1-4615-8333-4b73ae3ddff8). You've no doubt seen the Financial Times coronavirus trajectory tracker. Hear why they are using a log scale. And here's what our chart looks like with it. Note the y-axis scale.

```{r}
ggplot() + geom_line(data=nebraska, aes(x=date, y=cases, group=county, color=county)) + scale_y_continuous(trans="log10")
```

## Mapping continuously

But for a map, we can't have multiple days. We need a single day. Ideally, it would be the most recent date. We can get it using the `max` function.

```{r}
current <- nebraska |> summarize(max(date))
```

That will give us the most recent date in Nebraska in a variable called `current`. And now we can filter the most recent data for Nebraska, regardless of when this runs.

I'm adding one piece to the end to make joining this to a map easier and just renaming fips to GEOID, because they are identical in both datasets and can be used for joining.

```{r}
nebraskacurrent <- nebraska |> filter(date == current[[1]]) |> rename(GEOID = fips)
```

Now we can read in our counties map layer.

```{r}
counties <- st_read("data/cb_2018_us_county_5m/cb_2018_us_county_5m.shp")
```

And join the two together.

```{r}
counties <- counties |> left_join(nebraskacurrent)
```

Since we have every county in the United States in our counties map layer, we can filter just Nebraska like this:

```{r}
necounties <- counties |> filter(STATEFP == 31)
```

So now, we have a geographic dataframe that has both the county shapes and the number of cases in the most recent data update. We just need to see it now:

```{r}
ggplot() +
  geom_sf(data=necounties, aes(fill=cases)) +
  scale_fill_viridis_c(option = "plasma", trans = "sqrt") +
  theme_void() +
  labs(title = paste("COVID-19 cases as of ", current[[1]], sep=""))
```

As it stands, we can run this every day and get an up-to-date map.

<!--chapter:end:22-automatingmapanalysis.Rmd-->

# Basic Stats: The T-Test

During the coronavirus social distancing, former Lincoln Police Chief and Director of Public Safety Tom Casady tweeted the following:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">If you’re interested I how effective social distancing is working, a good surrogate measure would be the number of police incidents. Lincoln averages about 329 per day. <a href="https://t.co/tQAo7CN82r">https://t.co/tQAo7CN82r</a></p>&mdash; Tom Casady (@TCasady304) <a href="https://twitter.com/TCasady304/status/1239696474813992962?ref_src=twsrc%5Etfw">March 16, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

So, is it?

Let's find out. Can we see signs of Lincoln hunkering down in LPD data?

We're fortunate here. We can get [daily updated calls for service data from the Lincoln Open Data Portal](http://opendata.lincoln.ne.gov/datasets/lpd-dispatch-records). So download the spreadsheet version (which is really a csv) and load some libraries.

```{r}
library(tidyverse)
library(lubridate)
library(janitor)
```

And now load the data.

```{r}
calls <- read_csv("data/LPD_Dispatch_Records.csv")
```

The first issue we need to address is that the dates don't import as dates. So we need to fix RPT_Date to make it a date. Lubridate to the rescue.

```{r}
calls <- calls |> mutate(RPT_Date = ymd(RPT_Date))
```

For purposes of this work, we can live without the 13 (they don't occur when we're interested in).

First, let's just visualize regular call volumes and see: Does the end of the line drop sharply, indicating people are socially distancing?

```{r}
calls |> group_by(RPT_Date) |> summarize(total=n()) |> ggplot() + geom_line(aes(x=RPT_Date, y=total))
```

Short answer? No. Longer answer? It's obvious that Lincoln has a seasonality to crime. It generally is low over the dead of winter and rises as it gets warmer, peaking during early football season (no surprise there) and dropping back down as winter sets in. So to make this useful, we should only compare dates in this March to previous March dates. Then we eliminate the seasonality issue.

So we're going to do some filtering here. The following block is broken down like this:

1. First, filter days in March.
2. Then, group calls by day.
3. Then, count them. How many happened that day?
4. Then we're going to do some mutate magic. `case_when` is if/else logic. So what this says is if the date is less than March 16, 2020 -- the day Lincoln's mayor declared a local emergency -- then it's in the Previous group. After and equal to March 16, 2020, it's in the Current group.
5. Last, but not least, cutting out April dates in this dataset. Just going to look at March.

And we'll put it all in a dataframe called `calldays`.

```{r}
calldays <- calls |> filter(month(RPT_Date) == 3) |> group_by(RPT_Date) |> summarize(total_calls = n()) |> mutate(TimePeriod = case_when(
  RPT_Date < as.Date("2020-03-16") ~ "Previous",
  RPT_Date >= as.Date("2020-03-16") ~ "Current",
)) |> filter(RPT_Date < as.Date("2020-04-01"))
```

To test if the current period of time is different from previous March days, we're going to use something called a t-test. A t-test simply asks if two sample means are different -- is the average between sample A different from the mean of sample B. It looks at the variability in the data and boils it down to a p-value. The p-value tells us if it's statistically significant or not. Simply put: If our value is below .05, it's statistically significant and the two means are different. If it's above .05, then the differences between the two numbers are not meaningful -- it could just be random chance that they're different.

[More on the t-test here](https://conjointly.com/kb/statistical-student-t-test/). Worth reading.

In R, a `t.test` is a base R function, owing to it being a statistical language. So were going to look at `total_calls` split between `TimePeriod`s. Here's the code. We'll interpret the output below.

```{r}
t.test(total_calls ~ TimePeriod, data = calldays)
```

First, look at the two means on the bottom. On the left, it says the daily call average in the current group -- March 16 to March 31 -- is 295.73 calls per day. On the right, in previous March days, it's 297.86. So about 2.3 calls difference. Is that a significant difference? Look above to the p-value for that answer. At 0.7645, that's considerably higher than .05.

So, statistically speaking, these numbers are the same. There's no real difference between them. A bad reporter would write a story -- or a bad mayor would hold a press conference and shout from the heavens -- that crime is down 2.3 calls per day on average! This is good! But it's not real. It's statistically meaningless. It's just random noise.

## Specific call types

But you need only look outside your house to realize the world is different. So how is it that calls to Lincoln police aren't?

As they say, the devil is in the details.

Let's use `janitor`'s `tabyl` function to get a look at what kind of call groups we can look at.

```{r}
calls |> filter(RPT_Date >= as.Date("2020-03-01")) |> tabyl(CFS_LEGEND)
```

Let's first look at disturbances, one of the first big call types on that list. Disturbances, for those of you who have always stayed on the right side of the law, are anything where the police get called out to and people are acting the fool. Arguments between neighbors? Disturbance. Terrible racket coming from the back yard down the way? Disturbance. So it's pretty generic and it happens a lot.

To look specifically at a crime, we're going to take what we did before and add a filter into it, and we'll name the dataframe what we're looking at.

```{r}
disturbance <- calls |> filter(CFS_LEGEND == "DISTURBANCE") |> filter(month(RPT_Date) == 3) |> group_by(RPT_Date) |> summarize(total_calls = n()) |> mutate(TimePeriod = case_when(
  RPT_Date < as.Date("2020-03-16") ~ "Previous",
  RPT_Date >= as.Date("2020-03-16") ~ "Current",
)) |> filter(RPT_Date < as.Date("2020-04-01"))
```

Then we can do our t-test.

```{r}
t.test(total_calls ~ TimePeriod, data = disturbance)
```

So what does this say? Typically, you'd see 48.85 disturbances a day on average before coronavirus. Now? 56. Is that a significant difference? According to our p-value of 0.0138, it is. So police are going to more disturbances than usual.

What about traffic calls?

```{r}
traffic <- calls |> filter(CFS_LEGEND == "TRAFFIC") |> filter(month(RPT_Date) == 3) |> group_by(RPT_Date) |> summarize(total_calls = n()) |> mutate(TimePeriod = case_when(
  RPT_Date < as.Date("2020-03-16") ~ "Previous",
  RPT_Date >= as.Date("2020-03-16") ~ "Current",
)) |> filter(RPT_Date < as.Date("2020-04-01"))
```

```{r}
t.test(total_calls ~ TimePeriod, data = traffic)
```

Typically get about 32 calls a day, now 23. That's a big difference, and a statistically significant one.

How about car accidents?

```{r}
accident <- calls |> filter(CFS_LEGEND == "ACCIDENT") |> filter(month(RPT_Date) == 3) |> group_by(RPT_Date) |> summarize(total_calls = n()) |> mutate(TimePeriod = case_when(
  RPT_Date < as.Date("2020-03-16") ~ "Previous",
  RPT_Date >= as.Date("2020-03-16") ~ "Current",
)) |> filter(RPT_Date < as.Date("2020-04-01"))
```

```{r}
t.test(total_calls ~ TimePeriod, data = accident)
```

Typically get a little more than 25 accident calls a day. Now, it's 14.4. Insurance companies are making out like bandits right now.

## What else should we look at?

With a little magic using `tidyr` to transform the data ([learn more about that here](http://mattwaite.github.io/sports/transforming-data.html)), we can get a peek at other call groups worth exploring.

```{r}
calls |> filter(month(RPT_Date) == 3) |> group_by(RPT_Date, CFS_LEGEND) |> summarize(total_calls = n()) |> mutate(TimePeriod = case_when(
  RPT_Date < as.Date("2020-03-16") ~ "Previous",
  RPT_Date >= as.Date("2020-03-16") ~ "Current",
)) |> filter(RPT_Date < as.Date("2020-04-01")) |>
  group_by(TimePeriod, CFS_LEGEND) |> summarize(avg_calls = mean(total_calls)) |> pivot_wider(names_from = "TimePeriod", values_from = "avg_calls") |> na.omit()
```

Other interesting stories here? DUI is almost half of what it normally is. Thefts from vehicles are up way up. Also: trespassing? What's that about?

But are they meaningful differences? Put them to the test.

<!--chapter:end:23-basicstats.Rmd-->

# An intro to text analysis

Throughout this course, we've been focused on finding information in structured data. We've learned a lot of techniques to do that, and we've learned how the creative mixing and matching of those skills can find new insights.

What happens when the insights are in unstructured data? Like a block of text?

Turning unstructured text into data to analyze is a whole course in and of itself -- and one worth taking if you've got the credit hours -- but some simple stuff is in the grasp of basic data analysis.

To do this, we'll need a new library -- tidytext, which you can guess by the name plays very nicely with the tidyverse. So install it with `install.packages("tidytext")` and we'll get rolling.

```{r}
library(tidyverse)
library(tidytext)
library(janitor)
library(lubridate)
```

Here's the question we're going to go after: What's changed with campus crime since coronavirus chased everyone off campus?

[Here's a dataset that has crime data before and after the quarantine started](https://unl.box.com/s/1reagysljshlxgvhs92bcj7xid1o50ll).

This dataset has some flaws -- UNL uses some ... interesting ... datetime formats and there's a newline character in the report number field. We can fix all these issues on load with a little creative work.

The following block is going to ...

1. Make a new dataframe called unl
2. Read the csv of crime reports we downloaded.
3. Use `janitor` to clean the header names
4. Use mutate and `lubridate` to fix the dates in three fields
5. Use `gsub` to drop the newline character -- a newline is `\n` and the extra slash tells gsub to ignore the next character, meaning don't read this as a newline, read it as backslash n.

```{r}
unl <- read_csv("data/unlcrime_covid.csv") |>
  clean_names() |>
  mutate(
    reported = mdy_hm(reported),
    start_occurred = mdy_hm(start_occurred),
    end_occurred = mdy_hm(end_occurred),
    case = gsub("\\n", "", case)
  )
```

We can see what that all did with head:

```{r}
head(unl)
```

To look at a before and after picture, let's create two different dataframes -- one from January 1 to March 13, the day the University told everyone to go home. Campus that day was pretty empty -- few classes actually went on as scheduled, and most students didn't go to the ones that did.

```{r}
pre2020 <- unl |> filter(reported < "2020-03-13" & reported >= "2020-01-01")
```

And now we'll take that March 13 and everything after it.

```{r}
post2020 <- unl |> filter(reported >= "2020-03-13")
```

To answer this question, on the surface, we don't need to use text analysis to answer this question. We could simply ask what was the most common incident at UNLPD before classes were cancelled and then moved online? Let's just group it by incident_code, count them up and then get a top ten list. To go a step further, we'll calculate a percent that the crime represented by dividing the count by the sum of the counts and we'll use top_n to give us the top 10 incidents.

```{r}
pre2020 |>
  group_by(incident_code) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

So disturbances at the top with 32 of them reported between January 1 and the end. What about after the calamity?

```{r}
post2020 |>
  group_by(incident_code) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

Trespassing. That right there is a story -- the most common thing UNLPD has responded to in the few weeks since campus was pretty much closed is trespassing complaints. But really, there's not much going on here if 5 is the most common, and it's 12 percent of the total call volume.

Which begs the question: How many trespassing complaints did university police respond to before coronavirus?

```{r}
pre2020 |>
  group_by(incident_code) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  filter(incident_code == "TRESPASSING")
```

Six. So six in two and a half months, five in a few weeks. That's a campus crime story.

## Going further by analyzing text

But notice the summary field at the end of the report? See all that text? There's a lot of meaning in there. As a reporter, I used to read those reports at different law enforcement agencies every day, looking for news. So it's a rich stream of data in there. How do we get at it?

Text analysis attempts to find meaning in unstructured data by breaking apart the words. That's a bit of an oversimplification, but it'll do for now. Again, take a class in it. We're going to take all the words in the summary, break them apart into individual words. Now, it'd doesn't take a genius to realize if we did that, words like a, the, and, but and others would be at the top. So, we need to remove those. They're called stop words in text analysis. The tidytext library a dataframe of them. We can import it like this:

```{r}
data("stop_words")
```

The magic of tidytext is in a function called `unnest_tokens`, which is going to take the sentence that you read in the summary and break them apart in to words. The way that `unnest_tokens` works is that we tell it what we want to call the field we're creating with this breaking apart, then we tell it what we're breaking apart -- what field has all the text in it. For us, that's the summary.

Then we're going to use a dplyr function we haven't used yet called an `anti_join`, which filters out any matches. So we'll `anti_join the stop words and get a list of words that aren't stop words.

From there, we can get a simple word frequency by just grouping them together and counting them. We can borrow the percent code from above to get a percent of the words our top 10 words represent.

```{r}
pre2020 |>
  unnest_tokens(word, summary) |>
  anti_join(stop_words) |>
  group_by(word) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

Before the calamity, student, reported and university were your top three by a long ways. What about after?

```{r}
post2020 |>
  unnest_tokens(word, summary) |>
  anti_join(stop_words) |>
  group_by(word) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

Reported is now one, university two and student has dropped to three. But look at the proportions -- student was 7 percent of the words before, now students are 4 percent. I'm trying to decide if that's expected -- most students left campus -- or surprising that it's still that high.

## Going beyond a single word

The next step in text analysis is using `ngrams`. An `ngram` is any combination of words that you specify. Two word ngrams are called bigrams (bi-grams). Three would be trigrams. And so forth.

The code to make ngrams is similar to what we did above, but involves some more twists.

So this block is is going to do the following:

1. Use the pre2020 data we created above
2. Unnest the tokens again, but instead we're going to create a field called bigram, break apart summary, but we're going to specify the tokens in this case are ngrams of 2.
3. We're going to make things easier to read on ourselves and split bigrams into word1 and word2.
4. We're going to filter out stopwords again, but this time we're going to do it in both word1 and word2 using a slightly different filtering method.
5. Because of some weirdness in calculating the percentage, we're going to put bigram back together again, now that the stop words are gone.
6. We'll then group by, count and create a percent just like we did above.
7. We'll then use top_n to give us the top 10 bigrams.


```{r}
pre2020 |>
  unnest_tokens(bigram, summary, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  mutate(bigram = paste(word1, word2, sep=" ")) |>
  group_by(bigram) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

And we already have a different, more nuanced result. Student and reported were our top two single words. Why? Because they appear together a lot. In pre-calamity reports, student reported was the top bigram. What about after?

```{r}
post2020 |>
  unnest_tokens(bigram, summary, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  mutate(bigram = paste(word1, word2, sep=" ")) |>
  group_by(bigram) |>
  tally(sort=TRUE) |>
  mutate(percent = (n/sum(n))*100) |>
  top_n(10)
```

Look at that -- you can remove 90 percent of the students from campus, and they're still the top callers to UNLPD. The percent is almost no different.

So what are the students left on campus reporting to UNLPD?

```{r}
post2020 |>
  unnest_tokens(bigram, summary, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  mutate(bigram = paste(word1, word2, sep=" ")) |>
  filter(bigram == "student reported") |>
  group_by(incident_code) |>
  tally(sort=TRUE)
```

What about before?

```{r}
pre2020 |>
  unnest_tokens(bigram, summary, token = "ngrams", n = 2) |>
  separate(bigram, c("word1", "word2"), sep = " ") |>
  filter(!word1 %in% stop_words$word) |>
  filter(!word2 %in% stop_words$word) |>
  mutate(bigram = paste(word1, word2, sep=" ")) |>
  filter(bigram == "student reported") |>
  group_by(incident_code) |>
  tally(sort=TRUE)
```

<!--chapter:end:24-textanalysis.Rmd-->

# Writing with numbers

The number one sin of all early career data journalist is to get really, really, really attached to the analysis you've done and include every number you find.

Don't do that.

Numbers tell you what. Numbers rarely tell you why. What question has driven most people since they were three years old? Why. The very first thing to do is realize that is the purpose of reporting. You've done the analysis to determine the what. Now go do the reporting to do the why. Or as an old editor of mine used to say "Now go do that reporting shit you do."

The trick to writing a numbers story is to frame your story around people. Sometimes, your lead can be a number, if that number is compelling. Often, your lead is a person, a person who is one of the numbers you are writing about.

Tell their story. Briefly. Then, let us hear from them. Let them speak about what it is you are writing about.

Then come the numbers.

## How to write about numbers without overwhelming with numbers.

Writing complex stories is often a battle against that complexity. You don't want to overwhelm. You want to simplify where you can. The first place you can do that is only use exact numbers where an exact number is called for.

Where you can, do the following:

* Using ratios instead of percents
* Often, it's better to put it in counts of 10. 6 of 10, 4 of 10. It's easy to translate that from a percentage to a ratio.
* But be careful when your number is 45 percent. Is that 4 in 10 or 5 in 10?
* If a ratio doesn't make sense, round. There's 287,401 people in Lincoln, according to the Census Bureau. It's easier, and no less accurate, to say there's more than 287,000 people in Lincoln.

**A critical question your writing should answer: As compared to what?**

How does this compare to the average? The state? The nation? The top? The bottom?

One of the most damning numbers in the series of stories Craig Pittman and I wrote that became the book [Paving Paradise](https://www.amazon.com/Paving-Paradise-Floridas-Vanishing-Wetlands-ebook/dp/B004HZXZCE) was comparing approvals and denials.

We were looking at the US Army Corps of Engineers and their permitting program. We were able to get a dataset of just a few years of permits that was relatively clean. From that, we were able to count the number of times the corps had said yes to a developer to wipe out wetlands the law protected and how many times they said no.

They said yes 12,000 times. They said no once.

That one time? Someone wanted to build an eco-lodge in the Everglades. Literally. Almost every acre of the property was wetlands. So in order to build it, the developer would have to fill in the very thing they were going to try to bring people into. The corps said no.

## When exact numbers matter

Sometimes ratios and rounding are not appropriate.

This is being written in the days of the coronavirus. Case counts are where an exact number is called for. You don't say that there are more than 70 cases in Lancaster County on the day this was written. You specify. It's 75.

You don't say almost 30 deaths. It's 28.

Where this also comes into play is any time there are deaths: Do not round bodies.

## An example

[Read this story from USA Today and the Arizona Republic](https://www.azcentral.com/in-depth/news/local/arizona-wildfires/2019/07/22/wildfire-risks-more-than-500-spots-have-greater-hazard-than-paradise/1434502001/). Notice first that the top sets up a conflict: People say one thing, and that thing is not true.

> No one could have anticipated such a catastrophe, people said. The fire's speed was unprecedented, the ferocity unimaginable, the devastation unpredictable.

>Those declarations were simply untrue. Though the toll may be impossible to predict, worst-case fires are a historic and inevitable fact.

The first voice you hear? An expert who studies wildfires.

> Phillip Levin, a researcher at the University of Washington and lead scientist for the Nature Conservancy in Washington, puts it this way: "Fire is natural. But the disaster happens because people didn't know to leave, or couldn't leave. It didn't have to happen."

Then notice how they take what is a complex analysis using geographic information systems, raster analysis, the merging of multiple different datasets together and show that it's quite simple -- the averaging together of pixels on a 1-5 scale.

Then, the compare what they found to a truly massive fire: The Paradise fire that burned 19,000 structures.

> Across the West, 526 small communities — more than 10 percent of all places — rank higher.

And that is how it's done. Simplify, round, ratios: simple metrics, powerful results.

<!--chapter:end:25-writingwithdata.Rmd-->

# Ethics in data journalism

[This originally appeared on Open News in March 2013](https://source.opennews.org/articles/public-info-doesnt-always-want-be-free/).

In 2009, a senior web editor asked me and another developer a question: could our development group build a new news application for Tampabay.com that displayed a gallery of mug shots? Stories about goofy crimes with strange mug shots were popular with readers. The vision, on the part of management, was a website that would display the mugshots collected every day from publicly available websites by two editors—well paid, professional editors with other responsibilities.

Newsrooms are many things. Alive. Filled with energy. Fueled by stress, coffee and profanity. But they are also idea factories. Day after day, ideas come from everywhere. From reporters on the beat. From editors reading random things. From who knows where. Some of them are brilliant. Some would never work. Most need more people and time than are available. And some are dumber than anyone cares to admit.

We thought this idea was nuts. Why would we pay someone, let alone an editor, to fetch mug shots from the Internet? Couldn’t we do that with a scraper?

If only this were the most complex question we would face.

Because given enough time and enough creativity, scraping a mug shot website is easy. You need to recognize a pattern, parse some HTML and gather the pieces you need. At least that’s how it should work. Police agencies that put mugs online usually buy software from a vendor. Apparently, those vendors enjoy making horrific, non-standard, broken-in-interesting-and-unique-ways HTML. You’ll swear. A lot. But you’ll grind it out. And that’s part of the fun. Scraping isn’t any fun with clean, semantic, valid HTML. And scraping mug shot websites, by that definition, is tons of fun.

The complexity comes when you realize the data you are dealing with represent real people’s lives.

## Problems

The first problem we faced, long before we actually had data, was that data has a life of its own. Because we were going to put this information in front of a big audience, Google was going to find it. That meant if we used our normal open door policy for the Googlebot, someone’s mug shot was going to be the first record in Google for their name, most likely. It would show up first because most people dont actively cultivate their name on the web for visibility in Google. It would show up first because we know how SEO works and they dont. It would show up first because our site would have more traffic than their site, and so Google would rank us higher.

And that record in Google would exist as long as the URL did. Longer when you consider the cached versions Google keeps.

That was a problem because here are the things we could not know:

* Was this person wrongly arrested?
* Was this person innocent?
* Were the charges dropped against this person?
* Did this person lie about any of their information?

## The Googlebot

So it turned out to be very important to know the Googlebot. It’s your friend … until it isn’t. We went to our bosses and said words that no one had said to them before: we did not want Google to index these pages. In a news organization, the page view is the coin of the realm. It is — unfortunately — how many things are evaluated when the bosses ask if it was successful or not. So, with that in mind, Google is your friend. Google brings you traffic. Indeed, Google is your single largest referrer of traffic at a news organization, so you want to throw the doors open and make friends with the Googlebot.

But here we were, saying Google wasn’t our friend and that we needed to keep the Googlebot out. And, thankfully, our bosses listened to our argument. They too didn’t want to be the first result in Google for someone.

So, to make sure we were telling the Googlebot no, we used three lines of defense. We told it no in robots.txt and on individual pages as a meta tag, and we put the most interesting bits of data into a simple JavaScript wrapper that made it hard on the bot if the first two things failed.

The second solution had ramifications beyond the Googlebot. We decided that we were not trying to make a complete copy of the public record. That existed already. If you wanted to look at the actual public records, the sheriff’s offices in the area had websites and they were the official keeper of the record. We were making browsing those images easy, but we were not the public record.

That freedom had two consequences: it meant our scrapers could, at a certain point and given a number of failures, just give up on getting a mug. Data entered by humans will be flawed. There will be mistakes. Because of that, our code would have to try and deal with that. Well, there’s an infinite number of ways people can mess things up, so we decided that since we were not going to be an exact copy of the public record, we could deal with the most common failures and dump the rest. During testing, we were getting well over 98% of mugs without having to spend our lives coding for every possible variation of typo.

The second consequence of the decision actually came from the newspapers lawyers. They asked a question that dumbfounded us: How long are you keeping mugs? We never thought about it. Storage was cheap. We just assumed we’d keep them all. But, why should we do that? If we’re not a copy of the public record, we dont have to keep them. And, since we didnt know the result of each case, keeping them was really kind of pointless.

So, we asked around: How long does a misdemeanor case take to reach a judgement? The answer we got from various sources was about 60 days. From arrest to adjudication, it took about two months. So, at the 60 day mark, we deleted the data. We had no way of knowing if someone was guilty or innocent, so all of them had to go. We even called the script The Reaper.

We’d later learn that the practical impacts of this were nil. People looked at the day’s mugs and moved on. The amount of traffic a mug got after the day of arrest was nearly zero.

## Data Lifetimes

The life of your data matters. You have to ask yourself, Is it useful forever? Does it become harmful after a set time? We had to confront the real impact of deleting mugs after 60 days. People share them, potentially lengthening their lifetime long after they’ve fallen off the homepage. Delete them and that URL goes away.

We couldn’t stop people from sharing links on social media—and indeed probably didn’t want to stop them from doing it. Heck, we did it while we were building it. We kept IMing URLs to each other. And that’s how we realized we had a problem. All our work to minimize the impact on someone wrongly accused of a crime could be damaged by someone sharing a link on Facebook or Twitter.

There’s a difference between frictionless and unobstructed sharing and some reasonable constraints.

We couldn’t stop people from posting a mug on Facebook, but we didn’t have to make it easy and we didn’t have to put that mug front and center. So we blocked Facebook from using the mug as the thumbnail image on a shared link. And, after 60 days, the URL to the mug will throw a 404 page not found error. Because it’s gone.

We couldn’t block Google from memorializing someone’s arrest, only to let it live on forever on Facebook.

## You Are a Data Provider

The last problem didn’t come until months later. And it came in the middle of the night. Two months after we launched, my phone rang at 1 a.m. This is never a good thing. It was my fellow developer, Jeremy Bowers, now with NPR, calling me from a hotel in Washington DC where he was supposed to appear in a wedding the next day. Amazon, which we were using for image hosting, was alerting him that our bandwidth bills had tripled on that day. And our traffic hadn’t changed.

What was going on?

After some digging, we found out that another developer had scraped our site—because we were so much easier to scrape than the Sheriff’s office sites—and had built a game out of our data called Pick the Perp. There were two problems with this: 1. The game was going viral on Digg (when it was still a thing) and Reddit. It was getting huge traffic. 2. That developer had hotlinked our images. He/she was serving them from our S3 account, which meant we were bearing the costs. And they were going up exponentially by the minute.

What we didn’t realize when we launched, and what we figured out after Pick the Perp, was that we had become data provider, in a sense. We had done the hard work of getting the data out of a website and we put it into neat, semantic, easily digestible HTML. If you were after a stream of mugshots, why go through all the hassle of scraping four different sheriff’s office’s horrible HTML when you could just come get ours easily?

Whoever built Pick the Perp, at least at the time, chose to use our site. But, in doing so, they also chose to hotlink images—use the URL of our S3 bucket, which cost us money—instead of hosting the images themselves.

That was a problem we hadn’t considered. People hotlink images all the time. And, until those images are deleted from our system, they’ll stay hotlinked somewhere.

Amazon’s S3 has a system where you can attach a key to a file that expires after X period of time. In other words, the URL to your image only lasts 15 minutes, or an hour, or however long you decide, before it breaks. It gives you fine grained control over how long someone can use your image URL.

So at 3 a.m., after two hours of pulling our hair out, we figured out how to sync our image keys with our cache refreshes. So every 15 minutes, a url to an image expired and Pick the Perp came crashing down.

While the Pick the Perp example is an easy one—it’s never cool to hotlink an image—it does raise an issue to consider. Because you are thinking carefully about how to build your app the right way doesn’t mean someone else will. And it doesn’t mean they won’t just go take your data from your site. So how could you deal with that? Make the data available as a download? Create an API that uses your same ethical constructs? Terms of service? All have pros and cons and are worth talking about before going forward.

## Ethical Data

We live in marvelous times. The web offers you no end of tools to make things on the web, to put data from here on there, to make information freely available. But, we’re an optimistic lot. Developers want to believe that their software is being used only for good. And most people will use it for good. But, there are times where the data you’re working with makes people uncomfortable. Indeed, much of journalism is about making people uncomfortable, publishing things that make people angry, or expose people who don’t want to be exposed.

What I want you to think about, before you write a line of code, is what does it mean to put your data on the internet? What could happen, good and bad? What should you do to be responsible about it?

Because it can have consequences.

On Dec. 23, the Journal News in New York published a map of every legal gun permit holder in their home circulation county. It was a public record. They put it into Google Fusion Tables and Google dutifully geocoded the addresses. It was a short distance to publication from there.

Within days, angry gun owners had besieged the newspaper with complaints, saying the paper had given criminals directions to people’s houses where they’d find valuable guns to steal. They said the paper had violated their privacy. One outraged gun owner assembled a list of the paper’s staff, including their home addresses, telephone numbers, email addresses and other details. The paper hired armed security to stand watch at the paper.

By February, the New York state legislature removed handgun permits from the public record, citing the Journal News as the reason.

There’s no end of arguments to be had about this, but the simple fact is this: The reason people were angry was because you could click on a dot on the map and see a name and an address. In Fusion Tables, removing that info window would take two clicks.

Because you can put data on the web does not mean you should put data on the web. And there’s a difference between a record being “public” and “in front of a large audience.”

So before you write the first line of code, ask these questions:

* This data is public, but is it widely available? And does making it widely available and easy to use change anything?
* Should this data be searchable in a search engine?
* Does this data expose information someone has a reasonable expectation that it would remain at least semi-private?
* Does this data change over time?
* Does this data expire?
* What is my strategy to update or delete data?
* How easy should it be to share this data on social media?
* How should I deal with other people who want this data? API? Bulk download?

Your answers to these questions will guide how you build your app. And hopefully, it’ll guide you to better decisions about how to build an app with ethics in mind.

<!--chapter:end:26-ethics.Rmd-->

# Using GitHub

GitHub is a platform for managing and storing files, data and code built atop Git, a popular open source version control software. GitHub accounts are free and it's [easy to get started](https://docs.github.com/en/get-started/quickstart). The one prerequisite is that you have [Git installed on your local computer](https://docs.github.com/en/get-started/quickstart/set-up-git). There are installers for Mac, Windows and Linux.

## How It Works

Version control is based on the ideas that you want to keep track of changes you make to a collection of files and that multiple people can work together without getting in each other's way or having to do things in a set order. For individual users, it's great for making sure that you always have your work.

GitHub users work in what are known as repositories on their local computers and also _push_ changes to a remote repository located on GitHub. That remote repository is key: if you lose your computer, you can fetch a version of your files from GitHub. If you want to work with someone else on the same files, you can each have a local copy, push changes to GitHub and then pull each others' changes back to your local computers.

So, like Microsoft Word's track changes but with a remote backup and multiple editors.

## Getting Started

After installing Git and signing up for a GitHub account, [download and install GitHub Desktop](https://docs.github.com/en/desktop/installing-and-configuring-github-desktop). It will have you sign into your GitHub account and then you'll have access to any existing repositories. If you don't have any, that's fine! You can [make one locally](https://docs.github.com/en/desktop/installing-and-configuring-github-desktop/overview/creating-your-first-repository-using-github-desktop).

GitHub has [good documentation for working in the Desktop app](https://docs.github.com/en/desktop/contributing-and-collaborating-using-github-desktop), and while the emphasis in this book will be on using GitHub for version control, it also supports recording issues (read: problems or questions) with your files, contributing to projects that aren't yours and more.

## Advanced Use

Although our focus is on the GitHub Desktop app, you can use Git and GitHub from your computer's command line interface, and GitHub has a purpose-built [command line client](https://docs.github.com/en/github-cli), too. GitHub  can also serve as a publishing platform for many types of files, and entire websites are hosted on [GitHub Pages](https://docs.github.com/en/pages).

<!--chapter:end:27-github.Rmd-->
